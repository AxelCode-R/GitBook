<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Effect of batch size | gitbook-demo.knit</title>
  <meta name="description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Effect of batch size | gitbook-demo.knit" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  <meta name="github-repo" content="https://github.com/AxelCode-R/GitBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Effect of batch size | gitbook-demo.knit" />
  
  <meta name="twitter:description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  

<meta name="author" content="Axel Roth" />


<meta name="date" content="2021-11-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mlp-example-credit-default.html"/>
<link rel="next" href="class-mlp.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A GitBook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#me"><i class="fa fa-check"></i><b>1.1</b> Me</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-book"><i class="fa fa-check"></i><b>1.2</b> The Book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#how-it-works"><i class="fa fa-check"></i><b>1.3</b> How it works</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="single-perceptron.html"><a href="single-perceptron.html"><i class="fa fa-check"></i><b>2</b> Single Perceptron</a>
<ul>
<li class="chapter" data-level="2.1" data-path="single-perceptron.html"><a href="single-perceptron.html#neural-network-basics"><i class="fa fa-check"></i><b>2.1</b> Neural Network Basics</a></li>
<li class="chapter" data-level="2.2" data-path="single-perceptron.html"><a href="single-perceptron.html#forward-pass"><i class="fa fa-check"></i><b>2.2</b> Forward pass</a></li>
<li class="chapter" data-level="2.3" data-path="single-perceptron.html"><a href="single-perceptron.html#backward-pass"><i class="fa fa-check"></i><b>2.3</b> Backward pass</a></li>
<li class="chapter" data-level="2.4" data-path="single-perceptron.html"><a href="single-perceptron.html#single-perceptron-1"><i class="fa fa-check"></i><b>2.4</b> Single Perceptron</a></li>
<li class="chapter" data-level="2.5" data-path="single-perceptron.html"><a href="single-perceptron.html#why-does-it-work"><i class="fa fa-check"></i><b>2.5</b> Why does it work?</a></li>
<li class="chapter" data-level="2.6" data-path="single-perceptron.html"><a href="single-perceptron.html#appendix-complete-code"><i class="fa fa-check"></i><b>2.6</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="adding-trainable-bias.html"><a href="adding-trainable-bias.html"><i class="fa fa-check"></i><b>3</b> Adding trainable Bias</a>
<ul>
<li class="chapter" data-level="3.1" data-path="adding-trainable-bias.html"><a href="adding-trainable-bias.html#generalising-the-bias"><i class="fa fa-check"></i><b>3.1</b> Generalising the Bias</a></li>
<li class="chapter" data-level="3.2" data-path="adding-trainable-bias.html"><a href="adding-trainable-bias.html#appendix-complete-code-1"><i class="fa fa-check"></i><b>3.2</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multilayer-perceptrons-mlp.html"><a href="multilayer-perceptrons-mlp.html"><i class="fa fa-check"></i><b>4</b> Multilayer Perceptrons (MLP)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multilayer-perceptrons-mlp.html"><a href="multilayer-perceptrons-mlp.html#forward-pass-1"><i class="fa fa-check"></i><b>4.1</b> Forward pass</a></li>
<li class="chapter" data-level="4.2" data-path="multilayer-perceptrons-mlp.html"><a href="multilayer-perceptrons-mlp.html#backward-pass-1"><i class="fa fa-check"></i><b>4.2</b> Backward pass</a></li>
<li class="chapter" data-level="4.3" data-path="multilayer-perceptrons-mlp.html"><a href="multilayer-perceptrons-mlp.html#appendix-complete-code-2"><i class="fa fa-check"></i><b>4.3</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html"><i class="fa fa-check"></i><b>5</b> MLP example (Credit Default)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html#loading-and-analysing-the-data"><i class="fa fa-check"></i><b>5.1</b> Loading and analysing the data</a></li>
<li class="chapter" data-level="5.2" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html#train-and-test-phase"><i class="fa fa-check"></i><b>5.2</b> Train and test phase</a></li>
<li class="chapter" data-level="5.3" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html#appendix-complete-code-3"><i class="fa fa-check"></i><b>5.3</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="effect-of-batch-size.html"><a href="effect-of-batch-size.html"><i class="fa fa-check"></i><b>6</b> Effect of batch size</a>
<ul>
<li class="chapter" data-level="6.1" data-path="effect-of-batch-size.html"><a href="effect-of-batch-size.html#impact-of-diffrent-hyperparameters"><i class="fa fa-check"></i><b>6.1</b> Impact of diffrent hyperparameters</a></li>
<li class="chapter" data-level="6.2" data-path="effect-of-batch-size.html"><a href="effect-of-batch-size.html#appendix-complete-code-4"><i class="fa fa-check"></i><b>6.2</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="class-mlp.html"><a href="class-mlp.html"><i class="fa fa-check"></i><b>7</b> Class MLP</a></li>
<li class="chapter" data-level="8" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>8</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="8.1" data-path="decision-trees.html"><a href="decision-trees.html#entropy"><i class="fa fa-check"></i><b>8.1</b> Entropy</a></li>
<li class="chapter" data-level="8.2" data-path="decision-trees.html"><a href="decision-trees.html#constructing-the-tree"><i class="fa fa-check"></i><b>8.2</b> Constructing the Tree</a></li>
<li class="chapter" data-level="8.3" data-path="decision-trees.html"><a href="decision-trees.html#forcast-credit-defaults-with-dt"><i class="fa fa-check"></i><b>8.3</b> Forcast Credit Defaults with DT</a></li>
<li class="chapter" data-level="8.4" data-path="decision-trees.html"><a href="decision-trees.html#extern-packages"><i class="fa fa-check"></i><b>8.4</b> Extern Packages</a></li>
<li class="chapter" data-level="8.5" data-path="decision-trees.html"><a href="decision-trees.html#appendix-complete-code-5"><i class="fa fa-check"></i><b>8.5</b> Appendix (complete code)</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">My First Steps in Neuronal Networks<br />
(Beginners Guide)</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="effect-of-batch-size" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> Effect of batch size</h1>
<p>First and foremost, do we need to distinguish between the following definitions that I discovered <a href="https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks">here</a>:<br />
- one epoch := one forward pass and backward pass of all the training examples.
- batch size := the number of training scenarios in one forward/backward pass. The higher the batch size, the more memory space will be needed.
- number of iterations := number of passes, each pass using [batch size] number of scenarios. To be clear, one pass = forward pass + backward pass (we do not count the forward pass and backward pass as two different passes).</p>
<p>I noted in the first chapter that the standard method for defining a NN is to cycle over all training data by selecting a random scenario until all possibilities have been used, resulting in one epoch. To make it easier, I changed it to pick all scenarios at once (full batch size). But what happens if the batch size is equal to the number of rows in the training dataset?</p>
<p>Unfortunately, there is no solid proof for the differences, but I did find a neat post that tries to explain it <a href="https://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu">here</a>. As a result, optimizing with the full batch size yields sharper minimaz, whereas optimizing with a lesser batch size yields to flatter minimaz. More information about the issue of speed vs. accuracy via batch size selection may be found <a href="https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e">here</a>. He clearly outlines the issues and how switching to a dynamically growing batch size could be a smart option.</p>
<p>The underlying data determines the ideal hyperparameters like as bias, alpha, batch size, hidden neurons, and so on, according to what I’ve found so far. As a result, modifying the underlying dataset affects all of the ideal hyperparameters. Perhaps it would be more useful to test some hyperparameters on our Credit Default dataset and compare the outcomes.</p>
<div id="impact-of-diffrent-hyperparameters" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Impact of diffrent hyperparameters</h2>
<p>First of all do we add a <code>batch_size</code> parameter to the preview <code>train()</code> function and generate random batches that all contain distinct random integers in each batch. We will use the following function to generate the random batches:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="effect-of-batch-size.html#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb50-2"><a href="effect-of-batch-size.html#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> pyplot</span>
<span id="cb50-3"><a href="effect-of-batch-size.html#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb50-4"><a href="effect-of-batch-size.html#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb50-5"><a href="effect-of-batch-size.html#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math <span class="im">as</span> ma</span>
<span id="cb50-6"><a href="effect-of-batch-size.html#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb50-7"><a href="effect-of-batch-size.html#cb50-7" aria-hidden="true" tabindex="-1"></a>np.warnings.filterwarnings(<span class="st">&#39;ignore&#39;</span>, category<span class="op">=</span>np.VisibleDeprecationWarning) </span>
<span id="cb50-8"><a href="effect-of-batch-size.html#cb50-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-9"><a href="effect-of-batch-size.html#cb50-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_random_batches(batch_size, full_batch_size):</span>
<span id="cb50-10"><a href="effect-of-batch-size.html#cb50-10" aria-hidden="true" tabindex="-1"></a>  batches <span class="op">=</span> np.arange(full_batch_size)</span>
<span id="cb50-11"><a href="effect-of-batch-size.html#cb50-11" aria-hidden="true" tabindex="-1"></a>  np.random.shuffle(batches)</span>
<span id="cb50-12"><a href="effect-of-batch-size.html#cb50-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(np.array_split(batches, ma.ceil(full_batch_size<span class="op">/</span>batch_size)))</span>
<span id="cb50-13"><a href="effect-of-batch-size.html#cb50-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-14"><a href="effect-of-batch-size.html#cb50-14" aria-hidden="true" tabindex="-1"></a>generate_random_batches(<span class="dv">3</span>, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## [array([0, 8, 9]), array([4, 1, 5]), array([6, 2]), array([3, 7])]</code></pre>
<p>Now do we need to adjust the <code>train()</code> function to iterate over all batches:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="effect-of-batch-size.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(X, Y, hidden_layer_neurons, alpha, epochs, batch_size):</span>
<span id="cb52-2"><a href="effect-of-batch-size.html#cb52-2" aria-hidden="true" tabindex="-1"></a>  n_input <span class="op">=</span> <span class="bu">len</span>(X[<span class="dv">0</span>])</span>
<span id="cb52-3"><a href="effect-of-batch-size.html#cb52-3" aria-hidden="true" tabindex="-1"></a>  n_output <span class="op">=</span> <span class="bu">len</span>(Y[<span class="dv">0</span>])</span>
<span id="cb52-4"><a href="effect-of-batch-size.html#cb52-4" aria-hidden="true" tabindex="-1"></a>  W <span class="op">=</span> generate_weights(n_input, n_output, hidden_layer_neurons)</span>
<span id="cb52-5"><a href="effect-of-batch-size.html#cb52-5" aria-hidden="true" tabindex="-1"></a>  errors <span class="op">=</span> []</span>
<span id="cb52-6"><a href="effect-of-batch-size.html#cb52-6" aria-hidden="true" tabindex="-1"></a>  batches <span class="op">=</span> generate_random_batches(batch_size, full_batch_size <span class="op">=</span> <span class="bu">len</span>(X))</span>
<span id="cb52-7"><a href="effect-of-batch-size.html#cb52-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb52-8"><a href="effect-of-batch-size.html#cb52-8" aria-hidden="true" tabindex="-1"></a>    error_temp <span class="op">=</span> np.array([])</span>
<span id="cb52-9"><a href="effect-of-batch-size.html#cb52-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> z <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(batches)):</span>
<span id="cb52-10"><a href="effect-of-batch-size.html#cb52-10" aria-hidden="true" tabindex="-1"></a>      IN <span class="op">=</span> []</span>
<span id="cb52-11"><a href="effect-of-batch-size.html#cb52-11" aria-hidden="true" tabindex="-1"></a>      OUT <span class="op">=</span> []</span>
<span id="cb52-12"><a href="effect-of-batch-size.html#cb52-12" aria-hidden="true" tabindex="-1"></a>      grad <span class="op">=</span> [<span class="va">None</span>]<span class="op">*</span><span class="bu">len</span>(W)</span>
<span id="cb52-13"><a href="effect-of-batch-size.html#cb52-13" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)):</span>
<span id="cb52-14"><a href="effect-of-batch-size.html#cb52-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> k<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb52-15"><a href="effect-of-batch-size.html#cb52-15" aria-hidden="true" tabindex="-1"></a>          IN.append(add_ones_to_input(X[batches[z],:]))</span>
<span id="cb52-16"><a href="effect-of-batch-size.html#cb52-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb52-17"><a href="effect-of-batch-size.html#cb52-17" aria-hidden="true" tabindex="-1"></a>          IN.append(add_ones_to_input(OUT[k<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb52-18"><a href="effect-of-batch-size.html#cb52-18" aria-hidden="true" tabindex="-1"></a>        OUT.append(forward(x<span class="op">=</span>IN[k], w<span class="op">=</span>W[k]))</span>
<span id="cb52-19"><a href="effect-of-batch-size.html#cb52-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-20"><a href="effect-of-batch-size.html#cb52-20" aria-hidden="true" tabindex="-1"></a>      error_temp <span class="op">=</span> np.append(error_temp, Y[batches[z],:] <span class="op">-</span> OUT[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb52-21"><a href="effect-of-batch-size.html#cb52-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-22"><a href="effect-of-batch-size.html#cb52-22" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb52-23"><a href="effect-of-batch-size.html#cb52-23" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> backward(IN, OUT, W, Y[batches[z],:], grad, k) </span>
<span id="cb52-24"><a href="effect-of-batch-size.html#cb52-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb52-25"><a href="effect-of-batch-size.html#cb52-25" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)):</span>
<span id="cb52-26"><a href="effect-of-batch-size.html#cb52-26" aria-hidden="true" tabindex="-1"></a>        W[k] <span class="op">=</span> W[k] <span class="op">+</span> alpha <span class="op">*</span> (IN[k].T <span class="op">@</span> grad[k])</span>
<span id="cb52-27"><a href="effect-of-batch-size.html#cb52-27" aria-hidden="true" tabindex="-1"></a>    errors.append(error_temp)</span>
<span id="cb52-28"><a href="effect-of-batch-size.html#cb52-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb52-29"><a href="effect-of-batch-size.html#cb52-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> W, errors</span></code></pre></div>
<p>And all the previes created functions, dataloading and transformations are:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="effect-of-batch-size.html#cb53-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">&quot;example_data/credit_risk_dataset.csv&quot;</span>).fillna(<span class="dv">0</span>)</span>
<span id="cb53-2"><a href="effect-of-batch-size.html#cb53-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.replace({<span class="st">&quot;Y&quot;</span>: <span class="dv">1</span>, <span class="st">&quot;N&quot;</span>:<span class="dv">0</span>})</span>
<span id="cb53-3"><a href="effect-of-batch-size.html#cb53-3" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&quot;person_home_ownership&quot;</span>] <span class="op">=</span> data[<span class="st">&quot;person_home_ownership&quot;</span>].replace({<span class="st">&#39;OWN&#39;</span>:<span class="dv">1</span>, <span class="st">&#39;RENT&#39;</span>:<span class="dv">2</span>, <span class="st">&#39;MORTGAGE&#39;</span>:<span class="dv">3</span>, <span class="st">&#39;OTHER&#39;</span>:<span class="dv">4</span>})</span>
<span id="cb53-4"><a href="effect-of-batch-size.html#cb53-4" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&quot;loan_intent&quot;</span>] <span class="op">=</span> data[<span class="st">&quot;loan_intent&quot;</span>].replace({<span class="st">&#39;PERSONAL&#39;</span>:<span class="dv">1</span>, <span class="st">&#39;EDUCATION&#39;</span>:<span class="dv">2</span>, <span class="st">&#39;MEDICAL&#39;</span>:<span class="dv">3</span>, <span class="st">&#39;VENTURE&#39;</span>:<span class="dv">4</span>, <span class="st">&#39;HOMEIMPROVEMENT&#39;</span>:<span class="dv">5</span>,<span class="st">&#39;DEBTCONSOLIDATION&#39;</span>:<span class="dv">6</span>})</span>
<span id="cb53-5"><a href="effect-of-batch-size.html#cb53-5" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&quot;loan_grade&quot;</span>] <span class="op">=</span> data[<span class="st">&quot;loan_grade&quot;</span>].replace({<span class="st">&#39;A&#39;</span>:<span class="dv">1</span>, <span class="st">&#39;B&#39;</span>:<span class="dv">2</span>, <span class="st">&#39;C&#39;</span>:<span class="dv">3</span>, <span class="st">&#39;D&#39;</span>:<span class="dv">4</span>, <span class="st">&#39;E&#39;</span>:<span class="dv">5</span>, <span class="st">&#39;F&#39;</span>:<span class="dv">6</span>, <span class="st">&#39;G&#39;</span>:<span class="dv">7</span>})</span>
<span id="cb53-6"><a href="effect-of-batch-size.html#cb53-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-7"><a href="effect-of-batch-size.html#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> NormalizeData(np_arr):</span>
<span id="cb53-8"><a href="effect-of-batch-size.html#cb53-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(np_arr.shape[<span class="dv">1</span>]):</span>
<span id="cb53-9"><a href="effect-of-batch-size.html#cb53-9" aria-hidden="true" tabindex="-1"></a>    np_arr[:,i] <span class="op">=</span> (np_arr[:,i] <span class="op">-</span> np.<span class="bu">min</span>(np_arr[:,i])) <span class="op">/</span> (np.<span class="bu">max</span>(np_arr[:,i]) <span class="op">-</span> np.<span class="bu">min</span>(np_arr[:,i]))</span>
<span id="cb53-10"><a href="effect-of-batch-size.html#cb53-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(np_arr)</span>
<span id="cb53-11"><a href="effect-of-batch-size.html#cb53-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-12"><a href="effect-of-batch-size.html#cb53-12" aria-hidden="true" tabindex="-1"></a>training_n <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb53-13"><a href="effect-of-batch-size.html#cb53-13" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> NormalizeData( data.loc[<span class="dv">0</span>:(training_n<span class="op">-</span><span class="dv">1</span>), data.columns <span class="op">!=</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy() )</span>
<span id="cb53-14"><a href="effect-of-batch-size.html#cb53-14" aria-hidden="true" tabindex="-1"></a>Y_train <span class="op">=</span> data.loc[<span class="dv">0</span>:(training_n<span class="op">-</span><span class="dv">1</span>), data.columns <span class="op">==</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy()</span>
<span id="cb53-15"><a href="effect-of-batch-size.html#cb53-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-16"><a href="effect-of-batch-size.html#cb53-16" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> NormalizeData( data.loc[training_n:, data.columns <span class="op">!=</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy() )</span>
<span id="cb53-17"><a href="effect-of-batch-size.html#cb53-17" aria-hidden="true" tabindex="-1"></a>Y_test <span class="op">=</span> data.loc[training_n:, data.columns <span class="op">==</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy()</span>
<span id="cb53-18"><a href="effect-of-batch-size.html#cb53-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-19"><a href="effect-of-batch-size.html#cb53-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-20"><a href="effect-of-batch-size.html#cb53-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_weights(n_input, n_output, hidden_layer_neurons):</span>
<span id="cb53-21"><a href="effect-of-batch-size.html#cb53-21" aria-hidden="true" tabindex="-1"></a>  W <span class="op">=</span> []</span>
<span id="cb53-22"><a href="effect-of-batch-size.html#cb53-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(hidden_layer_neurons)<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb53-23"><a href="effect-of-batch-size.html#cb53-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>: <span class="co"># first layer</span></span>
<span id="cb53-24"><a href="effect-of-batch-size.html#cb53-24" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((n_input<span class="op">+</span><span class="dv">1</span>, hidden_layer_neurons[i])))</span>
<span id="cb53-25"><a href="effect-of-batch-size.html#cb53-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">==</span> <span class="bu">len</span>(hidden_layer_neurons): <span class="co"># last layer</span></span>
<span id="cb53-26"><a href="effect-of-batch-size.html#cb53-26" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((hidden_layer_neurons[i<span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>, n_output)))</span>
<span id="cb53-27"><a href="effect-of-batch-size.html#cb53-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: <span class="co"># middle layers</span></span>
<span id="cb53-28"><a href="effect-of-batch-size.html#cb53-28" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((hidden_layer_neurons[i<span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>, hidden_layer_neurons[i])))</span>
<span id="cb53-29"><a href="effect-of-batch-size.html#cb53-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(W)</span>
<span id="cb53-30"><a href="effect-of-batch-size.html#cb53-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-31"><a href="effect-of-batch-size.html#cb53-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_ones_to_input(x):</span>
<span id="cb53-32"><a href="effect-of-batch-size.html#cb53-32" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(np.append(x, np.array([np.ones(<span class="bu">len</span>(x))]).T, axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb53-33"><a href="effect-of-batch-size.html#cb53-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-34"><a href="effect-of-batch-size.html#cb53-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-35"><a href="effect-of-batch-size.html#cb53-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb53-36"><a href="effect-of-batch-size.html#cb53-36" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb53-37"><a href="effect-of-batch-size.html#cb53-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-38"><a href="effect-of-batch-size.html#cb53-38" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> deriv_sigmoid(x):</span>
<span id="cb53-39"><a href="effect-of-batch-size.html#cb53-39" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> x)</span>
<span id="cb53-40"><a href="effect-of-batch-size.html#cb53-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-41"><a href="effect-of-batch-size.html#cb53-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-42"><a href="effect-of-batch-size.html#cb53-42" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(x, w):</span>
<span id="cb53-43"><a href="effect-of-batch-size.html#cb53-43" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( sigmoid(x <span class="op">@</span> w) )</span>
<span id="cb53-44"><a href="effect-of-batch-size.html#cb53-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-45"><a href="effect-of-batch-size.html#cb53-45" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward(IN, OUT, W, Y, grad, k):</span>
<span id="cb53-46"><a href="effect-of-batch-size.html#cb53-46" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> k <span class="op">==</span> <span class="bu">len</span>(grad)<span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb53-47"><a href="effect-of-batch-size.html#cb53-47" aria-hidden="true" tabindex="-1"></a>    grad[k] <span class="op">=</span> deriv_sigmoid(OUT[k]) <span class="op">*</span> (Y<span class="op">-</span>OUT[k])</span>
<span id="cb53-48"><a href="effect-of-batch-size.html#cb53-48" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb53-49"><a href="effect-of-batch-size.html#cb53-49" aria-hidden="true" tabindex="-1"></a>    grad[k] <span class="op">=</span> deriv_sigmoid(OUT[k]) <span class="op">*</span>(grad[k<span class="op">+</span><span class="dv">1</span>] <span class="op">@</span> W[k<span class="op">+</span><span class="dv">1</span>][<span class="dv">0</span>:<span class="bu">len</span>(W[k<span class="op">+</span><span class="dv">1</span>])<span class="op">-</span><span class="dv">1</span>].T)</span>
<span id="cb53-50"><a href="effect-of-batch-size.html#cb53-50" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(grad)</span>
<span id="cb53-51"><a href="effect-of-batch-size.html#cb53-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-52"><a href="effect-of-batch-size.html#cb53-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-53"><a href="effect-of-batch-size.html#cb53-53" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mean_square_error(error):</span>
<span id="cb53-54"><a href="effect-of-batch-size.html#cb53-54" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(error <span class="op">**</span> <span class="dv">2</span>) )</span>
<span id="cb53-55"><a href="effect-of-batch-size.html#cb53-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-56"><a href="effect-of-batch-size.html#cb53-56" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_error(errors, title):</span>
<span id="cb53-57"><a href="effect-of-batch-size.html#cb53-57" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(errors)))</span>
<span id="cb53-58"><a href="effect-of-batch-size.html#cb53-58" aria-hidden="true" tabindex="-1"></a>  y <span class="op">=</span> np.array(errors)</span>
<span id="cb53-59"><a href="effect-of-batch-size.html#cb53-59" aria-hidden="true" tabindex="-1"></a>  pyplot.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb53-60"><a href="effect-of-batch-size.html#cb53-60" aria-hidden="true" tabindex="-1"></a>  pyplot.plot(x, y, <span class="st">&quot;g&quot;</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb53-61"><a href="effect-of-batch-size.html#cb53-61" aria-hidden="true" tabindex="-1"></a>  pyplot.xlabel(<span class="st">&quot;Iterations&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb53-62"><a href="effect-of-batch-size.html#cb53-62" aria-hidden="true" tabindex="-1"></a>  pyplot.ylabel(<span class="st">&quot;Mean Square Error&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb53-63"><a href="effect-of-batch-size.html#cb53-63" aria-hidden="true" tabindex="-1"></a>  pyplot.title(title)</span>
<span id="cb53-64"><a href="effect-of-batch-size.html#cb53-64" aria-hidden="true" tabindex="-1"></a>  pyplot.ylim(<span class="dv">0</span>,<span class="bu">max</span>(errors)<span class="op">*</span><span class="fl">1.1</span>)</span>
<span id="cb53-65"><a href="effect-of-batch-size.html#cb53-65" aria-hidden="true" tabindex="-1"></a>  pyplot.show()</span>
<span id="cb53-66"><a href="effect-of-batch-size.html#cb53-66" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb53-67"><a href="effect-of-batch-size.html#cb53-67" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test(X_test, W):</span>
<span id="cb53-68"><a href="effect-of-batch-size.html#cb53-68" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)):</span>
<span id="cb53-69"><a href="effect-of-batch-size.html#cb53-69" aria-hidden="true" tabindex="-1"></a>    X_test <span class="op">=</span> forward(add_ones_to_input(X_test), W[i])</span>
<span id="cb53-70"><a href="effect-of-batch-size.html#cb53-70" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(X_test)</span>
<span id="cb53-71"><a href="effect-of-batch-size.html#cb53-71" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb53-72"><a href="effect-of-batch-size.html#cb53-72" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> classify(Y_approx):</span>
<span id="cb53-73"><a href="effect-of-batch-size.html#cb53-73" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( np.<span class="bu">round</span>(Y_approx,<span class="dv">0</span>) )</span></code></pre></div>
<p>Everything is loaded and set up. Now can we compare the time and the error for different <code>batch_size</code>:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="effect-of-batch-size.html#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># full batch size</span></span>
<span id="cb54-2"><a href="effect-of-batch-size.html#cb54-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb54-3"><a href="effect-of-batch-size.html#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="effect-of-batch-size.html#cb54-4" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb54-5"><a href="effect-of-batch-size.html#cb54-5" aria-hidden="true" tabindex="-1"></a>W_train, errors_train <span class="op">=</span> train(X <span class="op">=</span> X_train, Y <span class="op">=</span> Y_train, hidden_layer_neurons <span class="op">=</span> [<span class="dv">11</span>,<span class="dv">4</span>], alpha <span class="op">=</span> <span class="fl">0.01</span>, epochs <span class="op">=</span> <span class="dv">5000</span>, batch_size <span class="op">=</span> <span class="dv">2000</span>)</span>
<span id="cb54-6"><a href="effect-of-batch-size.html#cb54-6" aria-hidden="true" tabindex="-1"></a>time_diff <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb54-7"><a href="effect-of-batch-size.html#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Time to train the NN: &quot;</span>, time_diff)</span>
<span id="cb54-8"><a href="effect-of-batch-size.html#cb54-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-9"><a href="effect-of-batch-size.html#cb54-9" aria-hidden="true" tabindex="-1"></a>ms_errors_train <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(mean_square_error, errors_train)))</span>
<span id="cb54-10"><a href="effect-of-batch-size.html#cb54-10" aria-hidden="true" tabindex="-1"></a>plot_error(ms_errors_train, <span class="st">&quot;MLP Credit Default&quot;</span>)</span>
<span id="cb54-11"><a href="effect-of-batch-size.html#cb54-11" aria-hidden="true" tabindex="-1"></a>result_test <span class="op">=</span> test(X_test, W_train)</span>
<span id="cb54-12"><a href="effect-of-batch-size.html#cb54-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Mean Square error over all testdata: &quot;</span>, mean_square_error(Y_test <span class="op">-</span> result_test))</span>
<span id="cb54-13"><a href="effect-of-batch-size.html#cb54-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-14"><a href="effect-of-batch-size.html#cb54-14" aria-hidden="true" tabindex="-1"></a>classified_error <span class="op">=</span> Y_test <span class="op">-</span> classify(result_test)</span>
<span id="cb54-15"><a href="effect-of-batch-size.html#cb54-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Mean Square error over all classified testdata: &quot;</span>, mean_square_error(classified_error))</span>
<span id="cb54-16"><a href="effect-of-batch-size.html#cb54-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-17"><a href="effect-of-batch-size.html#cb54-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Probability of a wrong output: &quot;</span>, np.<span class="bu">round</span>(np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(classified_error)) <span class="op">/</span> <span class="bu">len</span>(classified_error) <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>), <span class="st">&quot;%&quot;</span> )</span>
<span id="cb54-18"><a href="effect-of-batch-size.html#cb54-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Probability of a right output: &quot;</span>, np.<span class="bu">round</span>((<span class="dv">1</span> <span class="op">-</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(classified_error)) <span class="op">/</span> <span class="bu">len</span>(classified_error))<span class="op">*</span><span class="dv">100</span>,<span class="dv">2</span>),<span class="st">&quot;%&quot;</span> )</span>
<span id="cb54-19"><a href="effect-of-batch-size.html#cb54-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-20"><a href="effect-of-batch-size.html#cb54-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-21"><a href="effect-of-batch-size.html#cb54-21" aria-hidden="true" tabindex="-1"></a>confusion_matrix(Y_test, classify(result_test))</span></code></pre></div>
<pre><code>## Time to train the NN:  9.521212339401245</code></pre>
<p><img src="gitbook-demo_files/figure-html/unnamed-chunk-21-15.png" width="576" /></p>
<pre><code>## Mean Square error over all testdata:  2402.2283244767077
## Mean Square error over all classified testdata:  2932.0
## Probability of a wrong output:  19.18 %
## Probability of a right output:  80.82 %
## array([[20193,  4113],
##        [ 1751,  4524]], dtype=int64)</code></pre>
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="effect-of-batch-size.html#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># batch size = 100</span></span>
<span id="cb57-2"><a href="effect-of-batch-size.html#cb57-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb57-3"><a href="effect-of-batch-size.html#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="effect-of-batch-size.html#cb57-4" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb57-5"><a href="effect-of-batch-size.html#cb57-5" aria-hidden="true" tabindex="-1"></a>W_train, errors_train <span class="op">=</span> train(X <span class="op">=</span> X_train, Y <span class="op">=</span> Y_train, hidden_layer_neurons <span class="op">=</span> [<span class="dv">11</span>,<span class="dv">4</span>], alpha <span class="op">=</span> <span class="fl">0.01</span>, epochs <span class="op">=</span> <span class="dv">5000</span>, batch_size <span class="op">=</span> <span class="dv">100</span>)</span>
<span id="cb57-6"><a href="effect-of-batch-size.html#cb57-6" aria-hidden="true" tabindex="-1"></a>time_diff <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb57-7"><a href="effect-of-batch-size.html#cb57-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Time to train the NN: &quot;</span>, time_diff)</span>
<span id="cb57-8"><a href="effect-of-batch-size.html#cb57-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-9"><a href="effect-of-batch-size.html#cb57-9" aria-hidden="true" tabindex="-1"></a>ms_errors_train <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(mean_square_error, errors_train)))</span>
<span id="cb57-10"><a href="effect-of-batch-size.html#cb57-10" aria-hidden="true" tabindex="-1"></a>plot_error(ms_errors_train, <span class="st">&quot;MLP Credit Default&quot;</span>)</span>
<span id="cb57-11"><a href="effect-of-batch-size.html#cb57-11" aria-hidden="true" tabindex="-1"></a>result_test <span class="op">=</span> test(X_test, W_train)</span>
<span id="cb57-12"><a href="effect-of-batch-size.html#cb57-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Mean Square error over all testdata: &quot;</span>, mean_square_error(Y_test <span class="op">-</span> result_test))</span>
<span id="cb57-13"><a href="effect-of-batch-size.html#cb57-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-14"><a href="effect-of-batch-size.html#cb57-14" aria-hidden="true" tabindex="-1"></a>classified_error <span class="op">=</span> Y_test <span class="op">-</span> classify(result_test)</span>
<span id="cb57-15"><a href="effect-of-batch-size.html#cb57-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Mean Square error over all classified testdata: &quot;</span>, mean_square_error(classified_error))</span>
<span id="cb57-16"><a href="effect-of-batch-size.html#cb57-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-17"><a href="effect-of-batch-size.html#cb57-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Probability of a wrong output: &quot;</span>, np.<span class="bu">round</span>(np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(classified_error)) <span class="op">/</span> <span class="bu">len</span>(classified_error) <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>), <span class="st">&quot;%&quot;</span> )</span>
<span id="cb57-18"><a href="effect-of-batch-size.html#cb57-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Probability of a right output: &quot;</span>, np.<span class="bu">round</span>((<span class="dv">1</span> <span class="op">-</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(classified_error)) <span class="op">/</span> <span class="bu">len</span>(classified_error))<span class="op">*</span><span class="dv">100</span>,<span class="dv">2</span>),<span class="st">&quot;%&quot;</span> )</span>
<span id="cb57-19"><a href="effect-of-batch-size.html#cb57-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-20"><a href="effect-of-batch-size.html#cb57-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-21"><a href="effect-of-batch-size.html#cb57-21" aria-hidden="true" tabindex="-1"></a>confusion_matrix(Y_test, classify(result_test))</span></code></pre></div>
<pre><code>## Time to train the NN:  23.323018550872803</code></pre>
<p><img src="gitbook-demo_files/figure-html/unnamed-chunk-22-17.png" width="576" /></p>
<pre><code>## Mean Square error over all testdata:  2210.3693145823286
## Mean Square error over all classified testdata:  2559.5
## Probability of a wrong output:  16.74 %
## Probability of a right output:  83.26 %
## array([[21396,  2910],
##        [ 2209,  4066]], dtype=int64)</code></pre>
<p>The full batch size is clearly faster than the smaller batch size, yet the smaller batch size has a lower error. Perhaps the ideal batch size is determined by the problem itself. If you have a low-dimensional input matrix and need a quick answer, full batch size is the way to go. If your dimensional input is large, you won’t be able to use a complete batch size since your RAM will jump off. I believe it is important to study the underlying data and choose a batch size that is appropriate for it. For example, in the blog article I cited at the beginning of the chapter, some situations may require a dynamic batch size that lowers over time to provide the best outcomes.</p>
</div>
<div id="appendix-complete-code-4" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Appendix (complete code)</h2>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="effect-of-batch-size.html#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb60-2"><a href="effect-of-batch-size.html#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> pyplot</span>
<span id="cb60-3"><a href="effect-of-batch-size.html#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb60-4"><a href="effect-of-batch-size.html#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb60-5"><a href="effect-of-batch-size.html#cb60-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math <span class="im">as</span> ma</span>
<span id="cb60-6"><a href="effect-of-batch-size.html#cb60-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb60-7"><a href="effect-of-batch-size.html#cb60-7" aria-hidden="true" tabindex="-1"></a>np.warnings.filterwarnings(<span class="st">&#39;ignore&#39;</span>, category<span class="op">=</span>np.VisibleDeprecationWarning) </span>
<span id="cb60-8"><a href="effect-of-batch-size.html#cb60-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-9"><a href="effect-of-batch-size.html#cb60-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">&quot;example_data/credit_risk_dataset.csv&quot;</span>).fillna(<span class="dv">0</span>)</span>
<span id="cb60-10"><a href="effect-of-batch-size.html#cb60-10" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.replace({<span class="st">&quot;Y&quot;</span>: <span class="dv">1</span>, <span class="st">&quot;N&quot;</span>:<span class="dv">0</span>})</span>
<span id="cb60-11"><a href="effect-of-batch-size.html#cb60-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-12"><a href="effect-of-batch-size.html#cb60-12" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&quot;person_home_ownership&quot;</span>] <span class="op">=</span> data[<span class="st">&quot;person_home_ownership&quot;</span>].replace({<span class="st">&#39;OWN&#39;</span>:<span class="dv">1</span>, <span class="st">&#39;RENT&#39;</span>:<span class="dv">2</span>, <span class="st">&#39;MORTGAGE&#39;</span>:<span class="dv">3</span>, <span class="st">&#39;OTHER&#39;</span>:<span class="dv">4</span>})</span>
<span id="cb60-13"><a href="effect-of-batch-size.html#cb60-13" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&quot;loan_intent&quot;</span>] <span class="op">=</span> data[<span class="st">&quot;loan_intent&quot;</span>].replace({<span class="st">&#39;PERSONAL&#39;</span>:<span class="dv">1</span>, <span class="st">&#39;EDUCATION&#39;</span>:<span class="dv">2</span>, <span class="st">&#39;MEDICAL&#39;</span>:<span class="dv">3</span>, <span class="st">&#39;VENTURE&#39;</span>:<span class="dv">4</span>, <span class="st">&#39;HOMEIMPROVEMENT&#39;</span>:<span class="dv">5</span>,<span class="st">&#39;DEBTCONSOLIDATION&#39;</span>:<span class="dv">6</span>})</span>
<span id="cb60-14"><a href="effect-of-batch-size.html#cb60-14" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&quot;loan_grade&quot;</span>] <span class="op">=</span> data[<span class="st">&quot;loan_grade&quot;</span>].replace({<span class="st">&#39;A&#39;</span>:<span class="dv">1</span>, <span class="st">&#39;B&#39;</span>:<span class="dv">2</span>, <span class="st">&#39;C&#39;</span>:<span class="dv">3</span>, <span class="st">&#39;D&#39;</span>:<span class="dv">4</span>, <span class="st">&#39;E&#39;</span>:<span class="dv">5</span>, <span class="st">&#39;F&#39;</span>:<span class="dv">6</span>, <span class="st">&#39;G&#39;</span>:<span class="dv">7</span>})</span>
<span id="cb60-15"><a href="effect-of-batch-size.html#cb60-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-16"><a href="effect-of-batch-size.html#cb60-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-17"><a href="effect-of-batch-size.html#cb60-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> NormalizeData(np_arr):</span>
<span id="cb60-18"><a href="effect-of-batch-size.html#cb60-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(np_arr.shape[<span class="dv">1</span>]):</span>
<span id="cb60-19"><a href="effect-of-batch-size.html#cb60-19" aria-hidden="true" tabindex="-1"></a>    np_arr[:,i] <span class="op">=</span> (np_arr[:,i] <span class="op">-</span> np.<span class="bu">min</span>(np_arr[:,i])) <span class="op">/</span> (np.<span class="bu">max</span>(np_arr[:,i]) <span class="op">-</span> np.<span class="bu">min</span>(np_arr[:,i]))</span>
<span id="cb60-20"><a href="effect-of-batch-size.html#cb60-20" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(np_arr)</span>
<span id="cb60-21"><a href="effect-of-batch-size.html#cb60-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-22"><a href="effect-of-batch-size.html#cb60-22" aria-hidden="true" tabindex="-1"></a>training_n <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb60-23"><a href="effect-of-batch-size.html#cb60-23" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> NormalizeData( data.loc[<span class="dv">0</span>:(training_n<span class="op">-</span><span class="dv">1</span>), data.columns <span class="op">!=</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy() )</span>
<span id="cb60-24"><a href="effect-of-batch-size.html#cb60-24" aria-hidden="true" tabindex="-1"></a>Y_train <span class="op">=</span> data.loc[<span class="dv">0</span>:(training_n<span class="op">-</span><span class="dv">1</span>), data.columns <span class="op">==</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy()</span>
<span id="cb60-25"><a href="effect-of-batch-size.html#cb60-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-26"><a href="effect-of-batch-size.html#cb60-26" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> NormalizeData( data.loc[training_n:, data.columns <span class="op">!=</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy() )</span>
<span id="cb60-27"><a href="effect-of-batch-size.html#cb60-27" aria-hidden="true" tabindex="-1"></a>Y_test <span class="op">=</span> data.loc[training_n:, data.columns <span class="op">==</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy()</span>
<span id="cb60-28"><a href="effect-of-batch-size.html#cb60-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-29"><a href="effect-of-batch-size.html#cb60-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-30"><a href="effect-of-batch-size.html#cb60-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-31"><a href="effect-of-batch-size.html#cb60-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_weights(n_input, n_output, hidden_layer_neurons):</span>
<span id="cb60-32"><a href="effect-of-batch-size.html#cb60-32" aria-hidden="true" tabindex="-1"></a>  W <span class="op">=</span> []</span>
<span id="cb60-33"><a href="effect-of-batch-size.html#cb60-33" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(hidden_layer_neurons)<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb60-34"><a href="effect-of-batch-size.html#cb60-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>: <span class="co"># first layer</span></span>
<span id="cb60-35"><a href="effect-of-batch-size.html#cb60-35" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((n_input<span class="op">+</span><span class="dv">1</span>, hidden_layer_neurons[i])))</span>
<span id="cb60-36"><a href="effect-of-batch-size.html#cb60-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">==</span> <span class="bu">len</span>(hidden_layer_neurons): <span class="co"># last layer</span></span>
<span id="cb60-37"><a href="effect-of-batch-size.html#cb60-37" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((hidden_layer_neurons[i<span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>, n_output)))</span>
<span id="cb60-38"><a href="effect-of-batch-size.html#cb60-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: <span class="co"># middle layers</span></span>
<span id="cb60-39"><a href="effect-of-batch-size.html#cb60-39" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((hidden_layer_neurons[i<span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>, hidden_layer_neurons[i])))</span>
<span id="cb60-40"><a href="effect-of-batch-size.html#cb60-40" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(W)</span>
<span id="cb60-41"><a href="effect-of-batch-size.html#cb60-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-42"><a href="effect-of-batch-size.html#cb60-42" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_ones_to_input(x):</span>
<span id="cb60-43"><a href="effect-of-batch-size.html#cb60-43" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(np.append(x, np.array([np.ones(<span class="bu">len</span>(x))]).T, axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb60-44"><a href="effect-of-batch-size.html#cb60-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-45"><a href="effect-of-batch-size.html#cb60-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-46"><a href="effect-of-batch-size.html#cb60-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-47"><a href="effect-of-batch-size.html#cb60-47" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb60-48"><a href="effect-of-batch-size.html#cb60-48" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb60-49"><a href="effect-of-batch-size.html#cb60-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-50"><a href="effect-of-batch-size.html#cb60-50" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> deriv_sigmoid(x):</span>
<span id="cb60-51"><a href="effect-of-batch-size.html#cb60-51" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> x)</span>
<span id="cb60-52"><a href="effect-of-batch-size.html#cb60-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-53"><a href="effect-of-batch-size.html#cb60-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-54"><a href="effect-of-batch-size.html#cb60-54" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(x, w):</span>
<span id="cb60-55"><a href="effect-of-batch-size.html#cb60-55" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( sigmoid(x <span class="op">@</span> w) )</span>
<span id="cb60-56"><a href="effect-of-batch-size.html#cb60-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-57"><a href="effect-of-batch-size.html#cb60-57" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward(IN, OUT, W, Y, grad, k):</span>
<span id="cb60-58"><a href="effect-of-batch-size.html#cb60-58" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> k <span class="op">==</span> <span class="bu">len</span>(grad)<span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb60-59"><a href="effect-of-batch-size.html#cb60-59" aria-hidden="true" tabindex="-1"></a>    grad[k] <span class="op">=</span> deriv_sigmoid(OUT[k]) <span class="op">*</span> (Y<span class="op">-</span>OUT[k])</span>
<span id="cb60-60"><a href="effect-of-batch-size.html#cb60-60" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb60-61"><a href="effect-of-batch-size.html#cb60-61" aria-hidden="true" tabindex="-1"></a>    grad[k] <span class="op">=</span> deriv_sigmoid(OUT[k]) <span class="op">*</span>(grad[k<span class="op">+</span><span class="dv">1</span>] <span class="op">@</span> W[k<span class="op">+</span><span class="dv">1</span>][<span class="dv">0</span>:<span class="bu">len</span>(W[k<span class="op">+</span><span class="dv">1</span>])<span class="op">-</span><span class="dv">1</span>].T)</span>
<span id="cb60-62"><a href="effect-of-batch-size.html#cb60-62" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(grad)</span>
<span id="cb60-63"><a href="effect-of-batch-size.html#cb60-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-64"><a href="effect-of-batch-size.html#cb60-64" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_random_batches(batch_size, full_batch_size):</span>
<span id="cb60-65"><a href="effect-of-batch-size.html#cb60-65" aria-hidden="true" tabindex="-1"></a>  batches <span class="op">=</span> np.arange(full_batch_size)</span>
<span id="cb60-66"><a href="effect-of-batch-size.html#cb60-66" aria-hidden="true" tabindex="-1"></a>  np.random.shuffle(batches)</span>
<span id="cb60-67"><a href="effect-of-batch-size.html#cb60-67" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(np.array_split(batches, ma.ceil(full_batch_size<span class="op">/</span>batch_size)))</span>
<span id="cb60-68"><a href="effect-of-batch-size.html#cb60-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-69"><a href="effect-of-batch-size.html#cb60-69" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(X, Y, hidden_layer_neurons, alpha, epochs, batch_size):</span>
<span id="cb60-70"><a href="effect-of-batch-size.html#cb60-70" aria-hidden="true" tabindex="-1"></a>  n_input <span class="op">=</span> <span class="bu">len</span>(X[<span class="dv">0</span>])</span>
<span id="cb60-71"><a href="effect-of-batch-size.html#cb60-71" aria-hidden="true" tabindex="-1"></a>  n_output <span class="op">=</span> <span class="bu">len</span>(Y[<span class="dv">0</span>])</span>
<span id="cb60-72"><a href="effect-of-batch-size.html#cb60-72" aria-hidden="true" tabindex="-1"></a>  W <span class="op">=</span> generate_weights(n_input, n_output, hidden_layer_neurons)</span>
<span id="cb60-73"><a href="effect-of-batch-size.html#cb60-73" aria-hidden="true" tabindex="-1"></a>  errors <span class="op">=</span> []</span>
<span id="cb60-74"><a href="effect-of-batch-size.html#cb60-74" aria-hidden="true" tabindex="-1"></a>  batches <span class="op">=</span> generate_random_batches(batch_size, full_batch_size <span class="op">=</span> <span class="bu">len</span>(X))</span>
<span id="cb60-75"><a href="effect-of-batch-size.html#cb60-75" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb60-76"><a href="effect-of-batch-size.html#cb60-76" aria-hidden="true" tabindex="-1"></a>    error_temp <span class="op">=</span> np.array([])</span>
<span id="cb60-77"><a href="effect-of-batch-size.html#cb60-77" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> z <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(batches)):</span>
<span id="cb60-78"><a href="effect-of-batch-size.html#cb60-78" aria-hidden="true" tabindex="-1"></a>      IN <span class="op">=</span> []</span>
<span id="cb60-79"><a href="effect-of-batch-size.html#cb60-79" aria-hidden="true" tabindex="-1"></a>      OUT <span class="op">=</span> []</span>
<span id="cb60-80"><a href="effect-of-batch-size.html#cb60-80" aria-hidden="true" tabindex="-1"></a>      grad <span class="op">=</span> [<span class="va">None</span>]<span class="op">*</span><span class="bu">len</span>(W)</span>
<span id="cb60-81"><a href="effect-of-batch-size.html#cb60-81" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)):</span>
<span id="cb60-82"><a href="effect-of-batch-size.html#cb60-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> k<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb60-83"><a href="effect-of-batch-size.html#cb60-83" aria-hidden="true" tabindex="-1"></a>          IN.append(add_ones_to_input(X[batches[z],:]))</span>
<span id="cb60-84"><a href="effect-of-batch-size.html#cb60-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb60-85"><a href="effect-of-batch-size.html#cb60-85" aria-hidden="true" tabindex="-1"></a>          IN.append(add_ones_to_input(OUT[k<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb60-86"><a href="effect-of-batch-size.html#cb60-86" aria-hidden="true" tabindex="-1"></a>        OUT.append(forward(x<span class="op">=</span>IN[k], w<span class="op">=</span>W[k]))</span>
<span id="cb60-87"><a href="effect-of-batch-size.html#cb60-87" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-88"><a href="effect-of-batch-size.html#cb60-88" aria-hidden="true" tabindex="-1"></a>      error_temp <span class="op">=</span> np.append(error_temp, Y[batches[z],:] <span class="op">-</span> OUT[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb60-89"><a href="effect-of-batch-size.html#cb60-89" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-90"><a href="effect-of-batch-size.html#cb60-90" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb60-91"><a href="effect-of-batch-size.html#cb60-91" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> backward(IN, OUT, W, Y[batches[z],:], grad, k) </span>
<span id="cb60-92"><a href="effect-of-batch-size.html#cb60-92" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb60-93"><a href="effect-of-batch-size.html#cb60-93" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)):</span>
<span id="cb60-94"><a href="effect-of-batch-size.html#cb60-94" aria-hidden="true" tabindex="-1"></a>        W[k] <span class="op">=</span> W[k] <span class="op">+</span> alpha <span class="op">*</span> (IN[k].T <span class="op">@</span> grad[k])</span>
<span id="cb60-95"><a href="effect-of-batch-size.html#cb60-95" aria-hidden="true" tabindex="-1"></a>    errors.append(error_temp)</span>
<span id="cb60-96"><a href="effect-of-batch-size.html#cb60-96" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb60-97"><a href="effect-of-batch-size.html#cb60-97" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> W, errors</span>
<span id="cb60-98"><a href="effect-of-batch-size.html#cb60-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-99"><a href="effect-of-batch-size.html#cb60-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-100"><a href="effect-of-batch-size.html#cb60-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-101"><a href="effect-of-batch-size.html#cb60-101" aria-hidden="true" tabindex="-1"></a>W_train, errors_train <span class="op">=</span> train(X <span class="op">=</span> X_train, Y <span class="op">=</span> Y_train, hidden_layer_neurons <span class="op">=</span> [<span class="dv">11</span>,<span class="dv">4</span>], alpha <span class="op">=</span> <span class="fl">0.01</span>, epochs <span class="op">=</span> <span class="dv">2000</span>, batch_size <span class="op">=</span> <span class="dv">2000</span>)</span>
<span id="cb60-102"><a href="effect-of-batch-size.html#cb60-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-103"><a href="effect-of-batch-size.html#cb60-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-104"><a href="effect-of-batch-size.html#cb60-104" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mean_square_error(error):</span>
<span id="cb60-105"><a href="effect-of-batch-size.html#cb60-105" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(error <span class="op">**</span> <span class="dv">2</span>) )</span>
<span id="cb60-106"><a href="effect-of-batch-size.html#cb60-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-107"><a href="effect-of-batch-size.html#cb60-107" aria-hidden="true" tabindex="-1"></a>ms_errors_train <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(mean_square_error, errors_train)))</span>
<span id="cb60-108"><a href="effect-of-batch-size.html#cb60-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-109"><a href="effect-of-batch-size.html#cb60-109" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_error(errors, title):</span>
<span id="cb60-110"><a href="effect-of-batch-size.html#cb60-110" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(errors)))</span>
<span id="cb60-111"><a href="effect-of-batch-size.html#cb60-111" aria-hidden="true" tabindex="-1"></a>  y <span class="op">=</span> np.array(errors)</span>
<span id="cb60-112"><a href="effect-of-batch-size.html#cb60-112" aria-hidden="true" tabindex="-1"></a>  pyplot.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb60-113"><a href="effect-of-batch-size.html#cb60-113" aria-hidden="true" tabindex="-1"></a>  pyplot.plot(x, y, <span class="st">&quot;g&quot;</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb60-114"><a href="effect-of-batch-size.html#cb60-114" aria-hidden="true" tabindex="-1"></a>  pyplot.xlabel(<span class="st">&quot;Iterations&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb60-115"><a href="effect-of-batch-size.html#cb60-115" aria-hidden="true" tabindex="-1"></a>  pyplot.ylabel(<span class="st">&quot;Mean Square Error&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb60-116"><a href="effect-of-batch-size.html#cb60-116" aria-hidden="true" tabindex="-1"></a>  pyplot.title(title)</span>
<span id="cb60-117"><a href="effect-of-batch-size.html#cb60-117" aria-hidden="true" tabindex="-1"></a>  pyplot.ylim(<span class="dv">0</span>,<span class="bu">max</span>(errors)<span class="op">*</span><span class="fl">1.1</span>)</span>
<span id="cb60-118"><a href="effect-of-batch-size.html#cb60-118" aria-hidden="true" tabindex="-1"></a>  pyplot.show()</span>
<span id="cb60-119"><a href="effect-of-batch-size.html#cb60-119" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb60-120"><a href="effect-of-batch-size.html#cb60-120" aria-hidden="true" tabindex="-1"></a>plot_error(ms_errors_train, <span class="st">&quot;MLP Credit Default&quot;</span>)</span>
<span id="cb60-121"><a href="effect-of-batch-size.html#cb60-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-122"><a href="effect-of-batch-size.html#cb60-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-123"><a href="effect-of-batch-size.html#cb60-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-124"><a href="effect-of-batch-size.html#cb60-124" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test(X_test, W):</span>
<span id="cb60-125"><a href="effect-of-batch-size.html#cb60-125" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)):</span>
<span id="cb60-126"><a href="effect-of-batch-size.html#cb60-126" aria-hidden="true" tabindex="-1"></a>    X_test <span class="op">=</span> forward(add_ones_to_input(X_test), W[i])</span>
<span id="cb60-127"><a href="effect-of-batch-size.html#cb60-127" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(X_test)</span>
<span id="cb60-128"><a href="effect-of-batch-size.html#cb60-128" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb60-129"><a href="effect-of-batch-size.html#cb60-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-130"><a href="effect-of-batch-size.html#cb60-130" aria-hidden="true" tabindex="-1"></a>result_test <span class="op">=</span> test(X_test, W_train)</span>
<span id="cb60-131"><a href="effect-of-batch-size.html#cb60-131" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Mean Square error over all testdata: &quot;</span>, mean_square_error(Y_test <span class="op">-</span> result_test))</span>
<span id="cb60-132"><a href="effect-of-batch-size.html#cb60-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-133"><a href="effect-of-batch-size.html#cb60-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-134"><a href="effect-of-batch-size.html#cb60-134" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> classify(Y_approx):</span>
<span id="cb60-135"><a href="effect-of-batch-size.html#cb60-135" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( np.<span class="bu">round</span>(Y_approx,<span class="dv">0</span>) )</span>
<span id="cb60-136"><a href="effect-of-batch-size.html#cb60-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-137"><a href="effect-of-batch-size.html#cb60-137" aria-hidden="true" tabindex="-1"></a>classified_error <span class="op">=</span> Y_test <span class="op">-</span> classify(result_test)</span>
<span id="cb60-138"><a href="effect-of-batch-size.html#cb60-138" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Mean Square error over all classified testdata: &quot;</span>, mean_square_error(classified_error))</span>
<span id="cb60-139"><a href="effect-of-batch-size.html#cb60-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-140"><a href="effect-of-batch-size.html#cb60-140" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Probability of a wrong output: &quot;</span>, np.<span class="bu">round</span>(np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(classified_error)) <span class="op">/</span> <span class="bu">len</span>(classified_error) <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>), <span class="st">&quot;%&quot;</span> )</span>
<span id="cb60-141"><a href="effect-of-batch-size.html#cb60-141" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Probability of a right output: &quot;</span>, np.<span class="bu">round</span>((<span class="dv">1</span> <span class="op">-</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(classified_error)) <span class="op">/</span> <span class="bu">len</span>(classified_error))<span class="op">*</span><span class="dv">100</span>,<span class="dv">2</span>),<span class="st">&quot;%&quot;</span> )</span>
<span id="cb60-142"><a href="effect-of-batch-size.html#cb60-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-143"><a href="effect-of-batch-size.html#cb60-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-144"><a href="effect-of-batch-size.html#cb60-144" aria-hidden="true" tabindex="-1"></a>confusion_matrix(Y_test, classify(result_test))</span></code></pre></div>
<p><img src="gitbook-demo_files/figure-html/unnamed-chunk-23-19.png" width="576" /></p>
<pre><code>## Mean Square error over all testdata:  2330.8698549481937
## Mean Square error over all classified testdata:  2940.0
## Probability of a wrong output:  19.23 %
## Probability of a right output:  80.77 %
## array([[20516,  3790],
##        [ 2090,  4185]], dtype=int64)</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mlp-example-credit-default.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="class-mlp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/AxelCode-R/GitBook/edit/master/05_Batch_Size.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["gitbook-demo.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
