<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Effect of batch size | gitbook-demo.knit</title>
  <meta name="description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Effect of batch size | gitbook-demo.knit" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  <meta name="github-repo" content="https://github.com/AxelCode-R/GitBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Effect of batch size | gitbook-demo.knit" />
  
  <meta name="twitter:description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  

<meta name="author" content="Axel Roth" />


<meta name="date" content="2021-11-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mlp-example-credit-default.html"/>
<link rel="next" href="class-mlp.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A GitBook for Teaching</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#me"><i class="fa fa-check"></i><b>1.1</b> Me</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-book"><i class="fa fa-check"></i><b>1.2</b> The Book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#how-it-works"><i class="fa fa-check"></i><b>1.3</b> How it works</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="single-perceptron.html"><a href="single-perceptron.html"><i class="fa fa-check"></i><b>2</b> Single Perceptron</a>
<ul>
<li class="chapter" data-level="2.1" data-path="single-perceptron.html"><a href="single-perceptron.html#neural-network-basics"><i class="fa fa-check"></i><b>2.1</b> Neural Network Basics</a></li>
<li class="chapter" data-level="2.2" data-path="single-perceptron.html"><a href="single-perceptron.html#forward-pass"><i class="fa fa-check"></i><b>2.2</b> Forward pass</a></li>
<li class="chapter" data-level="2.3" data-path="single-perceptron.html"><a href="single-perceptron.html#backward-pass"><i class="fa fa-check"></i><b>2.3</b> Backward pass</a></li>
<li class="chapter" data-level="2.4" data-path="single-perceptron.html"><a href="single-perceptron.html#single-perceptron-1"><i class="fa fa-check"></i><b>2.4</b> Single Perceptron</a></li>
<li class="chapter" data-level="2.5" data-path="single-perceptron.html"><a href="single-perceptron.html#why-does-it-work"><i class="fa fa-check"></i><b>2.5</b> Why does it work?</a></li>
<li class="chapter" data-level="2.6" data-path="single-perceptron.html"><a href="single-perceptron.html#appendix-complete-code"><i class="fa fa-check"></i><b>2.6</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="adding-trainable-bias.html"><a href="adding-trainable-bias.html"><i class="fa fa-check"></i><b>3</b> Adding trainable Bias</a>
<ul>
<li class="chapter" data-level="3.1" data-path="adding-trainable-bias.html"><a href="adding-trainable-bias.html#generalising-the-bias"><i class="fa fa-check"></i><b>3.1</b> Generalising the Bias</a></li>
<li class="chapter" data-level="3.2" data-path="adding-trainable-bias.html"><a href="adding-trainable-bias.html#appendix-complete-code-1"><i class="fa fa-check"></i><b>3.2</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multi-layer-perceptrons-mlp.html"><a href="multi-layer-perceptrons-mlp.html"><i class="fa fa-check"></i><b>4</b> Multi Layer Perceptrons (MLP)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multi-layer-perceptrons-mlp.html"><a href="multi-layer-perceptrons-mlp.html#forward-pass-1"><i class="fa fa-check"></i><b>4.1</b> Forward pass</a></li>
<li class="chapter" data-level="4.2" data-path="multi-layer-perceptrons-mlp.html"><a href="multi-layer-perceptrons-mlp.html#backward-pass-1"><i class="fa fa-check"></i><b>4.2</b> Backward pass</a></li>
<li class="chapter" data-level="4.3" data-path="multi-layer-perceptrons-mlp.html"><a href="multi-layer-perceptrons-mlp.html#appendix-complete-code-2"><i class="fa fa-check"></i><b>4.3</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html"><i class="fa fa-check"></i><b>5</b> MLP example (Credit Default)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html#loading-and-analysing-the-data"><i class="fa fa-check"></i><b>5.1</b> Loading and analysing the data</a></li>
<li class="chapter" data-level="5.2" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html#train-and-test-phase"><i class="fa fa-check"></i><b>5.2</b> Train and test phase</a></li>
<li class="chapter" data-level="5.3" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html#appendix-complete-code-3"><i class="fa fa-check"></i><b>5.3</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="effect-of-batch-size.html"><a href="effect-of-batch-size.html"><i class="fa fa-check"></i><b>6</b> Effect of batch size</a>
<ul>
<li class="chapter" data-level="6.1" data-path="effect-of-batch-size.html"><a href="effect-of-batch-size.html#impact-of-diffrent-hyperparameters"><i class="fa fa-check"></i><b>6.1</b> Impact of diffrent hyperparameters</a></li>
<li class="chapter" data-level="6.2" data-path="effect-of-batch-size.html"><a href="effect-of-batch-size.html#appendix-complete-code-4"><i class="fa fa-check"></i><b>6.2</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="class-mlp.html"><a href="class-mlp.html"><i class="fa fa-check"></i><b>7</b> Class MLP</a></li>
<li class="chapter" data-level="8" data-path="decision-tree.html"><a href="decision-tree.html"><i class="fa fa-check"></i><b>8</b> Decision Tree</a>
<ul>
<li class="chapter" data-level="8.1" data-path="decision-tree.html"><a href="decision-tree.html#entropy"><i class="fa fa-check"></i><b>8.1</b> Entropy</a></li>
<li class="chapter" data-level="8.2" data-path="decision-tree.html"><a href="decision-tree.html#constructing-the-tree"><i class="fa fa-check"></i><b>8.2</b> Constructing the Tree</a></li>
<li class="chapter" data-level="8.3" data-path="decision-tree.html"><a href="decision-tree.html#forcast-credit-defaults-with-dt"><i class="fa fa-check"></i><b>8.3</b> Forcast Credit Defaults with DT</a></li>
<li class="chapter" data-level="8.4" data-path="decision-tree.html"><a href="decision-tree.html#extern-packages"><i class="fa fa-check"></i><b>8.4</b> Extern Packages</a></li>
<li class="chapter" data-level="8.5" data-path="decision-tree.html"><a href="decision-tree.html#appendix-complete-code-5"><i class="fa fa-check"></i><b>8.5</b> Appendix (complete code)</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">My First Steps in Neuronal Networks<br />
(Beginners Guide)</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="effect-of-batch-size" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> Effect of batch size</h1>
<p>First of all do we need to differentiate between the following definitions that i found <a href="https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks">here</a>:<br />
- one epoch := one forward pass and backward pass of all the training examples.<br />
- batch size := the number of training scenarios in one forward/backward pass. The higher the batch size, the more memory space will be needed.<br />
- number of iterations := number of passes, each pass using [batch size] number of scenarios. To be clear, one pass = forward pass + backward pass (we do not count the forward pass and backward pass as two different passes).</p>
<p>In the first chapter i mentioned that the normal approach to define a NN is to iterate over all training data by selecting a random scenario, until all scenarios are used ones, to create one epoch. I changed it to select all scenarios at the same time to make it simpler (full batch size). But what consequences do we get by setting the batch size to the number of rows in the training dataset?</p>
<p>Sadly there dosent exist a real proof for the differences, but i found a neat post that tries to explain its behavior <a href="https://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu">here</a>. The result is that considering the full batch size will lead to sharper minimaz in the optimization and a smaller batch size leads to flatter minimaz. If you want more information about the problem of speed vs accuracy by selecting the batch size can be found <a href="https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e">here</a>. He explains very well where the problems are and that switching to a dynamic growing batch size could be a good solution.</p>
<p>All the information i found until now, points out that the underlying data determinates the optimal hyperparameters like bias, alpha, batch size, hidden neurons and so on. That means that changing the underlying dataset will change all the optimal hyperparameters as well. Maybe its better to test some hyperparameters for our Credit Default dataset and visualize the difference in the results.</p>
<div id="impact-of-diffrent-hyperparameters" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Impact of diffrent hyperparameters</h2>
<p>First of all do we add a <code>batch_size</code> parameter to the preview <code>train()</code> function and generate random batches that all contain distinct random integers in each batch. We will use the following function to generate the random batches:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="effect-of-batch-size.html#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-2"><a href="effect-of-batch-size.html#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> pyplot</span>
<span id="cb49-3"><a href="effect-of-batch-size.html#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb49-4"><a href="effect-of-batch-size.html#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb49-5"><a href="effect-of-batch-size.html#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math <span class="im">as</span> ma</span>
<span id="cb49-6"><a href="effect-of-batch-size.html#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb49-7"><a href="effect-of-batch-size.html#cb49-7" aria-hidden="true" tabindex="-1"></a>np.warnings.filterwarnings(<span class="st">&#39;ignore&#39;</span>, category<span class="op">=</span>np.VisibleDeprecationWarning) </span>
<span id="cb49-8"><a href="effect-of-batch-size.html#cb49-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-9"><a href="effect-of-batch-size.html#cb49-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_random_batches(batch_size, full_batch_size):</span>
<span id="cb49-10"><a href="effect-of-batch-size.html#cb49-10" aria-hidden="true" tabindex="-1"></a>  batches <span class="op">=</span> np.arange(full_batch_size)</span>
<span id="cb49-11"><a href="effect-of-batch-size.html#cb49-11" aria-hidden="true" tabindex="-1"></a>  np.random.shuffle(batches)</span>
<span id="cb49-12"><a href="effect-of-batch-size.html#cb49-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(np.array_split(batches, ma.ceil(full_batch_size<span class="op">/</span>batch_size)))</span>
<span id="cb49-13"><a href="effect-of-batch-size.html#cb49-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-14"><a href="effect-of-batch-size.html#cb49-14" aria-hidden="true" tabindex="-1"></a>generate_random_batches(<span class="dv">3</span>, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## [array([0, 8, 9]), array([4, 1, 5]), array([6, 2]), array([3, 7])]</code></pre>
<p>Now do we need to adjust the <code>train()</code> function to iterate over all batches:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="effect-of-batch-size.html#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(X, Y, hidden_layer_neurons, alpha, epochs, batch_size):</span>
<span id="cb51-2"><a href="effect-of-batch-size.html#cb51-2" aria-hidden="true" tabindex="-1"></a>  n_input <span class="op">=</span> <span class="bu">len</span>(X[<span class="dv">0</span>])</span>
<span id="cb51-3"><a href="effect-of-batch-size.html#cb51-3" aria-hidden="true" tabindex="-1"></a>  n_output <span class="op">=</span> <span class="bu">len</span>(Y[<span class="dv">0</span>])</span>
<span id="cb51-4"><a href="effect-of-batch-size.html#cb51-4" aria-hidden="true" tabindex="-1"></a>  W <span class="op">=</span> generate_weights(n_input, n_output, hidden_layer_neurons)</span>
<span id="cb51-5"><a href="effect-of-batch-size.html#cb51-5" aria-hidden="true" tabindex="-1"></a>  errors <span class="op">=</span> []</span>
<span id="cb51-6"><a href="effect-of-batch-size.html#cb51-6" aria-hidden="true" tabindex="-1"></a>  batches <span class="op">=</span> generate_random_batches(batch_size, full_batch_size <span class="op">=</span> <span class="bu">len</span>(X))</span>
<span id="cb51-7"><a href="effect-of-batch-size.html#cb51-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb51-8"><a href="effect-of-batch-size.html#cb51-8" aria-hidden="true" tabindex="-1"></a>    error_temp <span class="op">=</span> np.array([])</span>
<span id="cb51-9"><a href="effect-of-batch-size.html#cb51-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> z <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(batches)):</span>
<span id="cb51-10"><a href="effect-of-batch-size.html#cb51-10" aria-hidden="true" tabindex="-1"></a>      IN <span class="op">=</span> []</span>
<span id="cb51-11"><a href="effect-of-batch-size.html#cb51-11" aria-hidden="true" tabindex="-1"></a>      OUT <span class="op">=</span> []</span>
<span id="cb51-12"><a href="effect-of-batch-size.html#cb51-12" aria-hidden="true" tabindex="-1"></a>      grad <span class="op">=</span> [<span class="va">None</span>]<span class="op">*</span><span class="bu">len</span>(W)</span>
<span id="cb51-13"><a href="effect-of-batch-size.html#cb51-13" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)):</span>
<span id="cb51-14"><a href="effect-of-batch-size.html#cb51-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> k<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb51-15"><a href="effect-of-batch-size.html#cb51-15" aria-hidden="true" tabindex="-1"></a>          IN.append(add_ones_to_input(X[batches[z],:]))</span>
<span id="cb51-16"><a href="effect-of-batch-size.html#cb51-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb51-17"><a href="effect-of-batch-size.html#cb51-17" aria-hidden="true" tabindex="-1"></a>          IN.append(add_ones_to_input(OUT[k<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb51-18"><a href="effect-of-batch-size.html#cb51-18" aria-hidden="true" tabindex="-1"></a>        OUT.append(forward(x<span class="op">=</span>IN[k], w<span class="op">=</span>W[k]))</span>
<span id="cb51-19"><a href="effect-of-batch-size.html#cb51-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb51-20"><a href="effect-of-batch-size.html#cb51-20" aria-hidden="true" tabindex="-1"></a>      error_temp <span class="op">=</span> np.append(error_temp, Y[batches[z],:] <span class="op">-</span> OUT[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb51-21"><a href="effect-of-batch-size.html#cb51-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb51-22"><a href="effect-of-batch-size.html#cb51-22" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb51-23"><a href="effect-of-batch-size.html#cb51-23" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> backward(IN, OUT, W, Y[batches[z],:], grad, k) </span>
<span id="cb51-24"><a href="effect-of-batch-size.html#cb51-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb51-25"><a href="effect-of-batch-size.html#cb51-25" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)):</span>
<span id="cb51-26"><a href="effect-of-batch-size.html#cb51-26" aria-hidden="true" tabindex="-1"></a>        W[k] <span class="op">=</span> W[k] <span class="op">+</span> alpha <span class="op">*</span> (IN[k].T <span class="op">@</span> grad[k])</span>
<span id="cb51-27"><a href="effect-of-batch-size.html#cb51-27" aria-hidden="true" tabindex="-1"></a>    errors.append(error_temp)</span>
<span id="cb51-28"><a href="effect-of-batch-size.html#cb51-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-29"><a href="effect-of-batch-size.html#cb51-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> W, errors</span></code></pre></div>
<p>And all the previes created functions, dataloading and transformations are:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="effect-of-batch-size.html#cb52-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">&quot;example_data/credit_risk_dataset.csv&quot;</span>).fillna(<span class="dv">0</span>)</span>
<span id="cb52-2"><a href="effect-of-batch-size.html#cb52-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.replace({<span class="st">&quot;Y&quot;</span>: <span class="dv">1</span>, <span class="st">&quot;N&quot;</span>:<span class="dv">0</span>})</span>
<span id="cb52-3"><a href="effect-of-batch-size.html#cb52-3" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&quot;person_home_ownership&quot;</span>] <span class="op">=</span> data[<span class="st">&quot;person_home_ownership&quot;</span>].replace({<span class="st">&#39;OWN&#39;</span>:<span class="dv">1</span>, <span class="st">&#39;RENT&#39;</span>:<span class="dv">2</span>, <span class="st">&#39;MORTGAGE&#39;</span>:<span class="dv">3</span>, <span class="st">&#39;OTHER&#39;</span>:<span class="dv">4</span>})</span>
<span id="cb52-4"><a href="effect-of-batch-size.html#cb52-4" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&quot;loan_intent&quot;</span>] <span class="op">=</span> data[<span class="st">&quot;loan_intent&quot;</span>].replace({<span class="st">&#39;PERSONAL&#39;</span>:<span class="dv">1</span>, <span class="st">&#39;EDUCATION&#39;</span>:<span class="dv">2</span>, <span class="st">&#39;MEDICAL&#39;</span>:<span class="dv">3</span>, <span class="st">&#39;VENTURE&#39;</span>:<span class="dv">4</span>, <span class="st">&#39;HOMEIMPROVEMENT&#39;</span>:<span class="dv">5</span>,<span class="st">&#39;DEBTCONSOLIDATION&#39;</span>:<span class="dv">6</span>})</span>
<span id="cb52-5"><a href="effect-of-batch-size.html#cb52-5" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&quot;loan_grade&quot;</span>] <span class="op">=</span> data[<span class="st">&quot;loan_grade&quot;</span>].replace({<span class="st">&#39;A&#39;</span>:<span class="dv">1</span>, <span class="st">&#39;B&#39;</span>:<span class="dv">2</span>, <span class="st">&#39;C&#39;</span>:<span class="dv">3</span>, <span class="st">&#39;D&#39;</span>:<span class="dv">4</span>, <span class="st">&#39;E&#39;</span>:<span class="dv">5</span>, <span class="st">&#39;F&#39;</span>:<span class="dv">6</span>, <span class="st">&#39;G&#39;</span>:<span class="dv">7</span>})</span>
<span id="cb52-6"><a href="effect-of-batch-size.html#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="effect-of-batch-size.html#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> NormalizeData(np_arr):</span>
<span id="cb52-8"><a href="effect-of-batch-size.html#cb52-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(np_arr.shape[<span class="dv">1</span>]):</span>
<span id="cb52-9"><a href="effect-of-batch-size.html#cb52-9" aria-hidden="true" tabindex="-1"></a>    np_arr[:,i] <span class="op">=</span> (np_arr[:,i] <span class="op">-</span> np.<span class="bu">min</span>(np_arr[:,i])) <span class="op">/</span> (np.<span class="bu">max</span>(np_arr[:,i]) <span class="op">-</span> np.<span class="bu">min</span>(np_arr[:,i]))</span>
<span id="cb52-10"><a href="effect-of-batch-size.html#cb52-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(np_arr)</span>
<span id="cb52-11"><a href="effect-of-batch-size.html#cb52-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-12"><a href="effect-of-batch-size.html#cb52-12" aria-hidden="true" tabindex="-1"></a>training_n <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb52-13"><a href="effect-of-batch-size.html#cb52-13" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> NormalizeData( data.loc[<span class="dv">0</span>:(training_n<span class="op">-</span><span class="dv">1</span>), data.columns <span class="op">!=</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy() )</span>
<span id="cb52-14"><a href="effect-of-batch-size.html#cb52-14" aria-hidden="true" tabindex="-1"></a>Y_train <span class="op">=</span> data.loc[<span class="dv">0</span>:(training_n<span class="op">-</span><span class="dv">1</span>), data.columns <span class="op">==</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy()</span>
<span id="cb52-15"><a href="effect-of-batch-size.html#cb52-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-16"><a href="effect-of-batch-size.html#cb52-16" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> NormalizeData( data.loc[training_n:, data.columns <span class="op">!=</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy() )</span>
<span id="cb52-17"><a href="effect-of-batch-size.html#cb52-17" aria-hidden="true" tabindex="-1"></a>Y_test <span class="op">=</span> data.loc[training_n:, data.columns <span class="op">==</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy()</span>
<span id="cb52-18"><a href="effect-of-batch-size.html#cb52-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-19"><a href="effect-of-batch-size.html#cb52-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-20"><a href="effect-of-batch-size.html#cb52-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_weights(n_input, n_output, hidden_layer_neurons):</span>
<span id="cb52-21"><a href="effect-of-batch-size.html#cb52-21" aria-hidden="true" tabindex="-1"></a>  W <span class="op">=</span> []</span>
<span id="cb52-22"><a href="effect-of-batch-size.html#cb52-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(hidden_layer_neurons)<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb52-23"><a href="effect-of-batch-size.html#cb52-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>: <span class="co"># first layer</span></span>
<span id="cb52-24"><a href="effect-of-batch-size.html#cb52-24" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((n_input<span class="op">+</span><span class="dv">1</span>, hidden_layer_neurons[i])))</span>
<span id="cb52-25"><a href="effect-of-batch-size.html#cb52-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">==</span> <span class="bu">len</span>(hidden_layer_neurons): <span class="co"># last layer</span></span>
<span id="cb52-26"><a href="effect-of-batch-size.html#cb52-26" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((hidden_layer_neurons[i<span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>, n_output)))</span>
<span id="cb52-27"><a href="effect-of-batch-size.html#cb52-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: <span class="co"># middle layers</span></span>
<span id="cb52-28"><a href="effect-of-batch-size.html#cb52-28" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((hidden_layer_neurons[i<span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>, hidden_layer_neurons[i])))</span>
<span id="cb52-29"><a href="effect-of-batch-size.html#cb52-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(W)</span>
<span id="cb52-30"><a href="effect-of-batch-size.html#cb52-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-31"><a href="effect-of-batch-size.html#cb52-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_ones_to_input(x):</span>
<span id="cb52-32"><a href="effect-of-batch-size.html#cb52-32" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(np.append(x, np.array([np.ones(<span class="bu">len</span>(x))]).T, axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb52-33"><a href="effect-of-batch-size.html#cb52-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-34"><a href="effect-of-batch-size.html#cb52-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-35"><a href="effect-of-batch-size.html#cb52-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb52-36"><a href="effect-of-batch-size.html#cb52-36" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb52-37"><a href="effect-of-batch-size.html#cb52-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-38"><a href="effect-of-batch-size.html#cb52-38" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> deriv_sigmoid(x):</span>
<span id="cb52-39"><a href="effect-of-batch-size.html#cb52-39" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> x)</span>
<span id="cb52-40"><a href="effect-of-batch-size.html#cb52-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-41"><a href="effect-of-batch-size.html#cb52-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-42"><a href="effect-of-batch-size.html#cb52-42" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(x, w):</span>
<span id="cb52-43"><a href="effect-of-batch-size.html#cb52-43" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( sigmoid(x <span class="op">@</span> w) )</span>
<span id="cb52-44"><a href="effect-of-batch-size.html#cb52-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-45"><a href="effect-of-batch-size.html#cb52-45" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward(IN, OUT, W, Y, grad, k):</span>
<span id="cb52-46"><a href="effect-of-batch-size.html#cb52-46" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> k <span class="op">==</span> <span class="bu">len</span>(grad)<span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb52-47"><a href="effect-of-batch-size.html#cb52-47" aria-hidden="true" tabindex="-1"></a>    grad[k] <span class="op">=</span> deriv_sigmoid(OUT[k]) <span class="op">*</span> (Y<span class="op">-</span>OUT[k])</span>
<span id="cb52-48"><a href="effect-of-batch-size.html#cb52-48" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb52-49"><a href="effect-of-batch-size.html#cb52-49" aria-hidden="true" tabindex="-1"></a>    grad[k] <span class="op">=</span> deriv_sigmoid(OUT[k]) <span class="op">*</span>(grad[k<span class="op">+</span><span class="dv">1</span>] <span class="op">@</span> W[k<span class="op">+</span><span class="dv">1</span>][<span class="dv">0</span>:<span class="bu">len</span>(W[k<span class="op">+</span><span class="dv">1</span>])<span class="op">-</span><span class="dv">1</span>].T)</span>
<span id="cb52-50"><a href="effect-of-batch-size.html#cb52-50" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(grad)</span>
<span id="cb52-51"><a href="effect-of-batch-size.html#cb52-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-52"><a href="effect-of-batch-size.html#cb52-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-53"><a href="effect-of-batch-size.html#cb52-53" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mean_square_error(error):</span>
<span id="cb52-54"><a href="effect-of-batch-size.html#cb52-54" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(error <span class="op">**</span> <span class="dv">2</span>) )</span>
<span id="cb52-55"><a href="effect-of-batch-size.html#cb52-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-56"><a href="effect-of-batch-size.html#cb52-56" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_error(errors, title):</span>
<span id="cb52-57"><a href="effect-of-batch-size.html#cb52-57" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(errors)))</span>
<span id="cb52-58"><a href="effect-of-batch-size.html#cb52-58" aria-hidden="true" tabindex="-1"></a>  y <span class="op">=</span> np.array(errors)</span>
<span id="cb52-59"><a href="effect-of-batch-size.html#cb52-59" aria-hidden="true" tabindex="-1"></a>  pyplot.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb52-60"><a href="effect-of-batch-size.html#cb52-60" aria-hidden="true" tabindex="-1"></a>  pyplot.plot(x, y, <span class="st">&quot;g&quot;</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb52-61"><a href="effect-of-batch-size.html#cb52-61" aria-hidden="true" tabindex="-1"></a>  pyplot.xlabel(<span class="st">&quot;Iterations&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb52-62"><a href="effect-of-batch-size.html#cb52-62" aria-hidden="true" tabindex="-1"></a>  pyplot.ylabel(<span class="st">&quot;Mean Square Error&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb52-63"><a href="effect-of-batch-size.html#cb52-63" aria-hidden="true" tabindex="-1"></a>  pyplot.title(title)</span>
<span id="cb52-64"><a href="effect-of-batch-size.html#cb52-64" aria-hidden="true" tabindex="-1"></a>  pyplot.ylim(<span class="dv">0</span>,<span class="bu">max</span>(errors)<span class="op">*</span><span class="fl">1.1</span>)</span>
<span id="cb52-65"><a href="effect-of-batch-size.html#cb52-65" aria-hidden="true" tabindex="-1"></a>  pyplot.show()</span>
<span id="cb52-66"><a href="effect-of-batch-size.html#cb52-66" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb52-67"><a href="effect-of-batch-size.html#cb52-67" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test(X_test, W):</span>
<span id="cb52-68"><a href="effect-of-batch-size.html#cb52-68" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)):</span>
<span id="cb52-69"><a href="effect-of-batch-size.html#cb52-69" aria-hidden="true" tabindex="-1"></a>    X_test <span class="op">=</span> forward(add_ones_to_input(X_test), W[i])</span>
<span id="cb52-70"><a href="effect-of-batch-size.html#cb52-70" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(X_test)</span>
<span id="cb52-71"><a href="effect-of-batch-size.html#cb52-71" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb52-72"><a href="effect-of-batch-size.html#cb52-72" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> classify(Y_approx):</span>
<span id="cb52-73"><a href="effect-of-batch-size.html#cb52-73" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( np.<span class="bu">round</span>(Y_approx,<span class="dv">0</span>) )</span></code></pre></div>
<p>Everything is loaded and set up. Now can we compare the time and the error for different <code>batch_size</code>:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="effect-of-batch-size.html#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># full batch size</span></span>
<span id="cb53-2"><a href="effect-of-batch-size.html#cb53-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb53-3"><a href="effect-of-batch-size.html#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="effect-of-batch-size.html#cb53-4" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb53-5"><a href="effect-of-batch-size.html#cb53-5" aria-hidden="true" tabindex="-1"></a>W_train, errors_train <span class="op">=</span> train(X <span class="op">=</span> X_train, Y <span class="op">=</span> Y_train, hidden_layer_neurons <span class="op">=</span> [<span class="dv">11</span>,<span class="dv">4</span>], alpha <span class="op">=</span> <span class="fl">0.01</span>, epochs <span class="op">=</span> <span class="dv">5000</span>, batch_size <span class="op">=</span> <span class="dv">2000</span>)</span>
<span id="cb53-6"><a href="effect-of-batch-size.html#cb53-6" aria-hidden="true" tabindex="-1"></a>time_diff <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb53-7"><a href="effect-of-batch-size.html#cb53-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Time to train the NN: &quot;</span>, time_diff)</span>
<span id="cb53-8"><a href="effect-of-batch-size.html#cb53-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-9"><a href="effect-of-batch-size.html#cb53-9" aria-hidden="true" tabindex="-1"></a>ms_errors_train <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(mean_square_error, errors_train)))</span>
<span id="cb53-10"><a href="effect-of-batch-size.html#cb53-10" aria-hidden="true" tabindex="-1"></a>plot_error(ms_errors_train, <span class="st">&quot;MLP Credit Default&quot;</span>)</span>
<span id="cb53-11"><a href="effect-of-batch-size.html#cb53-11" aria-hidden="true" tabindex="-1"></a>result_test <span class="op">=</span> test(X_test, W_train)</span>
<span id="cb53-12"><a href="effect-of-batch-size.html#cb53-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Mean Square error over all testdata: &quot;</span>, mean_square_error(Y_test <span class="op">-</span> result_test))</span>
<span id="cb53-13"><a href="effect-of-batch-size.html#cb53-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-14"><a href="effect-of-batch-size.html#cb53-14" aria-hidden="true" tabindex="-1"></a>classified_error <span class="op">=</span> Y_test <span class="op">-</span> classify(result_test)</span>
<span id="cb53-15"><a href="effect-of-batch-size.html#cb53-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Mean Square error over all classified testdata: &quot;</span>, mean_square_error(classified_error))</span>
<span id="cb53-16"><a href="effect-of-batch-size.html#cb53-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-17"><a href="effect-of-batch-size.html#cb53-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Probability of a wrong output: &quot;</span>, np.<span class="bu">round</span>(np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(classified_error)) <span class="op">/</span> <span class="bu">len</span>(classified_error) <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>), <span class="st">&quot;%&quot;</span> )</span>
<span id="cb53-18"><a href="effect-of-batch-size.html#cb53-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Probability of a right output: &quot;</span>, np.<span class="bu">round</span>((<span class="dv">1</span> <span class="op">-</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(classified_error)) <span class="op">/</span> <span class="bu">len</span>(classified_error))<span class="op">*</span><span class="dv">100</span>,<span class="dv">2</span>),<span class="st">&quot;%&quot;</span> )</span>
<span id="cb53-19"><a href="effect-of-batch-size.html#cb53-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-20"><a href="effect-of-batch-size.html#cb53-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-21"><a href="effect-of-batch-size.html#cb53-21" aria-hidden="true" tabindex="-1"></a>confusion_matrix(Y_test, classify(result_test))</span></code></pre></div>
<pre><code>## Time to train the NN:  8.68296504020691</code></pre>
<p><img src="gitbook-demo_files/figure-html/unnamed-chunk-21-15.png" width="576" /></p>
<pre><code>## Mean Square error over all testdata:  2402.2283244767077
## Mean Square error over all classified testdata:  2932.0
## Probability of a wrong output:  19.18 %
## Probability of a right output:  80.82 %
## array([[20193,  4113],
##        [ 1751,  4524]], dtype=int64)</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="effect-of-batch-size.html#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># batch size = 100</span></span>
<span id="cb56-2"><a href="effect-of-batch-size.html#cb56-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb56-3"><a href="effect-of-batch-size.html#cb56-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-4"><a href="effect-of-batch-size.html#cb56-4" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb56-5"><a href="effect-of-batch-size.html#cb56-5" aria-hidden="true" tabindex="-1"></a>W_train, errors_train <span class="op">=</span> train(X <span class="op">=</span> X_train, Y <span class="op">=</span> Y_train, hidden_layer_neurons <span class="op">=</span> [<span class="dv">11</span>,<span class="dv">4</span>], alpha <span class="op">=</span> <span class="fl">0.01</span>, epochs <span class="op">=</span> <span class="dv">5000</span>, batch_size <span class="op">=</span> <span class="dv">100</span>)</span>
<span id="cb56-6"><a href="effect-of-batch-size.html#cb56-6" aria-hidden="true" tabindex="-1"></a>time_diff <span class="op">=</span> time.time() <span class="op">-</span> start</span>
<span id="cb56-7"><a href="effect-of-batch-size.html#cb56-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Time to train the NN: &quot;</span>, time_diff)</span>
<span id="cb56-8"><a href="effect-of-batch-size.html#cb56-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-9"><a href="effect-of-batch-size.html#cb56-9" aria-hidden="true" tabindex="-1"></a>ms_errors_train <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(mean_square_error, errors_train)))</span>
<span id="cb56-10"><a href="effect-of-batch-size.html#cb56-10" aria-hidden="true" tabindex="-1"></a>plot_error(ms_errors_train, <span class="st">&quot;MLP Credit Default&quot;</span>)</span>
<span id="cb56-11"><a href="effect-of-batch-size.html#cb56-11" aria-hidden="true" tabindex="-1"></a>result_test <span class="op">=</span> test(X_test, W_train)</span>
<span id="cb56-12"><a href="effect-of-batch-size.html#cb56-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Mean Square error over all testdata: &quot;</span>, mean_square_error(Y_test <span class="op">-</span> result_test))</span>
<span id="cb56-13"><a href="effect-of-batch-size.html#cb56-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-14"><a href="effect-of-batch-size.html#cb56-14" aria-hidden="true" tabindex="-1"></a>classified_error <span class="op">=</span> Y_test <span class="op">-</span> classify(result_test)</span>
<span id="cb56-15"><a href="effect-of-batch-size.html#cb56-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Mean Square error over all classified testdata: &quot;</span>, mean_square_error(classified_error))</span>
<span id="cb56-16"><a href="effect-of-batch-size.html#cb56-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-17"><a href="effect-of-batch-size.html#cb56-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Probability of a wrong output: &quot;</span>, np.<span class="bu">round</span>(np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(classified_error)) <span class="op">/</span> <span class="bu">len</span>(classified_error) <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>), <span class="st">&quot;%&quot;</span> )</span>
<span id="cb56-18"><a href="effect-of-batch-size.html#cb56-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Probability of a right output: &quot;</span>, np.<span class="bu">round</span>((<span class="dv">1</span> <span class="op">-</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(classified_error)) <span class="op">/</span> <span class="bu">len</span>(classified_error))<span class="op">*</span><span class="dv">100</span>,<span class="dv">2</span>),<span class="st">&quot;%&quot;</span> )</span>
<span id="cb56-19"><a href="effect-of-batch-size.html#cb56-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-20"><a href="effect-of-batch-size.html#cb56-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-21"><a href="effect-of-batch-size.html#cb56-21" aria-hidden="true" tabindex="-1"></a>confusion_matrix(Y_test, classify(result_test))</span></code></pre></div>
<pre><code>## Time to train the NN:  22.47386336326599</code></pre>
<p><img src="gitbook-demo_files/figure-html/unnamed-chunk-22-17.png" width="576" /></p>
<pre><code>## Mean Square error over all testdata:  2210.3693145823286
## Mean Square error over all classified testdata:  2559.5
## Probability of a wrong output:  16.74 %
## Probability of a right output:  83.26 %
## array([[21396,  2910],
##        [ 2209,  4066]], dtype=int64)</code></pre>
<p>We can see that the full batch size is much faster than the smaller one, but the smaller batch size has a smaller error. Maybe the perfect batch size depends on the problem itself. If you have a low dimensional input matrix and need to get a fast solution, its better to go with full batch size. If you have high dimensional input its not possible to use a full batch size, because your ram is jumping off. I think its the best to analyze the underlying data and select a suitable batch size for it. Maybe some situations will need a dynamic batch size, that shrinks over time, to get the best results, like in the blog post i mentioned at the start of the chapter.</p>
</div>
<div id="appendix-complete-code-4" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Appendix (complete code)</h2>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="effect-of-batch-size.html#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb59-2"><a href="effect-of-batch-size.html#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> pyplot</span>
<span id="cb59-3"><a href="effect-of-batch-size.html#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb59-4"><a href="effect-of-batch-size.html#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb59-5"><a href="effect-of-batch-size.html#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math <span class="im">as</span> ma</span>
<span id="cb59-6"><a href="effect-of-batch-size.html#cb59-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb59-7"><a href="effect-of-batch-size.html#cb59-7" aria-hidden="true" tabindex="-1"></a>np.warnings.filterwarnings(<span class="st">&#39;ignore&#39;</span>, category<span class="op">=</span>np.VisibleDeprecationWarning) </span>
<span id="cb59-8"><a href="effect-of-batch-size.html#cb59-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-9"><a href="effect-of-batch-size.html#cb59-9" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">&quot;example_data/credit_risk_dataset.csv&quot;</span>).fillna(<span class="dv">0</span>)</span>
<span id="cb59-10"><a href="effect-of-batch-size.html#cb59-10" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.replace({<span class="st">&quot;Y&quot;</span>: <span class="dv">1</span>, <span class="st">&quot;N&quot;</span>:<span class="dv">0</span>})</span>
<span id="cb59-11"><a href="effect-of-batch-size.html#cb59-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-12"><a href="effect-of-batch-size.html#cb59-12" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&quot;person_home_ownership&quot;</span>] <span class="op">=</span> data[<span class="st">&quot;person_home_ownership&quot;</span>].replace({<span class="st">&#39;OWN&#39;</span>:<span class="dv">1</span>, <span class="st">&#39;RENT&#39;</span>:<span class="dv">2</span>, <span class="st">&#39;MORTGAGE&#39;</span>:<span class="dv">3</span>, <span class="st">&#39;OTHER&#39;</span>:<span class="dv">4</span>})</span>
<span id="cb59-13"><a href="effect-of-batch-size.html#cb59-13" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&quot;loan_intent&quot;</span>] <span class="op">=</span> data[<span class="st">&quot;loan_intent&quot;</span>].replace({<span class="st">&#39;PERSONAL&#39;</span>:<span class="dv">1</span>, <span class="st">&#39;EDUCATION&#39;</span>:<span class="dv">2</span>, <span class="st">&#39;MEDICAL&#39;</span>:<span class="dv">3</span>, <span class="st">&#39;VENTURE&#39;</span>:<span class="dv">4</span>, <span class="st">&#39;HOMEIMPROVEMENT&#39;</span>:<span class="dv">5</span>,<span class="st">&#39;DEBTCONSOLIDATION&#39;</span>:<span class="dv">6</span>})</span>
<span id="cb59-14"><a href="effect-of-batch-size.html#cb59-14" aria-hidden="true" tabindex="-1"></a>data[<span class="st">&quot;loan_grade&quot;</span>] <span class="op">=</span> data[<span class="st">&quot;loan_grade&quot;</span>].replace({<span class="st">&#39;A&#39;</span>:<span class="dv">1</span>, <span class="st">&#39;B&#39;</span>:<span class="dv">2</span>, <span class="st">&#39;C&#39;</span>:<span class="dv">3</span>, <span class="st">&#39;D&#39;</span>:<span class="dv">4</span>, <span class="st">&#39;E&#39;</span>:<span class="dv">5</span>, <span class="st">&#39;F&#39;</span>:<span class="dv">6</span>, <span class="st">&#39;G&#39;</span>:<span class="dv">7</span>})</span>
<span id="cb59-15"><a href="effect-of-batch-size.html#cb59-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-16"><a href="effect-of-batch-size.html#cb59-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-17"><a href="effect-of-batch-size.html#cb59-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> NormalizeData(np_arr):</span>
<span id="cb59-18"><a href="effect-of-batch-size.html#cb59-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(np_arr.shape[<span class="dv">1</span>]):</span>
<span id="cb59-19"><a href="effect-of-batch-size.html#cb59-19" aria-hidden="true" tabindex="-1"></a>    np_arr[:,i] <span class="op">=</span> (np_arr[:,i] <span class="op">-</span> np.<span class="bu">min</span>(np_arr[:,i])) <span class="op">/</span> (np.<span class="bu">max</span>(np_arr[:,i]) <span class="op">-</span> np.<span class="bu">min</span>(np_arr[:,i]))</span>
<span id="cb59-20"><a href="effect-of-batch-size.html#cb59-20" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(np_arr)</span>
<span id="cb59-21"><a href="effect-of-batch-size.html#cb59-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-22"><a href="effect-of-batch-size.html#cb59-22" aria-hidden="true" tabindex="-1"></a>training_n <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb59-23"><a href="effect-of-batch-size.html#cb59-23" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> NormalizeData( data.loc[<span class="dv">0</span>:(training_n<span class="op">-</span><span class="dv">1</span>), data.columns <span class="op">!=</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy() )</span>
<span id="cb59-24"><a href="effect-of-batch-size.html#cb59-24" aria-hidden="true" tabindex="-1"></a>Y_train <span class="op">=</span> data.loc[<span class="dv">0</span>:(training_n<span class="op">-</span><span class="dv">1</span>), data.columns <span class="op">==</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy()</span>
<span id="cb59-25"><a href="effect-of-batch-size.html#cb59-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-26"><a href="effect-of-batch-size.html#cb59-26" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> NormalizeData( data.loc[training_n:, data.columns <span class="op">!=</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy() )</span>
<span id="cb59-27"><a href="effect-of-batch-size.html#cb59-27" aria-hidden="true" tabindex="-1"></a>Y_test <span class="op">=</span> data.loc[training_n:, data.columns <span class="op">==</span> <span class="st">&#39;loan_status&#39;</span>].to_numpy()</span>
<span id="cb59-28"><a href="effect-of-batch-size.html#cb59-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-29"><a href="effect-of-batch-size.html#cb59-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-30"><a href="effect-of-batch-size.html#cb59-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-31"><a href="effect-of-batch-size.html#cb59-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_weights(n_input, n_output, hidden_layer_neurons):</span>
<span id="cb59-32"><a href="effect-of-batch-size.html#cb59-32" aria-hidden="true" tabindex="-1"></a>  W <span class="op">=</span> []</span>
<span id="cb59-33"><a href="effect-of-batch-size.html#cb59-33" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(hidden_layer_neurons)<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb59-34"><a href="effect-of-batch-size.html#cb59-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>: <span class="co"># first layer</span></span>
<span id="cb59-35"><a href="effect-of-batch-size.html#cb59-35" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((n_input<span class="op">+</span><span class="dv">1</span>, hidden_layer_neurons[i])))</span>
<span id="cb59-36"><a href="effect-of-batch-size.html#cb59-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">==</span> <span class="bu">len</span>(hidden_layer_neurons): <span class="co"># last layer</span></span>
<span id="cb59-37"><a href="effect-of-batch-size.html#cb59-37" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((hidden_layer_neurons[i<span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>, n_output)))</span>
<span id="cb59-38"><a href="effect-of-batch-size.html#cb59-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: <span class="co"># middle layers</span></span>
<span id="cb59-39"><a href="effect-of-batch-size.html#cb59-39" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((hidden_layer_neurons[i<span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>, hidden_layer_neurons[i])))</span>
<span id="cb59-40"><a href="effect-of-batch-size.html#cb59-40" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(W)</span>
<span id="cb59-41"><a href="effect-of-batch-size.html#cb59-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-42"><a href="effect-of-batch-size.html#cb59-42" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_ones_to_input(x):</span>
<span id="cb59-43"><a href="effect-of-batch-size.html#cb59-43" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(np.append(x, np.array([np.ones(<span class="bu">len</span>(x))]).T, axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb59-44"><a href="effect-of-batch-size.html#cb59-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-45"><a href="effect-of-batch-size.html#cb59-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-46"><a href="effect-of-batch-size.html#cb59-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-47"><a href="effect-of-batch-size.html#cb59-47" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb59-48"><a href="effect-of-batch-size.html#cb59-48" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb59-49"><a href="effect-of-batch-size.html#cb59-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-50"><a href="effect-of-batch-size.html#cb59-50" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> deriv_sigmoid(x):</span>
<span id="cb59-51"><a href="effect-of-batch-size.html#cb59-51" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> x)</span>
<span id="cb59-52"><a href="effect-of-batch-size.html#cb59-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-53"><a href="effect-of-batch-size.html#cb59-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-54"><a href="effect-of-batch-size.html#cb59-54" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(x, w):</span>
<span id="cb59-55"><a href="effect-of-batch-size.html#cb59-55" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( sigmoid(x <span class="op">@</span> w) )</span>
<span id="cb59-56"><a href="effect-of-batch-size.html#cb59-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-57"><a href="effect-of-batch-size.html#cb59-57" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward(IN, OUT, W, Y, grad, k):</span>
<span id="cb59-58"><a href="effect-of-batch-size.html#cb59-58" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> k <span class="op">==</span> <span class="bu">len</span>(grad)<span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb59-59"><a href="effect-of-batch-size.html#cb59-59" aria-hidden="true" tabindex="-1"></a>    grad[k] <span class="op">=</span> deriv_sigmoid(OUT[k]) <span class="op">*</span> (Y<span class="op">-</span>OUT[k])</span>
<span id="cb59-60"><a href="effect-of-batch-size.html#cb59-60" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb59-61"><a href="effect-of-batch-size.html#cb59-61" aria-hidden="true" tabindex="-1"></a>    grad[k] <span class="op">=</span> deriv_sigmoid(OUT[k]) <span class="op">*</span>(grad[k<span class="op">+</span><span class="dv">1</span>] <span class="op">@</span> W[k<span class="op">+</span><span class="dv">1</span>][<span class="dv">0</span>:<span class="bu">len</span>(W[k<span class="op">+</span><span class="dv">1</span>])<span class="op">-</span><span class="dv">1</span>].T)</span>
<span id="cb59-62"><a href="effect-of-batch-size.html#cb59-62" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(grad)</span>
<span id="cb59-63"><a href="effect-of-batch-size.html#cb59-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-64"><a href="effect-of-batch-size.html#cb59-64" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_random_batches(batch_size, full_batch_size):</span>
<span id="cb59-65"><a href="effect-of-batch-size.html#cb59-65" aria-hidden="true" tabindex="-1"></a>  batches <span class="op">=</span> np.arange(full_batch_size)</span>
<span id="cb59-66"><a href="effect-of-batch-size.html#cb59-66" aria-hidden="true" tabindex="-1"></a>  np.random.shuffle(batches)</span>
<span id="cb59-67"><a href="effect-of-batch-size.html#cb59-67" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(np.array_split(batches, ma.ceil(full_batch_size<span class="op">/</span>batch_size)))</span>
<span id="cb59-68"><a href="effect-of-batch-size.html#cb59-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-69"><a href="effect-of-batch-size.html#cb59-69" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(X, Y, hidden_layer_neurons, alpha, epochs, batch_size):</span>
<span id="cb59-70"><a href="effect-of-batch-size.html#cb59-70" aria-hidden="true" tabindex="-1"></a>  n_input <span class="op">=</span> <span class="bu">len</span>(X[<span class="dv">0</span>])</span>
<span id="cb59-71"><a href="effect-of-batch-size.html#cb59-71" aria-hidden="true" tabindex="-1"></a>  n_output <span class="op">=</span> <span class="bu">len</span>(Y[<span class="dv">0</span>])</span>
<span id="cb59-72"><a href="effect-of-batch-size.html#cb59-72" aria-hidden="true" tabindex="-1"></a>  W <span class="op">=</span> generate_weights(n_input, n_output, hidden_layer_neurons)</span>
<span id="cb59-73"><a href="effect-of-batch-size.html#cb59-73" aria-hidden="true" tabindex="-1"></a>  errors <span class="op">=</span> []</span>
<span id="cb59-74"><a href="effect-of-batch-size.html#cb59-74" aria-hidden="true" tabindex="-1"></a>  batches <span class="op">=</span> generate_random_batches(batch_size, full_batch_size <span class="op">=</span> <span class="bu">len</span>(X))</span>
<span id="cb59-75"><a href="effect-of-batch-size.html#cb59-75" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb59-76"><a href="effect-of-batch-size.html#cb59-76" aria-hidden="true" tabindex="-1"></a>    error_temp <span class="op">=</span> np.array([])</span>
<span id="cb59-77"><a href="effect-of-batch-size.html#cb59-77" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> z <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(batches)):</span>
<span id="cb59-78"><a href="effect-of-batch-size.html#cb59-78" aria-hidden="true" tabindex="-1"></a>      IN <span class="op">=</span> []</span>
<span id="cb59-79"><a href="effect-of-batch-size.html#cb59-79" aria-hidden="true" tabindex="-1"></a>      OUT <span class="op">=</span> []</span>
<span id="cb59-80"><a href="effect-of-batch-size.html#cb59-80" aria-hidden="true" tabindex="-1"></a>      grad <span class="op">=</span> [<span class="va">None</span>]<span class="op">*</span><span class="bu">len</span>(W)</span>
<span id="cb59-81"><a href="effect-of-batch-size.html#cb59-81" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)):</span>
<span id="cb59-82"><a href="effect-of-batch-size.html#cb59-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> k<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb59-83"><a href="effect-of-batch-size.html#cb59-83" aria-hidden="true" tabindex="-1"></a>          IN.append(add_ones_to_input(X[batches[z],:]))</span>
<span id="cb59-84"><a href="effect-of-batch-size.html#cb59-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb59-85"><a href="effect-of-batch-size.html#cb59-85" aria-hidden="true" tabindex="-1"></a>          IN.append(add_ones_to_input(OUT[k<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb59-86"><a href="effect-of-batch-size.html#cb59-86" aria-hidden="true" tabindex="-1"></a>        OUT.append(forward(x<span class="op">=</span>IN[k], w<span class="op">=</span>W[k]))</span>
<span id="cb59-87"><a href="effect-of-batch-size.html#cb59-87" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb59-88"><a href="effect-of-batch-size.html#cb59-88" aria-hidden="true" tabindex="-1"></a>      error_temp <span class="op">=</span> np.append(error_temp, Y[batches[z],:] <span class="op">-</span> OUT[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb59-89"><a href="effect-of-batch-size.html#cb59-89" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb59-90"><a href="effect-of-batch-size.html#cb59-90" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb59-91"><a href="effect-of-batch-size.html#cb59-91" aria-hidden="true" tabindex="-1"></a>        grad <span class="op">=</span> backward(IN, OUT, W, Y[batches[z],:], grad, k) </span>
<span id="cb59-92"><a href="effect-of-batch-size.html#cb59-92" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb59-93"><a href="effect-of-batch-size.html#cb59-93" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)):</span>
<span id="cb59-94"><a href="effect-of-batch-size.html#cb59-94" aria-hidden="true" tabindex="-1"></a>        W[k] <span class="op">=</span> W[k] <span class="op">+</span> alpha <span class="op">*</span> (IN[k].T <span class="op">@</span> grad[k])</span>
<span id="cb59-95"><a href="effect-of-batch-size.html#cb59-95" aria-hidden="true" tabindex="-1"></a>    errors.append(error_temp)</span>
<span id="cb59-96"><a href="effect-of-batch-size.html#cb59-96" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb59-97"><a href="effect-of-batch-size.html#cb59-97" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> W, errors</span>
<span id="cb59-98"><a href="effect-of-batch-size.html#cb59-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-99"><a href="effect-of-batch-size.html#cb59-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-100"><a href="effect-of-batch-size.html#cb59-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-101"><a href="effect-of-batch-size.html#cb59-101" aria-hidden="true" tabindex="-1"></a>W_train, errors_train <span class="op">=</span> train(X <span class="op">=</span> X_train, Y <span class="op">=</span> Y_train, hidden_layer_neurons <span class="op">=</span> [<span class="dv">11</span>,<span class="dv">4</span>], alpha <span class="op">=</span> <span class="fl">0.01</span>, epochs <span class="op">=</span> <span class="dv">2000</span>, batch_size <span class="op">=</span> <span class="dv">2000</span>)</span>
<span id="cb59-102"><a href="effect-of-batch-size.html#cb59-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-103"><a href="effect-of-batch-size.html#cb59-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-104"><a href="effect-of-batch-size.html#cb59-104" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mean_square_error(error):</span>
<span id="cb59-105"><a href="effect-of-batch-size.html#cb59-105" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(error <span class="op">**</span> <span class="dv">2</span>) )</span>
<span id="cb59-106"><a href="effect-of-batch-size.html#cb59-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-107"><a href="effect-of-batch-size.html#cb59-107" aria-hidden="true" tabindex="-1"></a>ms_errors_train <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(mean_square_error, errors_train)))</span>
<span id="cb59-108"><a href="effect-of-batch-size.html#cb59-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-109"><a href="effect-of-batch-size.html#cb59-109" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_error(errors, title):</span>
<span id="cb59-110"><a href="effect-of-batch-size.html#cb59-110" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(errors)))</span>
<span id="cb59-111"><a href="effect-of-batch-size.html#cb59-111" aria-hidden="true" tabindex="-1"></a>  y <span class="op">=</span> np.array(errors)</span>
<span id="cb59-112"><a href="effect-of-batch-size.html#cb59-112" aria-hidden="true" tabindex="-1"></a>  pyplot.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb59-113"><a href="effect-of-batch-size.html#cb59-113" aria-hidden="true" tabindex="-1"></a>  pyplot.plot(x, y, <span class="st">&quot;g&quot;</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb59-114"><a href="effect-of-batch-size.html#cb59-114" aria-hidden="true" tabindex="-1"></a>  pyplot.xlabel(<span class="st">&quot;Iterations&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb59-115"><a href="effect-of-batch-size.html#cb59-115" aria-hidden="true" tabindex="-1"></a>  pyplot.ylabel(<span class="st">&quot;Mean Square Error&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb59-116"><a href="effect-of-batch-size.html#cb59-116" aria-hidden="true" tabindex="-1"></a>  pyplot.title(title)</span>
<span id="cb59-117"><a href="effect-of-batch-size.html#cb59-117" aria-hidden="true" tabindex="-1"></a>  pyplot.ylim(<span class="dv">0</span>,<span class="bu">max</span>(errors)<span class="op">*</span><span class="fl">1.1</span>)</span>
<span id="cb59-118"><a href="effect-of-batch-size.html#cb59-118" aria-hidden="true" tabindex="-1"></a>  pyplot.show()</span>
<span id="cb59-119"><a href="effect-of-batch-size.html#cb59-119" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb59-120"><a href="effect-of-batch-size.html#cb59-120" aria-hidden="true" tabindex="-1"></a>plot_error(ms_errors_train, <span class="st">&quot;MLP Credit Default&quot;</span>)</span>
<span id="cb59-121"><a href="effect-of-batch-size.html#cb59-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-122"><a href="effect-of-batch-size.html#cb59-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-123"><a href="effect-of-batch-size.html#cb59-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-124"><a href="effect-of-batch-size.html#cb59-124" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test(X_test, W):</span>
<span id="cb59-125"><a href="effect-of-batch-size.html#cb59-125" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)):</span>
<span id="cb59-126"><a href="effect-of-batch-size.html#cb59-126" aria-hidden="true" tabindex="-1"></a>    X_test <span class="op">=</span> forward(add_ones_to_input(X_test), W[i])</span>
<span id="cb59-127"><a href="effect-of-batch-size.html#cb59-127" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(X_test)</span>
<span id="cb59-128"><a href="effect-of-batch-size.html#cb59-128" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb59-129"><a href="effect-of-batch-size.html#cb59-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-130"><a href="effect-of-batch-size.html#cb59-130" aria-hidden="true" tabindex="-1"></a>result_test <span class="op">=</span> test(X_test, W_train)</span>
<span id="cb59-131"><a href="effect-of-batch-size.html#cb59-131" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Mean Square error over all testdata: &quot;</span>, mean_square_error(Y_test <span class="op">-</span> result_test))</span>
<span id="cb59-132"><a href="effect-of-batch-size.html#cb59-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-133"><a href="effect-of-batch-size.html#cb59-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-134"><a href="effect-of-batch-size.html#cb59-134" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> classify(Y_approx):</span>
<span id="cb59-135"><a href="effect-of-batch-size.html#cb59-135" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( np.<span class="bu">round</span>(Y_approx,<span class="dv">0</span>) )</span>
<span id="cb59-136"><a href="effect-of-batch-size.html#cb59-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-137"><a href="effect-of-batch-size.html#cb59-137" aria-hidden="true" tabindex="-1"></a>classified_error <span class="op">=</span> Y_test <span class="op">-</span> classify(result_test)</span>
<span id="cb59-138"><a href="effect-of-batch-size.html#cb59-138" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Mean Square error over all classified testdata: &quot;</span>, mean_square_error(classified_error))</span>
<span id="cb59-139"><a href="effect-of-batch-size.html#cb59-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-140"><a href="effect-of-batch-size.html#cb59-140" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Probability of a wrong output: &quot;</span>, np.<span class="bu">round</span>(np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(classified_error)) <span class="op">/</span> <span class="bu">len</span>(classified_error) <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>), <span class="st">&quot;%&quot;</span> )</span>
<span id="cb59-141"><a href="effect-of-batch-size.html#cb59-141" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Probability of a right output: &quot;</span>, np.<span class="bu">round</span>((<span class="dv">1</span> <span class="op">-</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(classified_error)) <span class="op">/</span> <span class="bu">len</span>(classified_error))<span class="op">*</span><span class="dv">100</span>,<span class="dv">2</span>),<span class="st">&quot;%&quot;</span> )</span>
<span id="cb59-142"><a href="effect-of-batch-size.html#cb59-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-143"><a href="effect-of-batch-size.html#cb59-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-144"><a href="effect-of-batch-size.html#cb59-144" aria-hidden="true" tabindex="-1"></a>confusion_matrix(Y_test, classify(result_test))</span></code></pre></div>
<p><img src="gitbook-demo_files/figure-html/unnamed-chunk-23-19.png" width="576" /></p>
<pre><code>## Mean Square error over all testdata:  2330.8698549481937
## Mean Square error over all classified testdata:  2940.0
## Probability of a wrong output:  19.23 %
## Probability of a right output:  80.77 %
## array([[20516,  3790],
##        [ 2090,  4185]], dtype=int64)</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mlp-example-credit-default.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="class-mlp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/cjvanlissa/gitbook-demo/edit/master/05_Batch_Size.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["gitbook-demo.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
