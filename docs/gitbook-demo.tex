% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\title{My First Steps in Neuronal Networks\\
(Beginners Guide)}
\author{Axel Roth}
\date{2021-11-27}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdfauthor={Axel Roth},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\usepackage{fvextra} \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{about}{%
\chapter{About}\label{about}}

\hypertarget{me}{%
\section{Me}\label{me}}

Hello, my name is Axel Roth, and I'm pursuing a master's degree in mathematics in Germany while working part-time in finance as something between a data analyst and a full stack developer. I have a lot of experience with R and all of its features, but I have never written a line of Python code or worked with Neuronal Networks before. So, why am I writing a Python Beginner's Guide to Neuronal Networks in the first place? It's simple. I'm currently attending a lecture in which we're learning how to program a Neuronal Network from scratch using basic Python packages, and I'd like to share my experience. Furthermore, I learned everything I know from free internet sources, which is why I want to give something back. It's also a good use-case for me to write my first things in English.

\hypertarget{the-book}{%
\section{The Book}\label{the-book}}

This book will essentially be a transcript of my lecture, in which we will learn to program a simple Perceptron (the most basic Neuronal Network), then progress to a multilayer Perceptron, and finally to a brief overview of decision trees. On this journey, we'll put the Neuronal Network to the test in a variety of scenarios that are simple to replicate. Because these are my first steps in this field, I apologize for my terrible spelling and cannot guarantee the highest quality, but this may be the best way to educate and attract uninitiated readers to take a look.

\hypertarget{how-it-works}{%
\section{How it works}\label{how-it-works}}

I'm writing this book in the R-Studio IDE, using the Bookdown framework, and using the reticulate package to embed python code. This is why I need to load the Python interpreter in the R-chunk below:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(reticulate)}
\FunctionTok{Sys.setenv}\NormalTok{(}\AttributeTok{RETICULATE\_PYTHON =} \StringTok{"D:}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{WinPython2}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{WPy64{-}3950}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{python{-}3.9.5.amd64}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In addition, I use R-Studio, R, and Python in a portable manner. It comes in handy when you need to switch between university and home computers, for example. Python can be downloaded through WinPython to be fully portable, and R-Studio supports it. I created all of the Neural Network images using the free website draw.io and im using QuillBot to compensate for my poor spelling skills. I would recommend jupyter lab or PyCharm if you are new to Python and have never used R.

\hypertarget{single-perceptron}{%
\chapter{Single Perceptron}\label{single-perceptron}}

Throughout this chapter, I will show you how to program a single Perceptron in Python using only the numpy package. Numpy uses a vectorizable math structure, which allows you to easily perform normal matrix multiplications with just an expression (i always interpret vectors as one dimensional matrices!). In most cases, it is just a matter of translating mathematical formulas into python code without altering their structure. Before we begin, let's identify the necessary parameters, which will be explained later:

Number of iterations over all the training dataset := \texttt{epochs}\\
Learning rate := \(\alpha =\) \texttt{alpha}\\
Bias value := \(\beta =\) \texttt{bias}
and the activation function:\\
\[ 
step(s)= 
\begin{cases}
    1,& s   \geq \beta\\
    0,& s < \beta
\end{cases}
\]
This function is named the heavyside-function and should be the easiest activation function to understand. If the weighted sum is smaller than the bias \(\beta\), it will send the value zero to the next neuron. It is the same in the brain. If there is not enough electricity, the neuron will not activate, and the next does not receive electricity.

The training dataset is the following:
\[
\left[
\begin{array}{cc|c}
x_i,_1 & x_i,_2 & y_i \\
\end{array}
\right]
\]
\[
\left[
\begin{array}{cc|c}
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 1 \\
\end{array}
\right]
\]
The provided training dataset contains the \texttt{X} matrix with two inputs for each scenario and the \texttt{Y} matrix with the correct output (each row contains the input and output of one scenario). If your looking closely, you can see that this is the OR-Gate. Later you will understand, why these type of problems are the only suitable things to do with a single neuron.

The needed python imports are the following:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ pyplot}
\end{Highlighting}
\end{Shaded}

( Do you see more imports than only the numpy package? \href{https://www.youtube.com/watch?v=dQw4w9WgXcQ}{Yes} or No )

Now that we have all the needed parameters and settings, i can give you a quick overview of the algorithm.

\hypertarget{neural-network-basics}{%
\section{Neural Network Basics}\label{neural-network-basics}}

The forward pass and the backward pass are the two primary parts of a NN. To calculate the output, we calculate the weighted sum of each input neuron with the layer's weights and evaluate the activation function with in the forward pass. We analyze the inaccuracy in the backward pass and modify the weights accordingly. That's it! That is exactly what a Neuronal Network is doing. Everything was explained to you. Enjoy your life\ldots{}
No, no, no, we'll take a closer look:)

In a single Perceptron, what is the forward pass? It's just like I mentioned, evaluating the activation function using the weighted sum, so for one scenario of the training dataset, you have:
\[
  step(W \cdot x^T_i) = y_i
\]
Using this formula to iterate over all scenarios in the training dataset is the standard approach\ldots{}
But I don't think that's the best way to put it because it's difficult to interpret it for all scenarios at the same time.

My next strategy is to use a single formula to account for all scenarios in the training dataset. If your data isn't too large, this is also a much faster method. First and foremost, we must interpret the new \(W\) and \(X\) dimensions.
We have \(X\) as:
\[
  X = \left[
  \begin{array}{cc}
  0 & 0 \\
  0 & 1 \\
  1 & 0 \\
  1 & 1 \\
  \end{array}
  \right]
\]
each row describes the inputs for each neuron in the scenario \(i\).\\
For the weights \(W\) do we have for example:
\[
  W =\left[
  \begin{array}{c}
  0.1 \\ 
  0.2 \\ 
  \end{array}
  \right]
\]
The new formula looks like this:
\[
  step(X * W) = Y
\]
The \(*\) symbol defines a matrix to matrix multiplication. For example if you take a look at the \(i\)-th row (scenario) of \(X\) you will see the following:
\[
  Y_i,_0 = step([X_i,_0 \cdot W_0,_0 + X_i,_1 \cdot W_1,_0 ])
\]
and \(Y_i,_0\) is the approximated output of the \(i\)-th scenario. Now can we look at the NN and compare the formula with it:\\
\includegraphics[width=0.5\textwidth,height=\textheight]{./img/NN_01_v4.png}

Yes it is the same, its the weighted sum of the inputs and evaluated the activation function with it, to calculate the output of the scenario \(i\).

\hypertarget{forward-pass}{%
\section{Forward pass}\label{forward-pass}}

Now can we create the so called \texttt{forward()} function in python:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ forward(x, w):}
  \ControlFlowTok{return}\NormalTok{( step(x }\OperatorTok{@}\NormalTok{ w) )}
\end{Highlighting}
\end{Shaded}

(Numpy provides us with the \texttt{@} symbol to make a matrix to matrix multiplication and the \texttt{.T} to transpose)

Because we want to put one dimensional matrices into the \texttt{step()} function, its necessary to use numpy for the if-else statement:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ step(s):}
  \ControlFlowTok{return}\NormalTok{( np.where(s }\OperatorTok{\textgreater{}=}\NormalTok{ bias, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

In the next step will we create an small example for the forward pass:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{])}
\NormalTok{W }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\FloatTok{0.1}\NormalTok{], }
\NormalTok{  [}\FloatTok{0.2}\NormalTok{]}
\NormalTok{])}
\NormalTok{bias }\OperatorTok{=} \DecValTok{1}
\NormalTok{Y\_approx }\OperatorTok{=}\NormalTok{ forward(X, W)}
\BuiltInTok{print}\NormalTok{(Y\_approx)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[0]
##  [0]
##  [0]
##  [0]]
\end{verbatim}

And these are all the generated outputs of our NN over all scenarios. Now do we need to calculate the error and adjust the weights accordingly.

\hypertarget{backward-pass}{%
\section{Backward pass}\label{backward-pass}}

We need the Delta-Rule to adjust the weights in a single Perceptron:
\[
  W(t+1) = W(t) + \Delta W(t)
\]
with
\[
  \Delta W(t) = \alpha \cdot X^{T} * (Y - \hat{Y})
\]
and \(\hat{Y}\) is the output of the NN. Translatet to code it is:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ backward(W, X, Y, alpha, Y\_approx):}
    \ControlFlowTok{return}\NormalTok{(W }\OperatorTok{+}\NormalTok{ alpha }\OperatorTok{*}\NormalTok{ X.T }\OperatorTok{@}\NormalTok{ (Y }\OperatorTok{{-}}\NormalTok{ Y\_approx))}
\end{Highlighting}
\end{Shaded}

With the result of the forward pass and and the correct outputs, do we have the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Y }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{]}
\NormalTok{])}
\NormalTok{alpha }\OperatorTok{=} \FloatTok{0.01}
\NormalTok{W }\OperatorTok{=}\NormalTok{ backward(W, X, Y, alpha, Y\_approx)}
\BuiltInTok{print}\NormalTok{(W)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[0.12]
##  [0.22]]
\end{verbatim}

and these are the new weight.

\hypertarget{single-perceptron-1}{%
\section{Single Perceptron}\label{single-perceptron-1}}

Now do we want to do the same process multiple times, to train the NN:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{])}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{]}
\NormalTok{])}
\NormalTok{W }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\FloatTok{0.1}\NormalTok{], }
\NormalTok{  [}\FloatTok{0.2}\NormalTok{]}
\NormalTok{])}
\NormalTok{alpha }\OperatorTok{=} \FloatTok{0.01}
\NormalTok{bias }\OperatorTok{=} \DecValTok{1}
\NormalTok{epochs }\OperatorTok{=} \DecValTok{100}

\NormalTok{errors }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(epochs):}
\NormalTok{  Y\_approx }\OperatorTok{=}\NormalTok{ forward(X, W)}
\NormalTok{  errors.append(Y }\OperatorTok{{-}}\NormalTok{ Y\_approx)}
\NormalTok{  W }\OperatorTok{=}\NormalTok{ backward(W, X, Y, alpha, Y\_approx)}
\end{Highlighting}
\end{Shaded}

The KNN is trained. In the next step, we will analyze the errors of each epoch. The best way to do so is to measure the mean-square-error with the following formula:
\[
  Errors = \frac{1}{2} \cdot \sum(Y-\hat{Y})^2
\]
or as python code:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mean\_square\_error(error):}
  \ControlFlowTok{return}\NormalTok{( }\FloatTok{0.5} \OperatorTok{*}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(error }\OperatorTok{**} \DecValTok{2}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

Now do we need to calculate the mean-square-error for each element in the list \texttt{errors} which can be performed with \texttt{map()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mean\_square\_errors }\OperatorTok{=}\NormalTok{ np.array(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{map}\NormalTok{(mean\_square\_error, errors)))}
\end{Highlighting}
\end{Shaded}

To plot the errors, im using the following function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ plot\_error(errors, title):}
\NormalTok{  x }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(errors)))}
\NormalTok{  y }\OperatorTok{=}\NormalTok{ np.array(errors)}
\NormalTok{  pyplot.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{))}
\NormalTok{  pyplot.plot(x, y, }\StringTok{"g"}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{  pyplot.xlabel(}\StringTok{"Iterations"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.ylabel(}\StringTok{"Mean Square Error"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.title(title)}
\NormalTok{  pyplot.ylim(}\OperatorTok{{-}}\FloatTok{0.01}\NormalTok{,}\BuiltInTok{max}\NormalTok{(errors)}\OperatorTok{*}\FloatTok{1.2}\NormalTok{)}
\NormalTok{  pyplot.show()}
  
  
\NormalTok{plot\_error(mean\_square\_errors, }\StringTok{"Single Perceptron"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{gitbook-demo_files/figure-latex/0_9-1.pdf}

If you survived until now, you have learned how to program a single Perceptron!

\hypertarget{why-does-it-work}{%
\section{Why does it work?}\label{why-does-it-work}}

A single Perceptron with the heavyside activationfunction defines a classifier with the outputs 0 and 1. To find the correct solution, it needs to define a combination of weights and bias so that the inputs can be transferred to the groups \(X \cdot W \geq \beta\) or \(X \cdot W < \beta\). The single Perceptron only converges to the given results, if the inputs could be splitted into groups by a straight line in the graph. For example the OR-Gate:

\begin{verbatim}
## (-0.1, 1.1)
## (-0.1, 1.1)
\end{verbatim}

\includegraphics{gitbook-demo_files/figure-latex/unnamed-chunk-4-3.pdf}

The red points (equals \(Y_i=1\)) and the black points (equals \(Y_i=0\)) can be split up with a straight line. If this isn't possible, as in the XOR-Gate, the single Perceptron will never find a combination of bias and weights to perform well. This explains why we need atleast multiple layers, as described in the chapter on multilayer Perceptrons. \href{https://towardsdatascience.com/how-neural-networks-solve-the-xor-problem-59763136bdd7}{Here} can you find a good source with more explanation.

\hypertarget{appendix-complete-code}{%
\section{Appendix (complete code)}\label{appendix-complete-code}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ pyplot}


\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{])}
\NormalTok{Y }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{]}
\NormalTok{])}
\NormalTok{W }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\FloatTok{0.1}\NormalTok{], }
\NormalTok{  [}\FloatTok{0.2}\NormalTok{]}
\NormalTok{])}
\NormalTok{alpha }\OperatorTok{=} \FloatTok{0.01}
\NormalTok{bias }\OperatorTok{=} \DecValTok{1}
\NormalTok{train\_n }\OperatorTok{=} \DecValTok{100}

\KeywordTok{def}\NormalTok{ step(s):}
  \ControlFlowTok{return}\NormalTok{( np.where(s }\OperatorTok{\textgreater{}=}\NormalTok{ bias, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{) )}


\KeywordTok{def}\NormalTok{ forward(X, W):}
  \ControlFlowTok{return}\NormalTok{( step(X }\OperatorTok{@}\NormalTok{ W) )}

\KeywordTok{def}\NormalTok{ backward(W, X, Y, alpha, Y\_approx):}
  \ControlFlowTok{return}\NormalTok{(W }\OperatorTok{+}\NormalTok{ alpha }\OperatorTok{*}\NormalTok{ X.T }\OperatorTok{@}\NormalTok{ (Y }\OperatorTok{{-}}\NormalTok{ Y\_approx))}
  
  
\NormalTok{errors }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(train\_n):}
\NormalTok{  Y\_approx }\OperatorTok{=}\NormalTok{ forward(X, W)}
\NormalTok{  errors.append(Y }\OperatorTok{{-}}\NormalTok{ Y\_approx)}
\NormalTok{  W }\OperatorTok{=}\NormalTok{ backward(W, X, Y, alpha, Y\_approx)}
  
  
  
\KeywordTok{def}\NormalTok{ mean\_square\_error(error):}
  \ControlFlowTok{return}\NormalTok{( }\FloatTok{0.5} \OperatorTok{*}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(error }\OperatorTok{**} \DecValTok{2}\NormalTok{) )}


\NormalTok{mean\_square\_errors }\OperatorTok{=}\NormalTok{ np.array(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{map}\NormalTok{(mean\_square\_error, errors)))}


\KeywordTok{def}\NormalTok{ plot\_error(errors, title):}
\NormalTok{  x }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(errors)))}
\NormalTok{  y }\OperatorTok{=}\NormalTok{ np.array(errors)}
\NormalTok{  pyplot.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{))}
\NormalTok{  pyplot.plot(x, y, }\StringTok{"g"}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{  pyplot.xlabel(}\StringTok{"Iterations"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.ylabel(}\StringTok{"Mean Square Error"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.title(title)}
\NormalTok{  pyplot.ylim(}\OperatorTok{{-}}\FloatTok{0.01}\NormalTok{,}\BuiltInTok{max}\NormalTok{(errors)}\OperatorTok{*}\FloatTok{1.2}\NormalTok{)}
\NormalTok{  pyplot.show()}
  
  
\NormalTok{plot\_error(mean\_square\_errors, }\StringTok{"Single Perceptron"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{adding-trainable-bias}{%
\chapter{Adding trainable Bias}\label{adding-trainable-bias}}

The single Perceptron, you saw in the previews chapter had the following activation function:
\[ 
step(s)= 
\begin{cases}
    1,& s   \geq \beta\\
    0,& s < \beta
\end{cases}
\]
with \(\beta = 1\). It is the perfect \(\beta\) for the given training dataset. But what happens if you shift the training data for example adding \(-5\) to the \(X\) matrix? Now it will never find the correct answer. That is because you need to select the \(\beta\) accordingly. But this wouldn't be intelligent to search for each dataset the optimal \(\beta\) by hand.

\hypertarget{generalising-the-bias}{%
\section{Generalising the Bias}\label{generalising-the-bias}}

First of all do we need to generalise the use of the bias, stating with the generalization of the activation function:
\[ 
step(s)= 
\begin{cases}
    1,& s   \geq 0\\
    0,& s < 0
\end{cases}
\]
Now can we list it in the weighted sum:
\[
  step(X * W - \beta) = Y
\]
But we have the same problem as previews, because we need to specify the \(\beta\) explicit. Adding the bias to the training process is done by adding ones on the right side of the \(X\) matrix and adding the negative bias to the last row of \(W\). The output of one scenario is calculated as the following:
\[
  Y_i,_0 = step([X_i,_0 \cdot W_0,_0 + X_i,_1 \cdot W_1,_0 + X_i,_2 \cdot W_2,_0]) = step([X_i,_0 \cdot W_0,_0 + X_i,_1 \cdot W_1,_0 - \beta])
\]
The resulting NN includes the bias into the re-adjusting process of the backward pass, because of that we will generate a random number for the bias that will be corrected anyway.

Now we have a NN that looks like all the other pictures of a single Perceptron in the internet:\\
\includegraphics[width=0.5\textwidth,height=\textheight]{./img/NN_02.png}\\
The same step can be made with the following python code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{])}\OperatorTok{{-}}\DecValTok{5}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.append(X, np.array([np.ones(}\BuiltInTok{len}\NormalTok{(X))]).T, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{W }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\FloatTok{0.1}\NormalTok{], }
\NormalTok{  [}\FloatTok{0.2}\NormalTok{]}
\NormalTok{])}
\NormalTok{W }\OperatorTok{=}\NormalTok{ np.append(W, }\OperatorTok{{-}}\NormalTok{np.array([np.random.random(}\BuiltInTok{len}\NormalTok{(W[}\DecValTok{0}\NormalTok{]))]).T, axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"X: }\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, X)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"W: }\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, W)}
\end{Highlighting}
\end{Shaded}

We added \(-5\) to the \(X\) matrix to simulate the problem of shifted data, added ones on the left side of \(X\) and added negative random numbers in \((0,1)\) to the weights. Yes, if you would have a clue, what \(\beta\) would be great for the given problem, its better to choose it explicit. The new NN is slower, because it needs to find a good \(\beta\) by it self.

\hypertarget{appendix-complete-code-1}{%
\section{Appendix (complete code)}\label{appendix-complete-code-1}}

The complete code is the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.random.seed(}\DecValTok{0}\NormalTok{)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{]) }\OperatorTok{{-}} \DecValTok{5}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.append(X, np.array([np.ones(}\BuiltInTok{len}\NormalTok{(X))]).T, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}

\NormalTok{W }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\FloatTok{0.1}\NormalTok{], }
\NormalTok{  [}\FloatTok{0.2}\NormalTok{]}
\NormalTok{])}
\NormalTok{W }\OperatorTok{=}\NormalTok{ np.append(W, }\OperatorTok{{-}}\NormalTok{np.array([np.random.random(}\BuiltInTok{len}\NormalTok{(W[}\DecValTok{0}\NormalTok{]))]).T, axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)}

\NormalTok{Y }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{]}
\NormalTok{])}

\NormalTok{alpha }\OperatorTok{=} \FloatTok{0.01}
\NormalTok{epochs }\OperatorTok{=} \DecValTok{1000}

\KeywordTok{def}\NormalTok{ step(s):}
  \ControlFlowTok{return}\NormalTok{( np.where(s }\OperatorTok{\textgreater{}=} \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{) )}


\KeywordTok{def}\NormalTok{ forward(X, W):}
  \ControlFlowTok{return}\NormalTok{( step(X }\OperatorTok{@}\NormalTok{ W) )}

\KeywordTok{def}\NormalTok{ backward(W, X, Y, alpha, Y\_approx):}
    \ControlFlowTok{return}\NormalTok{(W }\OperatorTok{+}\NormalTok{ alpha }\OperatorTok{*}\NormalTok{ X.T }\OperatorTok{@}\NormalTok{ (Y }\OperatorTok{{-}}\NormalTok{ Y\_approx))}
  
  
\NormalTok{errors }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(epochs):}
\NormalTok{  Y\_approx }\OperatorTok{=}\NormalTok{ forward(X, W)}
\NormalTok{  errors.append(Y }\OperatorTok{{-}}\NormalTok{ Y\_approx)}
\NormalTok{  W }\OperatorTok{=}\NormalTok{ backward(W, X, Y, alpha, Y\_approx)}
  
  
  
\KeywordTok{def}\NormalTok{ mean\_square\_error(error):}
  \ControlFlowTok{return}\NormalTok{( }\FloatTok{0.5} \OperatorTok{*}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(error }\OperatorTok{**} \DecValTok{2}\NormalTok{) )}


\NormalTok{mean\_square\_errors }\OperatorTok{=}\NormalTok{ np.array(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{map}\NormalTok{(mean\_square\_error, errors)))}


\KeywordTok{def}\NormalTok{ plot\_error(errors, title):}
\NormalTok{  x }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(errors)))}
\NormalTok{  y }\OperatorTok{=}\NormalTok{ np.array(errors)}
\NormalTok{  pyplot.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{))}
\NormalTok{  pyplot.plot(x, y, }\StringTok{"g"}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{  pyplot.xlabel(}\StringTok{"Iterations"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.ylabel(}\StringTok{"Mean Square Error"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.title(title)}
\NormalTok{  pyplot.ylim(}\OperatorTok{{-}}\FloatTok{0.01}\NormalTok{,}\BuiltInTok{max}\NormalTok{(errors)}\OperatorTok{*}\FloatTok{1.2}\NormalTok{)}
\NormalTok{  pyplot.show()}
  
  
\NormalTok{plot\_error(mean\_square\_errors, }\StringTok{"Single Perceptron with trainable bias"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{gitbook-demo_files/figure-latex/1_2-5.pdf}

\hypertarget{multilayer-perceptrons-mlp}{%
\chapter{Multilayer Perceptrons (MLP)}\label{multilayer-perceptrons-mlp}}

Multiple layers of neurons make up a multilayer Perceptron. As a result, we must calculate the forward pass several times, as well as the backward pass. First and foremost, do we need to broaden some definitions in order to support this behavior? The NN of the following image is what we want to do:\\
\includegraphics[width=1\textwidth,height=\textheight]{./img/NN_03_new.png}

It's my own definition of layers since I believed it would be easier to transition from \(n\) to \(n+1\) hidden layers if they were displayed as indicated in the image. By evaluating \(f(IN^{layer} \cdot W^{layer}) = OUT^{layer}\) and just transferring the result to the next layer like \(OUT^{layer} = IN^{layer+1}\), with \(f\) as the chosen activation function, you can see that each layer has the identical procedure in the forward pass.
We'll choose the sigmoid function as the activation function \(f\) because it has a simple deviation \(f'\) which is used for the backward pass and behaves similarly to the heavyside function with an output between zero and one.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ sigmoid(x):}
  \ControlFlowTok{return} \FloatTok{1.0} \OperatorTok{/}\NormalTok{ (}\FloatTok{1.0} \OperatorTok{+}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{x))}

\KeywordTok{def}\NormalTok{ deriv\_sigmoid(x):}
  \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x)}
\end{Highlighting}
\end{Shaded}

Additionaly will we choose the XOR-Gate as training dataset and generate random weights in a very generic approach:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{]) }

\NormalTok{Y }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{0}\NormalTok{]}
\NormalTok{])}

\NormalTok{n\_input }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(X[}\DecValTok{0}\NormalTok{])}
\NormalTok{n\_output }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(Y[}\DecValTok{0}\NormalTok{])}
\NormalTok{hidden\_layer\_neurons }\OperatorTok{=}\NormalTok{ [}\DecValTok{2}\NormalTok{] }\CommentTok{\# the 2 means that there is one hidden layer with 2 neurons}

\KeywordTok{def}\NormalTok{ generate\_weights(n\_input, n\_output, hidden\_layer\_neurons):}
\NormalTok{  W }\OperatorTok{=}\NormalTok{ []}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(hidden\_layer\_neurons)}\OperatorTok{+}\DecValTok{1}\NormalTok{):}
    \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{==} \DecValTok{0}\NormalTok{: }\CommentTok{\# first layer}
\NormalTok{      W.append(np.random.random((n\_input}\OperatorTok{+}\DecValTok{1}\NormalTok{, hidden\_layer\_neurons[i])))}
    \ControlFlowTok{elif}\NormalTok{ i }\OperatorTok{==} \BuiltInTok{len}\NormalTok{(hidden\_layer\_neurons): }\CommentTok{\# last layer}
\NormalTok{      W.append(np.random.random((hidden\_layer\_neurons[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{, n\_output)))}
    \ControlFlowTok{else}\NormalTok{: }\CommentTok{\# middle layers}
\NormalTok{      W.append(np.random.random((hidden\_layer\_neurons[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{, hidden\_layer\_neurons[i])))}
   
  \ControlFlowTok{return}\NormalTok{(W)}

\NormalTok{W }\OperatorTok{=}\NormalTok{ generate\_weights(n\_input, n\_output, hidden\_layer\_neurons)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"W[0]: }\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, W[}\DecValTok{0}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"W[1]: }\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, W[}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## W[0]: 
##  [[0.5488135  0.71518937]
##  [0.60276338 0.54488318]
##  [0.4236548  0.64589411]]
## W[1]: 
##  [[0.43758721]
##  [0.891773  ]
##  [0.96366276]]
\end{verbatim}

The neurons for the hidden layers are generated using the \texttt{hidden\_layer\_neurons} list, while the input and output layer neurons are calculated from the training dataset. \texttt{hidden\_layer\_neurons\ =\ {[}4,2{]}}, for example, can construct two hidden layers with 4 and 2 neurons. I didn't choose the bias because it is automatically rectified.
Is it now necessary to construct a helper function to add the biases to the last column of the inputs, like follows:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ add\_ones\_to\_input(x):}
  \ControlFlowTok{return}\NormalTok{(np.append(x, np.array([np.ones(}\BuiltInTok{len}\NormalTok{(x))]).T, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{forward-pass-1}{%
\section{Forward pass}\label{forward-pass-1}}

The structur of the new forward function looks exactly like in the single Perceptron:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ forward(x, w):}
  \ControlFlowTok{return}\NormalTok{( sigmoid(x }\OperatorTok{@}\NormalTok{ w) )}
\end{Highlighting}
\end{Shaded}

Now we have everything to calculate the forward pass of the NN from above with the generated weights step by step:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{IN }\OperatorTok{=}\NormalTok{ []}
\NormalTok{OUT }\OperatorTok{=}\NormalTok{ []}

\CommentTok{\# layer 0}
\NormalTok{i }\OperatorTok{=} \DecValTok{0}
\NormalTok{IN.append( add\_ones\_to\_input(X) )}
\NormalTok{OUT.append( forward(IN[i], W[i]) )}

\CommentTok{\# layer 1}
\NormalTok{i }\OperatorTok{=} \DecValTok{1}
\NormalTok{IN.append( add\_ones\_to\_input(OUT[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]) )}
\NormalTok{OUT.append( forward(IN[i], W[i]) )}

\CommentTok{\# error}
\NormalTok{Y}\OperatorTok{{-}}\NormalTok{OUT[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## array([[-0.85974823],
##        [ 0.12242041],
##        [ 0.12015376],
##        [-0.89115202]])
\end{verbatim}

Thats all! We calculated the forward pass in a very generic way for the NN with 2 input neurons, 2 hidden neurons and 1 output neuron for all 4 scenarios at the same time. Sadly is the forward pass the easiest part of the multilayer Perceptron :)

\hypertarget{backward-pass-1}{%
\section{Backward pass}\label{backward-pass-1}}

The weights will be adjusted using the backpropagation algorithm, which is a variant of the descent gradient algorithm. Calculating the sensitives of the outputs according to the activation function multiplied by the error that occurred is done in the output layer. On all other layers, it's calculated by reversing the previously calculated gradient, splitting it up on each neuron by the previews weights, and multiplying the sensitivity of that layer's outputs by the activation function's sensitivity. The following is the formula:
\[
  grad^i= 
\begin{cases}
    f^`(OUT^i) \cdot (Y-OUT^i),& i =\text{last layer}\\
    f^`(OUT^i) \cdot (grad^{i+1} * \widetilde{W}^{i+1\ T}),& \text{else}
\end{cases}
\]
with \(\widetilde{W}\) as the weights of the layer \(i\), without the connection to the bias neuron. You can calculate \(\widetilde{W}\) in our datastructure by removing the last row.\\
The gradients for the backward pass are calculated using the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grad }\OperatorTok{=}\NormalTok{ [}\VariableTok{None}\NormalTok{] }\OperatorTok{*} \DecValTok{2}

\CommentTok{\# layer 1}
\NormalTok{i }\OperatorTok{=} \DecValTok{1}
\NormalTok{grad[i] }\OperatorTok{=}\NormalTok{ deriv\_sigmoid(OUT[i]) }\OperatorTok{*}\NormalTok{ (Y}\OperatorTok{{-}}\NormalTok{OUT[i])}

\CommentTok{\# layer 0}
\NormalTok{i }\OperatorTok{=} \DecValTok{0}
\NormalTok{grad[i] }\OperatorTok{=}\NormalTok{ deriv\_sigmoid(OUT[i]) }\OperatorTok{*}\NormalTok{(grad[i}\OperatorTok{+}\DecValTok{1}\NormalTok{] }\OperatorTok{@}\NormalTok{ W[i}\OperatorTok{+}\DecValTok{1}\NormalTok{][}\DecValTok{0}\NormalTok{:}\BuiltInTok{len}\NormalTok{(W[i}\OperatorTok{+}\DecValTok{1}\NormalTok{])}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].T) }\CommentTok{\# without bias weights}
\end{Highlighting}
\end{Shaded}

You can now examine the gradient of the last layer and the direction in which it is displayed:

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"Y: }\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,Y)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"OUT: }\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,OUT[}\DecValTok{1}\NormalTok{])}
\BuiltInTok{print}\NormalTok{(}\StringTok{"grad: }\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{,grad[}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Y: 
##  [[0]
##  [1]
##  [1]
##  [0]]
## OUT: 
##  [[0.85974823]
##  [0.87757959]
##  [0.87984624]
##  [0.89115202]]
## grad: 
##  [[-0.10366948]
##  [ 0.01315207]
##  [ 0.01270227]
##  [-0.08644184]]
\end{verbatim}

The gradient appears to be pointing in the direction of drifting the output \(OUT\) closer to the desired output \(Y\). The gradient descent algorithm accomplishes exactly that.
The weights with the gradients and the learningrate \(\alpha\) must then be adjusted according to the direction of the gradients using the following formula:
\[
  W^i_{new} = W^i_{old} + \alpha \cdot ( IN^{i\ T} * grad^i) 
\]
After the first epoch, we get the following results for the adjusted weights in the example above:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha }\OperatorTok{=} \FloatTok{0.03}

\NormalTok{W[}\DecValTok{1}\NormalTok{] }\OperatorTok{=}\NormalTok{ W[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\NormalTok{ alpha }\OperatorTok{*}\NormalTok{ (IN[}\DecValTok{1}\NormalTok{].T }\OperatorTok{@}\NormalTok{ grad[}\DecValTok{1}\NormalTok{]) }
\NormalTok{W[}\DecValTok{0}\NormalTok{] }\OperatorTok{=}\NormalTok{ W[}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ alpha }\OperatorTok{*}\NormalTok{ (IN[}\DecValTok{0}\NormalTok{].T }\OperatorTok{@}\NormalTok{ grad[}\DecValTok{0}\NormalTok{]) }
\end{Highlighting}
\end{Shaded}

This was the forward and backward pass process in a multilayer Perceptron with two input neurons, two hidden layer neurons, and one output neuron for one epoch. It's simple to use the above code to make a more generic NN with a variable number of hidden layers and variable training datasets for a given number of epochs, as shown in the appendix below.

\hypertarget{appendix-complete-code-2}{%
\section{Appendix (complete code)}\label{appendix-complete-code-2}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ pyplot}
\NormalTok{np.random.seed(}\DecValTok{0}\NormalTok{)}

\NormalTok{X }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{],}
\NormalTok{])}

\NormalTok{Y }\OperatorTok{=}\NormalTok{ np.array([}
\NormalTok{  [}\DecValTok{0}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{1}\NormalTok{],}
\NormalTok{  [}\DecValTok{0}\NormalTok{]}
\NormalTok{])}

\NormalTok{n\_input }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(X[}\DecValTok{0}\NormalTok{])}
\NormalTok{n\_output }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(Y[}\DecValTok{0}\NormalTok{])}
\NormalTok{hidden\_layer\_neurons }\OperatorTok{=}\NormalTok{ [}\DecValTok{2}\NormalTok{]}


\KeywordTok{def}\NormalTok{ generate\_weights(n\_input, n\_output, hidden\_layer\_neurons):}
\NormalTok{  W }\OperatorTok{=}\NormalTok{ []}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(hidden\_layer\_neurons)}\OperatorTok{+}\DecValTok{1}\NormalTok{):}
    \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{==} \DecValTok{0}\NormalTok{: }\CommentTok{\# first layer}
\NormalTok{      W.append(np.random.random((n\_input }\OperatorTok{+} \DecValTok{1}\NormalTok{, hidden\_layer\_neurons[i])))}
    \ControlFlowTok{elif}\NormalTok{ i }\OperatorTok{==} \BuiltInTok{len}\NormalTok{(hidden\_layer\_neurons): }\CommentTok{\# last layer}
\NormalTok{      W.append(np.random.random((hidden\_layer\_neurons[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{, n\_output)))}
    \ControlFlowTok{else}\NormalTok{: }\CommentTok{\# middle layers}
\NormalTok{      W.append(np.random.random((hidden\_layer\_neurons[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{, hidden\_layer\_neurons[i])))}
  \ControlFlowTok{return}\NormalTok{(W)}

\KeywordTok{def}\NormalTok{ add\_ones\_to\_input(x):}
  \ControlFlowTok{return}\NormalTok{(np.append(x, np.array([np.ones(}\BuiltInTok{len}\NormalTok{(x))]).T, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{))}


\NormalTok{W }\OperatorTok{=}\NormalTok{ generate\_weights(n\_input, n\_output, hidden\_layer\_neurons)}


\KeywordTok{def}\NormalTok{ sigmoid(x):}
  \ControlFlowTok{return} \FloatTok{1.0} \OperatorTok{/}\NormalTok{ (}\FloatTok{1.0} \OperatorTok{+}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{x))}

\KeywordTok{def}\NormalTok{ deriv\_sigmoid(x):}
  \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x)}


\KeywordTok{def}\NormalTok{ forward(x, w):}
  \ControlFlowTok{return}\NormalTok{( sigmoid(x }\OperatorTok{@}\NormalTok{ w) )}

\KeywordTok{def}\NormalTok{ backward(IN, OUT, W, Y, grad, k):}
  \ControlFlowTok{if}\NormalTok{ k }\OperatorTok{==} \BuiltInTok{len}\NormalTok{(grad)}\OperatorTok{{-}}\DecValTok{1}\NormalTok{:}
\NormalTok{    grad[k] }\OperatorTok{=}\NormalTok{ deriv\_sigmoid(OUT[k]) }\OperatorTok{*}\NormalTok{ (Y}\OperatorTok{{-}}\NormalTok{OUT[k])}
  \ControlFlowTok{else}\NormalTok{:}
\NormalTok{    grad[k] }\OperatorTok{=}\NormalTok{ deriv\_sigmoid(OUT[k]) }\OperatorTok{*}\NormalTok{(grad[k}\OperatorTok{+}\DecValTok{1}\NormalTok{] }\OperatorTok{@}\NormalTok{ W[k}\OperatorTok{+}\DecValTok{1}\NormalTok{][}\DecValTok{0}\NormalTok{:}\BuiltInTok{len}\NormalTok{(W[k}\OperatorTok{+}\DecValTok{1}\NormalTok{])}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].T)}
  \ControlFlowTok{return}\NormalTok{(grad)}

\NormalTok{alpha }\OperatorTok{=} \FloatTok{0.03}
\NormalTok{errors }\OperatorTok{=}\NormalTok{ []}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{40000}\NormalTok{):}
\NormalTok{  IN }\OperatorTok{=}\NormalTok{ []}
\NormalTok{  OUT }\OperatorTok{=}\NormalTok{ []}
\NormalTok{  grad }\OperatorTok{=}\NormalTok{ [}\VariableTok{None}\NormalTok{]}\OperatorTok{*}\BuiltInTok{len}\NormalTok{(W)}
  \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
    \ControlFlowTok{if}\NormalTok{ k}\OperatorTok{==}\DecValTok{0}\NormalTok{:}
\NormalTok{      IN.append(add\_ones\_to\_input(X))}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{      IN.append(add\_ones\_to\_input(OUT[k}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]))}
\NormalTok{    OUT.append(forward(x}\OperatorTok{=}\NormalTok{IN[k], w}\OperatorTok{=}\NormalTok{W[k]))}
    
\NormalTok{  errors.append(Y }\OperatorTok{{-}}\NormalTok{ OUT[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
    
  \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
\NormalTok{    grad }\OperatorTok{=}\NormalTok{ backward(IN, OUT, W, Y, grad, k) }
    
  \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
\NormalTok{    W[k] }\OperatorTok{=}\NormalTok{ W[k] }\OperatorTok{+}\NormalTok{ alpha }\OperatorTok{*}\NormalTok{ (IN[k].T }\OperatorTok{@}\NormalTok{ grad[k])}



\KeywordTok{def}\NormalTok{ mean\_square\_error(error):}
  \ControlFlowTok{return}\NormalTok{( }\FloatTok{0.5} \OperatorTok{*}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(error }\OperatorTok{**} \DecValTok{2}\NormalTok{) )}

\NormalTok{mean\_square\_errors }\OperatorTok{=}\NormalTok{ np.array(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{map}\NormalTok{(mean\_square\_error, errors)))}

\KeywordTok{def}\NormalTok{ plot\_error(errors, title):}
\NormalTok{  x }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(errors)))}
\NormalTok{  y }\OperatorTok{=}\NormalTok{ np.array(errors)}
\NormalTok{  pyplot.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{))}
\NormalTok{  pyplot.plot(x, y, }\StringTok{"g"}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{  pyplot.xlabel(}\StringTok{"Iterations"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.ylabel(}\StringTok{"Mean Square Error"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.title(title)}
\NormalTok{  pyplot.ylim(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{  pyplot.show()}
  
\NormalTok{plot\_error(mean\_square\_errors, }\StringTok{"Mean{-}Square{-}Errors of MLP 2x2x1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{gitbook-demo_files/figure-latex/2_9-7.pdf}

\hypertarget{mlp-example-credit-default}{%
\chapter{MLP example (Credit Default)}\label{mlp-example-credit-default}}

Now can we use our generic MLP model from the previews chapter to forecast real life credit defaults. The csv can be downloaded from \href{https://www.kaggle.com/laotse/credit-risk-dataset}{Kaggle Data Source} or from my \href{https://github.com/AxelCode-R/GitBook}{github repo} in the example\_data folder.

\hypertarget{loading-and-analysing-the-data}{%
\section{Loading and analysing the data}\label{loading-and-analysing-the-data}}

First all do we need to load the csv via pandas and analyse it:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ pyplot}
\NormalTok{pd.set\_option(}\StringTok{\textquotesingle{}display.float\_format\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}}\SpecialCharTok{\{:.2f\}}\StringTok{\textquotesingle{}}\NormalTok{.}\BuiltInTok{format}\NormalTok{)}
\NormalTok{np.random.seed(}\DecValTok{0}\NormalTok{)}

\NormalTok{data }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"example\_data/credit\_risk\_dataset.csv"}\NormalTok{).fillna(}\DecValTok{0}\NormalTok{)}

\NormalTok{data.shape}
\NormalTok{data.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (32581, 12)
##    person_age  person_income person_home_ownership  person_emp_length loan_intent loan_grade  loan_amnt  loan_int_rate  loan_status  loan_percent_income cb_person_default_on_file  cb_person_cred_hist_length
## 0          22          59000                  RENT             123.00    PERSONAL          D      35000          16.02            1                 0.59                         Y                           3
## 1          21           9600                   OWN               5.00   EDUCATION          B       1000          11.14            0                 0.10                         N                           2
## 2          25           9600              MORTGAGE               1.00     MEDICAL          C       5500          12.87            1                 0.57                         N                           3
## 3          23          65500                  RENT               4.00     MEDICAL          C      35000          15.23            1                 0.53                         N                           2
## 4          24          54400                  RENT               8.00     MEDICAL          C      35000          14.27            1                 0.55                         Y                           4
\end{verbatim}

You can find the more details about the columns on kaggle, additionaly to the next table:\\
\includegraphics[width=1\textwidth,height=\textheight]{./img/credit_default_kaggle_data_info.png}
The important column is \texttt{load\_status} that determinates the customers credit default and is used as the correct outputs \(Y\). All other columns are considered as the input matrix \(X\). First of all do we need to analyse the underlying data a little bit more. The columns with numerical data are visualized in the following charts:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l }\OperatorTok{=}\NormalTok{ [}\StringTok{"person\_age"}\NormalTok{, }\StringTok{"person\_income"}\NormalTok{, }\StringTok{"person\_emp\_length"}\NormalTok{, }\StringTok{"loan\_amnt"}\NormalTok{, }\StringTok{"loan\_int\_rate"}\NormalTok{, }\StringTok{"loan\_status"}\NormalTok{, }\StringTok{"loan\_percent\_income"}\NormalTok{, }\StringTok{"cb\_person\_cred\_hist\_length"}\NormalTok{]}
\NormalTok{data[l].hist(bins}\OperatorTok{=}\DecValTok{10}\NormalTok{,figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{8}\NormalTok{,}\DecValTok{8}\NormalTok{))}
\NormalTok{pyplot.tight\_layout()}
\NormalTok{pyplot.show()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## array([[<AxesSubplot:title={'center':'person_age'}>,
##         <AxesSubplot:title={'center':'person_income'}>,
##         <AxesSubplot:title={'center':'person_emp_length'}>],
##        [<AxesSubplot:title={'center':'loan_amnt'}>,
##         <AxesSubplot:title={'center':'loan_int_rate'}>,
##         <AxesSubplot:title={'center':'loan_status'}>],
##        [<AxesSubplot:title={'center':'loan_percent_income'}>,
##         <AxesSubplot:title={'center':'cb_person_cred_hist_length'}>,
##         <AxesSubplot:>]], dtype=object)
\end{verbatim}

\includegraphics{gitbook-demo_files/figure-latex/unnamed-chunk-6-9.pdf}
All other input columns are categorical that cant be processed by Neuronal Networks. Luckily there exist some methods to convert the categories to numbers for example with Ordinal Encoding, One hot Encoding or Embedding (more information can be found \href{https://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/}{here}). I will choose the Ordinal Encoding for our dataset, because it is the simplest method. Ordinal Encoding just maps numbers to the categories. The best would be to arrange the categories as good as possible and just map numbers to it like in the following code:

All other input columns are categorical, and Neuronal Networks cannot process them. Fortunately, there are some methods for converting categories to numbers, such as Ordinal Encoding, One-hot Encoding, and Embedding (more information can be found \href{https://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/}{here}). Because it is the easiest way, I will use Ordinal Encoding for our dataset. Ordinal Encoding simply converts numbers into categories. The best technique would be to organize each column's categories in a meaningful way before assigning numbers to them, as seen in the code below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{=}\NormalTok{ data.replace(\{}\StringTok{"Y"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"N"}\NormalTok{:}\DecValTok{0}\NormalTok{\})}
\NormalTok{data[}\StringTok{"person\_home\_ownership"}\NormalTok{] }\OperatorTok{=}\NormalTok{ data[}\StringTok{"person\_home\_ownership"}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}OWN\textquotesingle{}}\NormalTok{:}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}RENT\textquotesingle{}}\NormalTok{:}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}MORTGAGE\textquotesingle{}}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}OTHER\textquotesingle{}}\NormalTok{:}\DecValTok{4}\NormalTok{\})}
\NormalTok{data[}\StringTok{"loan\_intent"}\NormalTok{] }\OperatorTok{=}\NormalTok{ data[}\StringTok{"loan\_intent"}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}PERSONAL\textquotesingle{}}\NormalTok{:}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}EDUCATION\textquotesingle{}}\NormalTok{:}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}MEDICAL\textquotesingle{}}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}VENTURE\textquotesingle{}}\NormalTok{:}\DecValTok{4}\NormalTok{, }\StringTok{\textquotesingle{}HOMEIMPROVEMENT\textquotesingle{}}\NormalTok{:}\DecValTok{5}\NormalTok{,}\StringTok{\textquotesingle{}DEBTCONSOLIDATION\textquotesingle{}}\NormalTok{:}\DecValTok{6}\NormalTok{\})}
\NormalTok{data[}\StringTok{"loan\_grade"}\NormalTok{] }\OperatorTok{=}\NormalTok{ data[}\StringTok{"loan\_grade"}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{:}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{:}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}D\textquotesingle{}}\NormalTok{:}\DecValTok{4}\NormalTok{, }\StringTok{\textquotesingle{}E\textquotesingle{}}\NormalTok{:}\DecValTok{5}\NormalTok{, }\StringTok{\textquotesingle{}F\textquotesingle{}}\NormalTok{:}\DecValTok{6}\NormalTok{, }\StringTok{\textquotesingle{}G\textquotesingle{}}\NormalTok{:}\DecValTok{7}\NormalTok{\})}

\NormalTok{data.head()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    person_age  person_income  person_home_ownership  person_emp_length  loan_intent  loan_grade  loan_amnt  loan_int_rate  loan_status  loan_percent_income  cb_person_default_on_file  cb_person_cred_hist_length
## 0          22          59000                      2             123.00            1           4      35000          16.02            1                 0.59                          1                           3
## 1          21           9600                      1               5.00            2           2       1000          11.14            0                 0.10                          0                           2
## 2          25           9600                      3               1.00            3           3       5500          12.87            1                 0.57                          0                           3
## 3          23          65500                      2               4.00            3           3      35000          15.23            1                 0.53                          0                           2
## 4          24          54400                      2               8.00            3           3      35000          14.27            1                 0.55                          1                           4
\end{verbatim}

It's not the most precise method for encoding categorical data, but it's the simplest and doesn't add to the input matrix's size.

It's also crucial to standardize the data, which improves the learning process's stability and speed (for more \href{https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/}{information}). Because we're using the sigmoid function, we'll normalize the data to the interval \([0,1]\). The following function will work for the entire numpy array you've entered:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ NormalizeData(np\_arr):}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(np\_arr.shape[}\DecValTok{1}\NormalTok{]):}
\NormalTok{    np\_arr[:,i] }\OperatorTok{=}\NormalTok{ (np\_arr[:,i] }\OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(np\_arr[:,i])) }\OperatorTok{/}\NormalTok{ (np.}\BuiltInTok{max}\NormalTok{(np\_arr[:,i]) }\OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(np\_arr[:,i]))}
  \ControlFlowTok{return}\NormalTok{(np\_arr)}
\end{Highlighting}
\end{Shaded}

Now we must divide the data into a training and a test dataset, convert the pandas dataframe to a numpy array, and normalize it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{training\_n }\OperatorTok{=} \DecValTok{2000}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ NormalizeData( data.loc[}\DecValTok{0}\NormalTok{:(training\_n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{), data.columns }\OperatorTok{!=} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy() )}
\NormalTok{Y\_train }\OperatorTok{=}\NormalTok{ data.loc[}\DecValTok{0}\NormalTok{:(training\_n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{), data.columns }\OperatorTok{==} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy()}

\NormalTok{X\_test }\OperatorTok{=}\NormalTok{ NormalizeData( data.loc[training\_n:, data.columns }\OperatorTok{!=} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy() )}
\NormalTok{Y\_test }\OperatorTok{=}\NormalTok{ data.loc[training\_n:, data.columns }\OperatorTok{==} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy()}
\end{Highlighting}
\end{Shaded}

It's now time to load the functions that were created in the preview chapters.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ generate\_weights(n\_input, n\_output, hidden\_layer\_neurons):}
\NormalTok{  W }\OperatorTok{=}\NormalTok{ []}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(hidden\_layer\_neurons)}\OperatorTok{+}\DecValTok{1}\NormalTok{):}
    \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{==} \DecValTok{0}\NormalTok{: }\CommentTok{\# first layer}
\NormalTok{      W.append(np.random.random((n\_input}\OperatorTok{+}\DecValTok{1}\NormalTok{, hidden\_layer\_neurons[i])))}
    \ControlFlowTok{elif}\NormalTok{ i }\OperatorTok{==} \BuiltInTok{len}\NormalTok{(hidden\_layer\_neurons): }\CommentTok{\# last layer}
\NormalTok{      W.append(np.random.random((hidden\_layer\_neurons[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{, n\_output)))}
    \ControlFlowTok{else}\NormalTok{: }\CommentTok{\# middle layers}
\NormalTok{      W.append(np.random.random((hidden\_layer\_neurons[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{, hidden\_layer\_neurons[i])))}
  \ControlFlowTok{return}\NormalTok{(W)}

\KeywordTok{def}\NormalTok{ add\_ones\_to\_input(x):}
  \ControlFlowTok{return}\NormalTok{(np.append(x, np.array([np.ones(}\BuiltInTok{len}\NormalTok{(x))]).T, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{))}



\KeywordTok{def}\NormalTok{ sigmoid(x):}
  \ControlFlowTok{return} \FloatTok{1.0} \OperatorTok{/}\NormalTok{ (}\FloatTok{1.0} \OperatorTok{+}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{x))}

\KeywordTok{def}\NormalTok{ deriv\_sigmoid(x):}
  \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x)}


\KeywordTok{def}\NormalTok{ forward(x, w):}
  \ControlFlowTok{return}\NormalTok{( sigmoid(x }\OperatorTok{@}\NormalTok{ w) )}

\KeywordTok{def}\NormalTok{ backward(IN, OUT, W, Y, grad, k):}
  \ControlFlowTok{if}\NormalTok{ k }\OperatorTok{==} \BuiltInTok{len}\NormalTok{(grad)}\OperatorTok{{-}}\DecValTok{1}\NormalTok{:}
\NormalTok{    grad[k] }\OperatorTok{=}\NormalTok{ deriv\_sigmoid(OUT[k]) }\OperatorTok{*}\NormalTok{ (Y}\OperatorTok{{-}}\NormalTok{OUT[k])}
  \ControlFlowTok{else}\NormalTok{:}
\NormalTok{    grad[k] }\OperatorTok{=}\NormalTok{ deriv\_sigmoid(OUT[k]) }\OperatorTok{*}\NormalTok{(grad[k}\OperatorTok{+}\DecValTok{1}\NormalTok{] }\OperatorTok{@}\NormalTok{ W[k}\OperatorTok{+}\DecValTok{1}\NormalTok{][}\DecValTok{0}\NormalTok{:}\BuiltInTok{len}\NormalTok{(W[k}\OperatorTok{+}\DecValTok{1}\NormalTok{])}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].T)}
  \ControlFlowTok{return}\NormalTok{(grad)}
\end{Highlighting}
\end{Shaded}

\hypertarget{train-and-test-phase}{%
\section{Train and test phase}\label{train-and-test-phase}}

For the training and testing phases, we're making a simple wrapper:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ train(X, Y, hidden\_layer\_neurons, alpha, epochs):}
\NormalTok{  n\_input }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(X\_train[}\DecValTok{0}\NormalTok{])}
\NormalTok{  n\_output }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(Y\_train[}\DecValTok{0}\NormalTok{])}
\NormalTok{  W }\OperatorTok{=}\NormalTok{ generate\_weights(n\_input, n\_output, hidden\_layer\_neurons)}
\NormalTok{  errors }\OperatorTok{=}\NormalTok{ []}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(epochs):}
\NormalTok{    IN }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    OUT }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    grad }\OperatorTok{=}\NormalTok{ [}\VariableTok{None}\NormalTok{]}\OperatorTok{*}\BuiltInTok{len}\NormalTok{(W)}
    \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
      \ControlFlowTok{if}\NormalTok{ k}\OperatorTok{==}\DecValTok{0}\NormalTok{:}
\NormalTok{        IN.append(add\_ones\_to\_input(X))}
      \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        IN.append(add\_ones\_to\_input(OUT[k}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]))}
\NormalTok{      OUT.append(forward(x}\OperatorTok{=}\NormalTok{IN[k], w}\OperatorTok{=}\NormalTok{W[k]))}
      
\NormalTok{    errors.append(Y }\OperatorTok{{-}}\NormalTok{ OUT[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
      
    \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
\NormalTok{      grad }\OperatorTok{=}\NormalTok{ backward(IN, OUT, W, Y, grad, k) }
      
    \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
\NormalTok{      W[k] }\OperatorTok{=}\NormalTok{ W[k] }\OperatorTok{+}\NormalTok{ alpha }\OperatorTok{*}\NormalTok{ (IN[k].T }\OperatorTok{@}\NormalTok{ grad[k])}
      
  \ControlFlowTok{return}\NormalTok{ W, errors}

\KeywordTok{def}\NormalTok{ test(X\_test, W):}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
\NormalTok{    X\_test }\OperatorTok{=}\NormalTok{ forward(add\_ones\_to\_input(X\_test), W[i])}
  \ControlFlowTok{return}\NormalTok{(X\_test)}
\end{Highlighting}
\end{Shaded}

The \texttt{train()} function is just a simple wrapper around the things done in the last chapter and will fit the weights to the given \texttt{X\_train} and \texttt{Y\_train}. The \texttt{test()} function only contains the forward pass to calculate the output without adjusting the weights of the NN. Its used to evaluate the quality of the results.\\
Its time to train the NN with the first 2000 rows of the given data, 20000 epochs, alpha of 0.01 and two hidden layers with 11 and 4 neurons:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{W\_train, errors\_train }\OperatorTok{=}\NormalTok{ train(X\_train, Y\_train, hidden\_layer\_neurons }\OperatorTok{=}\NormalTok{ [}\DecValTok{11}\NormalTok{,}\DecValTok{4}\NormalTok{], alpha }\OperatorTok{=} \FloatTok{0.01}\NormalTok{, epochs }\OperatorTok{=} \DecValTok{10000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The return contains multiple values, that are assigned with the \texttt{a,\ b\ =\ fun\_that\_returns\_2\_vals()} pattern. We can visualize the learning process by calculating the mean-square-error and plotting it with the familiar line-chart:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ mean\_square\_error(error):}
  \ControlFlowTok{return}\NormalTok{( }\FloatTok{0.5} \OperatorTok{*}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(error }\OperatorTok{**} \DecValTok{2}\NormalTok{) )}

\NormalTok{ms\_errors\_train }\OperatorTok{=}\NormalTok{ np.array(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{map}\NormalTok{(mean\_square\_error, errors\_train)))}

\KeywordTok{def}\NormalTok{ plot\_error(errors, title):}
\NormalTok{  x }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(errors)))}
\NormalTok{  y }\OperatorTok{=}\NormalTok{ np.array(errors)}
\NormalTok{  pyplot.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{))}
\NormalTok{  pyplot.plot(x, y, }\StringTok{"g"}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{  pyplot.xlabel(}\StringTok{"Iterations"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.ylabel(}\StringTok{"Mean Square Error"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.title(title)}
\NormalTok{  pyplot.ylim(}\DecValTok{0}\NormalTok{,}\BuiltInTok{max}\NormalTok{(errors)}\OperatorTok{*}\FloatTok{1.1}\NormalTok{)}
\NormalTok{  pyplot.show()}
  
\NormalTok{plot\_error(ms\_errors\_train, }\StringTok{"MLP Credit Default"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{gitbook-demo_files/figure-latex/unnamed-chunk-13-11.pdf}

In the next step its time to test the NN on the never seen \texttt{X\_test} and \texttt{Y\_test} dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{result\_test }\OperatorTok{=}\NormalTok{ test(X\_test, W\_train)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean Square error over all testdata: "}\NormalTok{, mean\_square\_error(Y\_test }\OperatorTok{{-}}\NormalTok{ result\_test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Mean Square error over all testdata:  2012.1713984629632
\end{verbatim}

Because the Mean Square Error is hard to interpret, we will classify the output of the NN to be 1 or 0 and analyze the given answer for the credit defaults.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ classify(Y\_approx):}
  \ControlFlowTok{return}\NormalTok{( np.}\BuiltInTok{round}\NormalTok{(Y\_approx,}\DecValTok{0}\NormalTok{) )}

\NormalTok{classified\_error }\OperatorTok{=}\NormalTok{ Y\_test }\OperatorTok{{-}}\NormalTok{ classify(result\_test)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean Square error over all classified testdata: "}\NormalTok{, mean\_square\_error(classified\_error))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Probability of a wrong output: "}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(classified\_error)) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(classified\_error) }\OperatorTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{ )}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Probability of a correct output: "}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{((}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(classified\_error)) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(classified\_error))}\OperatorTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{2}\NormalTok{),}\StringTok{"\%"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Mean Square error over all classified testdata:  2457.0
## Probability of a wrong output:  16.07 %
## Probability of a correct output:  83.93 %
\end{verbatim}

An incredible tool to qualify the result is the \href{https://en.wikipedia.org/wiki/Confusion_matrix}{confusion matrix} from the sklearn package. It splits the results into 4 categories that can be used to qualify the results with the following table:
\includegraphics[width=0.5\textwidth,height=\textheight]{./img/confusion_matrix.png}

For instance, `TP' stands for True-Positiv, which means that the prediction was True=1 and the actual result was Positiv=1, indicating that the prediction was correct. For the classified result of the test phase, we have the following confusion matrix in our example:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ confusion\_matrix}
\NormalTok{confusion\_matrix(Y\_test, classify(result\_test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## array([[21164,  3142],
##        [ 1772,  4503]], dtype=int64)
\end{verbatim}

\hypertarget{appendix-complete-code-3}{%
\section{Appendix (complete code)}\label{appendix-complete-code-3}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ pyplot}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ confusion\_matrix}
\NormalTok{np.random.seed(}\DecValTok{0}\NormalTok{)}

\NormalTok{data }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"example\_data/credit\_risk\_dataset.csv"}\NormalTok{).fillna(}\DecValTok{0}\NormalTok{)}
\NormalTok{data }\OperatorTok{=}\NormalTok{ data.replace(\{}\StringTok{"Y"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"N"}\NormalTok{:}\DecValTok{0}\NormalTok{\})}

\NormalTok{data[}\StringTok{"person\_home\_ownership"}\NormalTok{] }\OperatorTok{=}\NormalTok{ data[}\StringTok{"person\_home\_ownership"}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}OWN\textquotesingle{}}\NormalTok{:}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}RENT\textquotesingle{}}\NormalTok{:}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}MORTGAGE\textquotesingle{}}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}OTHER\textquotesingle{}}\NormalTok{:}\DecValTok{4}\NormalTok{\})}
\NormalTok{data[}\StringTok{"loan\_intent"}\NormalTok{] }\OperatorTok{=}\NormalTok{ data[}\StringTok{"loan\_intent"}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}PERSONAL\textquotesingle{}}\NormalTok{:}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}EDUCATION\textquotesingle{}}\NormalTok{:}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}MEDICAL\textquotesingle{}}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}VENTURE\textquotesingle{}}\NormalTok{:}\DecValTok{4}\NormalTok{, }\StringTok{\textquotesingle{}HOMEIMPROVEMENT\textquotesingle{}}\NormalTok{:}\DecValTok{5}\NormalTok{,}\StringTok{\textquotesingle{}DEBTCONSOLIDATION\textquotesingle{}}\NormalTok{:}\DecValTok{6}\NormalTok{\})}
\NormalTok{data[}\StringTok{"loan\_grade"}\NormalTok{] }\OperatorTok{=}\NormalTok{ data[}\StringTok{"loan\_grade"}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{:}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{:}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}D\textquotesingle{}}\NormalTok{:}\DecValTok{4}\NormalTok{, }\StringTok{\textquotesingle{}E\textquotesingle{}}\NormalTok{:}\DecValTok{5}\NormalTok{, }\StringTok{\textquotesingle{}F\textquotesingle{}}\NormalTok{:}\DecValTok{6}\NormalTok{, }\StringTok{\textquotesingle{}G\textquotesingle{}}\NormalTok{:}\DecValTok{7}\NormalTok{\})}


\KeywordTok{def}\NormalTok{ NormalizeData(np\_arr):}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(np\_arr.shape[}\DecValTok{1}\NormalTok{]):}
\NormalTok{    np\_arr[:,i] }\OperatorTok{=}\NormalTok{ (np\_arr[:,i] }\OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(np\_arr[:,i])) }\OperatorTok{/}\NormalTok{ (np.}\BuiltInTok{max}\NormalTok{(np\_arr[:,i]) }\OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(np\_arr[:,i]))}
  \ControlFlowTok{return}\NormalTok{(np\_arr)}

\NormalTok{training\_n }\OperatorTok{=} \DecValTok{2000}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ NormalizeData( data.loc[}\DecValTok{0}\NormalTok{:(training\_n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{), data.columns }\OperatorTok{!=} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy() )}
\NormalTok{Y\_train }\OperatorTok{=}\NormalTok{ data.loc[}\DecValTok{0}\NormalTok{:(training\_n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{), data.columns }\OperatorTok{==} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy()}

\NormalTok{X\_test }\OperatorTok{=}\NormalTok{ NormalizeData( data.loc[training\_n:, data.columns }\OperatorTok{!=} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy() )}
\NormalTok{Y\_test }\OperatorTok{=}\NormalTok{ data.loc[training\_n:, data.columns }\OperatorTok{==} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy()}



\KeywordTok{def}\NormalTok{ generate\_weights(n\_input, n\_output, hidden\_layer\_neurons):}
\NormalTok{  W }\OperatorTok{=}\NormalTok{ []}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(hidden\_layer\_neurons)}\OperatorTok{+}\DecValTok{1}\NormalTok{):}
    \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{==} \DecValTok{0}\NormalTok{: }\CommentTok{\# first layer}
\NormalTok{      W.append(np.random.random((n\_input}\OperatorTok{+}\DecValTok{1}\NormalTok{, hidden\_layer\_neurons[i])))}
    \ControlFlowTok{elif}\NormalTok{ i }\OperatorTok{==} \BuiltInTok{len}\NormalTok{(hidden\_layer\_neurons): }\CommentTok{\# last layer}
\NormalTok{      W.append(np.random.random((hidden\_layer\_neurons[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{, n\_output)))}
    \ControlFlowTok{else}\NormalTok{: }\CommentTok{\# middle layers}
\NormalTok{      W.append(np.random.random((hidden\_layer\_neurons[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{, hidden\_layer\_neurons[i])))}
  \ControlFlowTok{return}\NormalTok{(W)}

\KeywordTok{def}\NormalTok{ add\_ones\_to\_input(x):}
  \ControlFlowTok{return}\NormalTok{(np.append(x, np.array([np.ones(}\BuiltInTok{len}\NormalTok{(x))]).T, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{))}



\KeywordTok{def}\NormalTok{ sigmoid(x):}
  \ControlFlowTok{return} \FloatTok{1.0} \OperatorTok{/}\NormalTok{ (}\FloatTok{1.0} \OperatorTok{+}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{x))}

\KeywordTok{def}\NormalTok{ deriv\_sigmoid(x):}
  \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x)}


\KeywordTok{def}\NormalTok{ forward(x, w):}
  \ControlFlowTok{return}\NormalTok{( sigmoid(x }\OperatorTok{@}\NormalTok{ w) )}

\KeywordTok{def}\NormalTok{ backward(IN, OUT, W, Y, grad, k):}
  \ControlFlowTok{if}\NormalTok{ k }\OperatorTok{==} \BuiltInTok{len}\NormalTok{(grad)}\OperatorTok{{-}}\DecValTok{1}\NormalTok{:}
\NormalTok{    grad[k] }\OperatorTok{=}\NormalTok{ deriv\_sigmoid(OUT[k]) }\OperatorTok{*}\NormalTok{ (Y}\OperatorTok{{-}}\NormalTok{OUT[k])}
  \ControlFlowTok{else}\NormalTok{:}
\NormalTok{    grad[k] }\OperatorTok{=}\NormalTok{ deriv\_sigmoid(OUT[k]) }\OperatorTok{*}\NormalTok{(grad[k}\OperatorTok{+}\DecValTok{1}\NormalTok{] }\OperatorTok{@}\NormalTok{ W[k}\OperatorTok{+}\DecValTok{1}\NormalTok{][}\DecValTok{0}\NormalTok{:}\BuiltInTok{len}\NormalTok{(W[k}\OperatorTok{+}\DecValTok{1}\NormalTok{])}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].T)}
  \ControlFlowTok{return}\NormalTok{(grad)}

\KeywordTok{def}\NormalTok{ train(X, Y, hidden\_layer\_neurons, alpha, epochs):}
\NormalTok{  n\_input }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(X[}\DecValTok{0}\NormalTok{])}
\NormalTok{  n\_output }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(Y[}\DecValTok{0}\NormalTok{])}
\NormalTok{  W }\OperatorTok{=}\NormalTok{ generate\_weights(n\_input, n\_output, hidden\_layer\_neurons)}
\NormalTok{  errors }\OperatorTok{=}\NormalTok{ []}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(epochs):}
\NormalTok{    IN }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    OUT }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    grad }\OperatorTok{=}\NormalTok{ [}\VariableTok{None}\NormalTok{]}\OperatorTok{*}\BuiltInTok{len}\NormalTok{(W)}
    \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
      \ControlFlowTok{if}\NormalTok{ k}\OperatorTok{==}\DecValTok{0}\NormalTok{:}
\NormalTok{        IN.append(add\_ones\_to\_input(X))}
      \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        IN.append(add\_ones\_to\_input(OUT[k}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]))}
\NormalTok{      OUT.append(forward(x}\OperatorTok{=}\NormalTok{IN[k], w}\OperatorTok{=}\NormalTok{W[k]))}
      
\NormalTok{    errors.append(Y }\OperatorTok{{-}}\NormalTok{ OUT[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
      
    \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
\NormalTok{      grad }\OperatorTok{=}\NormalTok{ backward(IN, OUT, W, Y, grad, k) }
      
    \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
\NormalTok{      W[k] }\OperatorTok{=}\NormalTok{ W[k] }\OperatorTok{+}\NormalTok{ alpha }\OperatorTok{*}\NormalTok{ (IN[k].T }\OperatorTok{@}\NormalTok{ grad[k])}
      
  \ControlFlowTok{return}\NormalTok{ W, errors}



\NormalTok{W\_train, errors\_train }\OperatorTok{=}\NormalTok{ train(X\_train, Y\_train, hidden\_layer\_neurons }\OperatorTok{=}\NormalTok{ [}\DecValTok{11}\NormalTok{,}\DecValTok{4}\NormalTok{], alpha }\OperatorTok{=} \FloatTok{0.01}\NormalTok{, epochs }\OperatorTok{=} \DecValTok{10000}\NormalTok{)}


\KeywordTok{def}\NormalTok{ mean\_square\_error(error):}
  \ControlFlowTok{return}\NormalTok{( }\FloatTok{0.5} \OperatorTok{*}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(error }\OperatorTok{**} \DecValTok{2}\NormalTok{) )}

\NormalTok{ms\_errors\_train }\OperatorTok{=}\NormalTok{ np.array(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{map}\NormalTok{(mean\_square\_error, errors\_train)))}

\KeywordTok{def}\NormalTok{ plot\_error(errors, title):}
\NormalTok{  x }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(errors)))}
\NormalTok{  y }\OperatorTok{=}\NormalTok{ np.array(errors)}
\NormalTok{  pyplot.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{))}
\NormalTok{  pyplot.plot(x, y, }\StringTok{"g"}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{  pyplot.xlabel(}\StringTok{"Iterations"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.ylabel(}\StringTok{"Mean Square Error"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.title(title)}
\NormalTok{  pyplot.ylim(}\DecValTok{0}\NormalTok{,}\BuiltInTok{max}\NormalTok{(errors)}\OperatorTok{*}\FloatTok{1.1}\NormalTok{)}
\NormalTok{  pyplot.show()}
  
\NormalTok{plot\_error(ms\_errors\_train, }\StringTok{"MLP Credit Default"}\NormalTok{)}



\KeywordTok{def}\NormalTok{ test(X\_test, W):}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
\NormalTok{    X\_test }\OperatorTok{=}\NormalTok{ forward(add\_ones\_to\_input(X\_test), W[i])}
  \ControlFlowTok{return}\NormalTok{(X\_test)}
  

\NormalTok{result\_test }\OperatorTok{=}\NormalTok{ test(X\_test, W\_train)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean Square error over all testdata: "}\NormalTok{, mean\_square\_error(Y\_test }\OperatorTok{{-}}\NormalTok{ result\_test))}


\KeywordTok{def}\NormalTok{ classify(Y\_approx):}
  \ControlFlowTok{return}\NormalTok{( np.}\BuiltInTok{round}\NormalTok{(Y\_approx,}\DecValTok{0}\NormalTok{) )}

\NormalTok{classified\_error }\OperatorTok{=}\NormalTok{ Y\_test }\OperatorTok{{-}}\NormalTok{ classify(result\_test)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean Square error over all classified testdata: "}\NormalTok{, mean\_square\_error(classified\_error))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Probability of a wrong output: "}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(classified\_error)) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(classified\_error) }\OperatorTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{ )}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Probability of a right output: "}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{((}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(classified\_error)) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(classified\_error))}\OperatorTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{2}\NormalTok{),}\StringTok{"\%"}\NormalTok{ )}


\NormalTok{confusion\_matrix(Y\_test, classify(result\_test))}
\end{Highlighting}
\end{Shaded}

\includegraphics{gitbook-demo_files/figure-latex/unnamed-chunk-17-13.pdf}

\begin{verbatim}
## Mean Square error over all testdata:  2012.1713984629632
## Mean Square error over all classified testdata:  2457.0
## Probability of a wrong output:  16.07 %
## Probability of a right output:  83.93 %
## array([[21164,  3142],
##        [ 1772,  4503]], dtype=int64)
\end{verbatim}

\hypertarget{effect-of-batch-size}{%
\chapter{Effect of batch size}\label{effect-of-batch-size}}

First and foremost, do we need to distinguish between the following definitions that I discovered \href{https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks}{here}:\\
- one epoch := one forward pass and backward pass of all the training examples.
- batch size := the number of training scenarios in one forward/backward pass. The higher the batch size, the more memory space will be needed.
- number of iterations := number of passes, each pass using {[}batch size{]} number of scenarios. To be clear, one pass = forward pass + backward pass (we do not count the forward pass and backward pass as two different passes).

I noted in the first chapter that the standard method for defining a NN is to cycle over all training data by selecting a random scenario until all possibilities have been used, resulting in one epoch. To make it easier, I changed it to pick all scenarios at once (full batch size). But what happens if the batch size is equal to the number of rows in the training dataset?

Unfortunately, there is no solid proof for the differences, but I did find a neat post that tries to explain it \href{https://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu}{here}. As a result, optimizing with the full batch size yields sharper minimaz, whereas optimizing with a lesser batch size yields to flatter minimaz. More information about the issue of speed vs.~accuracy via batch size selection may be found \href{https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e}{here}. He clearly outlines the issues and how switching to a dynamically growing batch size could be a smart option.

The underlying data determines the ideal hyperparameters like as bias, alpha, batch size, hidden neurons, and so on, according to what I've found so far. As a result, modifying the underlying dataset affects all of the ideal hyperparameters. Perhaps it would be more useful to test some hyperparameters on our Credit Default dataset and compare the outcomes.

\hypertarget{impact-of-diffrent-hyperparameters}{%
\section{Impact of diffrent hyperparameters}\label{impact-of-diffrent-hyperparameters}}

First of all do we add a \texttt{batch\_size} parameter to the preview \texttt{train()} function and generate random batches that all contain distinct random integers in each batch. We will use the following function to generate the random batches:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ pyplot}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ confusion\_matrix}
\ImportTok{import}\NormalTok{ math }\ImportTok{as}\NormalTok{ ma}
\ImportTok{import}\NormalTok{ time}
\NormalTok{np.warnings.filterwarnings(}\StringTok{\textquotesingle{}ignore\textquotesingle{}}\NormalTok{, category}\OperatorTok{=}\NormalTok{np.VisibleDeprecationWarning) }

\KeywordTok{def}\NormalTok{ generate\_random\_batches(batch\_size, full\_batch\_size):}
\NormalTok{  batches }\OperatorTok{=}\NormalTok{ np.arange(full\_batch\_size)}
\NormalTok{  np.random.shuffle(batches)}
  \ControlFlowTok{return}\NormalTok{(np.array\_split(batches, ma.ceil(full\_batch\_size}\OperatorTok{/}\NormalTok{batch\_size)))}

\NormalTok{generate\_random\_batches(}\DecValTok{3}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [array([0, 8, 9]), array([4, 1, 5]), array([6, 2]), array([3, 7])]
\end{verbatim}

Now do we need to adjust the \texttt{train()} function to iterate over all batches:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ train(X, Y, hidden\_layer\_neurons, alpha, epochs, batch\_size):}
\NormalTok{  n\_input }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(X[}\DecValTok{0}\NormalTok{])}
\NormalTok{  n\_output }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(Y[}\DecValTok{0}\NormalTok{])}
\NormalTok{  W }\OperatorTok{=}\NormalTok{ generate\_weights(n\_input, n\_output, hidden\_layer\_neurons)}
\NormalTok{  errors }\OperatorTok{=}\NormalTok{ []}
\NormalTok{  batches }\OperatorTok{=}\NormalTok{ generate\_random\_batches(batch\_size, full\_batch\_size }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(X))}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(epochs):}
\NormalTok{    error\_temp }\OperatorTok{=}\NormalTok{ np.array([])}
    \ControlFlowTok{for}\NormalTok{ z }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(batches)):}
\NormalTok{      IN }\OperatorTok{=}\NormalTok{ []}
\NormalTok{      OUT }\OperatorTok{=}\NormalTok{ []}
\NormalTok{      grad }\OperatorTok{=}\NormalTok{ [}\VariableTok{None}\NormalTok{]}\OperatorTok{*}\BuiltInTok{len}\NormalTok{(W)}
      \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
        \ControlFlowTok{if}\NormalTok{ k}\OperatorTok{==}\DecValTok{0}\NormalTok{:}
\NormalTok{          IN.append(add\_ones\_to\_input(X[batches[z],:]))}
        \ControlFlowTok{else}\NormalTok{:}
\NormalTok{          IN.append(add\_ones\_to\_input(OUT[k}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]))}
\NormalTok{        OUT.append(forward(x}\OperatorTok{=}\NormalTok{IN[k], w}\OperatorTok{=}\NormalTok{W[k]))}
        
\NormalTok{      error\_temp }\OperatorTok{=}\NormalTok{ np.append(error\_temp, Y[batches[z],:] }\OperatorTok{{-}}\NormalTok{ OUT[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
        
      \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
\NormalTok{        grad }\OperatorTok{=}\NormalTok{ backward(IN, OUT, W, Y[batches[z],:], grad, k) }
        
      \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
\NormalTok{        W[k] }\OperatorTok{=}\NormalTok{ W[k] }\OperatorTok{+}\NormalTok{ alpha }\OperatorTok{*}\NormalTok{ (IN[k].T }\OperatorTok{@}\NormalTok{ grad[k])}
\NormalTok{    errors.append(error\_temp)}
    
  \ControlFlowTok{return}\NormalTok{ W, errors}
\end{Highlighting}
\end{Shaded}

And all the previes created functions, dataloading and transformations are:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"example\_data/credit\_risk\_dataset.csv"}\NormalTok{).fillna(}\DecValTok{0}\NormalTok{)}
\NormalTok{data }\OperatorTok{=}\NormalTok{ data.replace(\{}\StringTok{"Y"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"N"}\NormalTok{:}\DecValTok{0}\NormalTok{\})}
\NormalTok{data[}\StringTok{"person\_home\_ownership"}\NormalTok{] }\OperatorTok{=}\NormalTok{ data[}\StringTok{"person\_home\_ownership"}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}OWN\textquotesingle{}}\NormalTok{:}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}RENT\textquotesingle{}}\NormalTok{:}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}MORTGAGE\textquotesingle{}}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}OTHER\textquotesingle{}}\NormalTok{:}\DecValTok{4}\NormalTok{\})}
\NormalTok{data[}\StringTok{"loan\_intent"}\NormalTok{] }\OperatorTok{=}\NormalTok{ data[}\StringTok{"loan\_intent"}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}PERSONAL\textquotesingle{}}\NormalTok{:}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}EDUCATION\textquotesingle{}}\NormalTok{:}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}MEDICAL\textquotesingle{}}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}VENTURE\textquotesingle{}}\NormalTok{:}\DecValTok{4}\NormalTok{, }\StringTok{\textquotesingle{}HOMEIMPROVEMENT\textquotesingle{}}\NormalTok{:}\DecValTok{5}\NormalTok{,}\StringTok{\textquotesingle{}DEBTCONSOLIDATION\textquotesingle{}}\NormalTok{:}\DecValTok{6}\NormalTok{\})}
\NormalTok{data[}\StringTok{"loan\_grade"}\NormalTok{] }\OperatorTok{=}\NormalTok{ data[}\StringTok{"loan\_grade"}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{:}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{:}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}D\textquotesingle{}}\NormalTok{:}\DecValTok{4}\NormalTok{, }\StringTok{\textquotesingle{}E\textquotesingle{}}\NormalTok{:}\DecValTok{5}\NormalTok{, }\StringTok{\textquotesingle{}F\textquotesingle{}}\NormalTok{:}\DecValTok{6}\NormalTok{, }\StringTok{\textquotesingle{}G\textquotesingle{}}\NormalTok{:}\DecValTok{7}\NormalTok{\})}

\KeywordTok{def}\NormalTok{ NormalizeData(np\_arr):}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(np\_arr.shape[}\DecValTok{1}\NormalTok{]):}
\NormalTok{    np\_arr[:,i] }\OperatorTok{=}\NormalTok{ (np\_arr[:,i] }\OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(np\_arr[:,i])) }\OperatorTok{/}\NormalTok{ (np.}\BuiltInTok{max}\NormalTok{(np\_arr[:,i]) }\OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(np\_arr[:,i]))}
  \ControlFlowTok{return}\NormalTok{(np\_arr)}

\NormalTok{training\_n }\OperatorTok{=} \DecValTok{2000}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ NormalizeData( data.loc[}\DecValTok{0}\NormalTok{:(training\_n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{), data.columns }\OperatorTok{!=} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy() )}
\NormalTok{Y\_train }\OperatorTok{=}\NormalTok{ data.loc[}\DecValTok{0}\NormalTok{:(training\_n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{), data.columns }\OperatorTok{==} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy()}

\NormalTok{X\_test }\OperatorTok{=}\NormalTok{ NormalizeData( data.loc[training\_n:, data.columns }\OperatorTok{!=} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy() )}
\NormalTok{Y\_test }\OperatorTok{=}\NormalTok{ data.loc[training\_n:, data.columns }\OperatorTok{==} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy()}


\KeywordTok{def}\NormalTok{ generate\_weights(n\_input, n\_output, hidden\_layer\_neurons):}
\NormalTok{  W }\OperatorTok{=}\NormalTok{ []}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(hidden\_layer\_neurons)}\OperatorTok{+}\DecValTok{1}\NormalTok{):}
    \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{==} \DecValTok{0}\NormalTok{: }\CommentTok{\# first layer}
\NormalTok{      W.append(np.random.random((n\_input}\OperatorTok{+}\DecValTok{1}\NormalTok{, hidden\_layer\_neurons[i])))}
    \ControlFlowTok{elif}\NormalTok{ i }\OperatorTok{==} \BuiltInTok{len}\NormalTok{(hidden\_layer\_neurons): }\CommentTok{\# last layer}
\NormalTok{      W.append(np.random.random((hidden\_layer\_neurons[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{, n\_output)))}
    \ControlFlowTok{else}\NormalTok{: }\CommentTok{\# middle layers}
\NormalTok{      W.append(np.random.random((hidden\_layer\_neurons[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{, hidden\_layer\_neurons[i])))}
  \ControlFlowTok{return}\NormalTok{(W)}

\KeywordTok{def}\NormalTok{ add\_ones\_to\_input(x):}
  \ControlFlowTok{return}\NormalTok{(np.append(x, np.array([np.ones(}\BuiltInTok{len}\NormalTok{(x))]).T, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{))}


\KeywordTok{def}\NormalTok{ sigmoid(x):}
  \ControlFlowTok{return} \FloatTok{1.0} \OperatorTok{/}\NormalTok{ (}\FloatTok{1.0} \OperatorTok{+}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{x))}

\KeywordTok{def}\NormalTok{ deriv\_sigmoid(x):}
  \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x)}


\KeywordTok{def}\NormalTok{ forward(x, w):}
  \ControlFlowTok{return}\NormalTok{( sigmoid(x }\OperatorTok{@}\NormalTok{ w) )}

\KeywordTok{def}\NormalTok{ backward(IN, OUT, W, Y, grad, k):}
  \ControlFlowTok{if}\NormalTok{ k }\OperatorTok{==} \BuiltInTok{len}\NormalTok{(grad)}\OperatorTok{{-}}\DecValTok{1}\NormalTok{:}
\NormalTok{    grad[k] }\OperatorTok{=}\NormalTok{ deriv\_sigmoid(OUT[k]) }\OperatorTok{*}\NormalTok{ (Y}\OperatorTok{{-}}\NormalTok{OUT[k])}
  \ControlFlowTok{else}\NormalTok{:}
\NormalTok{    grad[k] }\OperatorTok{=}\NormalTok{ deriv\_sigmoid(OUT[k]) }\OperatorTok{*}\NormalTok{(grad[k}\OperatorTok{+}\DecValTok{1}\NormalTok{] }\OperatorTok{@}\NormalTok{ W[k}\OperatorTok{+}\DecValTok{1}\NormalTok{][}\DecValTok{0}\NormalTok{:}\BuiltInTok{len}\NormalTok{(W[k}\OperatorTok{+}\DecValTok{1}\NormalTok{])}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].T)}
  \ControlFlowTok{return}\NormalTok{(grad)}


\KeywordTok{def}\NormalTok{ mean\_square\_error(error):}
  \ControlFlowTok{return}\NormalTok{( }\FloatTok{0.5} \OperatorTok{*}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(error }\OperatorTok{**} \DecValTok{2}\NormalTok{) )}

\KeywordTok{def}\NormalTok{ plot\_error(errors, title):}
\NormalTok{  x }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(errors)))}
\NormalTok{  y }\OperatorTok{=}\NormalTok{ np.array(errors)}
\NormalTok{  pyplot.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{))}
\NormalTok{  pyplot.plot(x, y, }\StringTok{"g"}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{  pyplot.xlabel(}\StringTok{"Iterations"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.ylabel(}\StringTok{"Mean Square Error"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.title(title)}
\NormalTok{  pyplot.ylim(}\DecValTok{0}\NormalTok{,}\BuiltInTok{max}\NormalTok{(errors)}\OperatorTok{*}\FloatTok{1.1}\NormalTok{)}
\NormalTok{  pyplot.show()}
  
\KeywordTok{def}\NormalTok{ test(X\_test, W):}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
\NormalTok{    X\_test }\OperatorTok{=}\NormalTok{ forward(add\_ones\_to\_input(X\_test), W[i])}
  \ControlFlowTok{return}\NormalTok{(X\_test)}
  
\KeywordTok{def}\NormalTok{ classify(Y\_approx):}
  \ControlFlowTok{return}\NormalTok{( np.}\BuiltInTok{round}\NormalTok{(Y\_approx,}\DecValTok{0}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

Everything is loaded and set up. Now can we compare the time and the error for different \texttt{batch\_size}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# full batch size}
\NormalTok{np.random.seed(}\DecValTok{0}\NormalTok{)}

\NormalTok{start }\OperatorTok{=}\NormalTok{ time.time()}
\NormalTok{W\_train, errors\_train }\OperatorTok{=}\NormalTok{ train(X }\OperatorTok{=}\NormalTok{ X\_train, Y }\OperatorTok{=}\NormalTok{ Y\_train, hidden\_layer\_neurons }\OperatorTok{=}\NormalTok{ [}\DecValTok{11}\NormalTok{,}\DecValTok{4}\NormalTok{], alpha }\OperatorTok{=} \FloatTok{0.01}\NormalTok{, epochs }\OperatorTok{=} \DecValTok{5000}\NormalTok{, batch\_size }\OperatorTok{=} \DecValTok{2000}\NormalTok{)}
\NormalTok{time\_diff }\OperatorTok{=}\NormalTok{ time.time() }\OperatorTok{{-}}\NormalTok{ start}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Time to train the NN: "}\NormalTok{, time\_diff)}

\NormalTok{ms\_errors\_train }\OperatorTok{=}\NormalTok{ np.array(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{map}\NormalTok{(mean\_square\_error, errors\_train)))}
\NormalTok{plot\_error(ms\_errors\_train, }\StringTok{"MLP Credit Default"}\NormalTok{)}
\NormalTok{result\_test }\OperatorTok{=}\NormalTok{ test(X\_test, W\_train)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean Square error over all testdata: "}\NormalTok{, mean\_square\_error(Y\_test }\OperatorTok{{-}}\NormalTok{ result\_test))}

\NormalTok{classified\_error }\OperatorTok{=}\NormalTok{ Y\_test }\OperatorTok{{-}}\NormalTok{ classify(result\_test)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean Square error over all classified testdata: "}\NormalTok{, mean\_square\_error(classified\_error))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Probability of a wrong output: "}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(classified\_error)) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(classified\_error) }\OperatorTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{ )}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Probability of a right output: "}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{((}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(classified\_error)) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(classified\_error))}\OperatorTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{2}\NormalTok{),}\StringTok{"\%"}\NormalTok{ )}


\NormalTok{confusion\_matrix(Y\_test, classify(result\_test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Time to train the NN:  7.976114273071289
\end{verbatim}

\includegraphics{gitbook-demo_files/figure-latex/unnamed-chunk-21-15.pdf}

\begin{verbatim}
## Mean Square error over all testdata:  2402.2283244767077
## Mean Square error over all classified testdata:  2932.0
## Probability of a wrong output:  19.18 %
## Probability of a right output:  80.82 %
## array([[20193,  4113],
##        [ 1751,  4524]], dtype=int64)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# batch size = 100}
\NormalTok{np.random.seed(}\DecValTok{0}\NormalTok{)}

\NormalTok{start }\OperatorTok{=}\NormalTok{ time.time()}
\NormalTok{W\_train, errors\_train }\OperatorTok{=}\NormalTok{ train(X }\OperatorTok{=}\NormalTok{ X\_train, Y }\OperatorTok{=}\NormalTok{ Y\_train, hidden\_layer\_neurons }\OperatorTok{=}\NormalTok{ [}\DecValTok{11}\NormalTok{,}\DecValTok{4}\NormalTok{], alpha }\OperatorTok{=} \FloatTok{0.01}\NormalTok{, epochs }\OperatorTok{=} \DecValTok{5000}\NormalTok{, batch\_size }\OperatorTok{=} \DecValTok{100}\NormalTok{)}
\NormalTok{time\_diff }\OperatorTok{=}\NormalTok{ time.time() }\OperatorTok{{-}}\NormalTok{ start}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Time to train the NN: "}\NormalTok{, time\_diff)}

\NormalTok{ms\_errors\_train }\OperatorTok{=}\NormalTok{ np.array(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{map}\NormalTok{(mean\_square\_error, errors\_train)))}
\NormalTok{plot\_error(ms\_errors\_train, }\StringTok{"MLP Credit Default"}\NormalTok{)}
\NormalTok{result\_test }\OperatorTok{=}\NormalTok{ test(X\_test, W\_train)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean Square error over all testdata: "}\NormalTok{, mean\_square\_error(Y\_test }\OperatorTok{{-}}\NormalTok{ result\_test))}

\NormalTok{classified\_error }\OperatorTok{=}\NormalTok{ Y\_test }\OperatorTok{{-}}\NormalTok{ classify(result\_test)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean Square error over all classified testdata: "}\NormalTok{, mean\_square\_error(classified\_error))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Probability of a wrong output: "}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(classified\_error)) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(classified\_error) }\OperatorTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{ )}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Probability of a right output: "}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{((}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(classified\_error)) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(classified\_error))}\OperatorTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{2}\NormalTok{),}\StringTok{"\%"}\NormalTok{ )}


\NormalTok{confusion\_matrix(Y\_test, classify(result\_test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Time to train the NN:  23.500654220581055
\end{verbatim}

\includegraphics{gitbook-demo_files/figure-latex/unnamed-chunk-22-17.pdf}

\begin{verbatim}
## Mean Square error over all testdata:  2210.3693145823286
## Mean Square error over all classified testdata:  2559.5
## Probability of a wrong output:  16.74 %
## Probability of a right output:  83.26 %
## array([[21396,  2910],
##        [ 2209,  4066]], dtype=int64)
\end{verbatim}

The full batch size is clearly faster than the smaller batch size, yet the smaller batch size has a lower error. Perhaps the ideal batch size is determined by the problem itself. If you have a low-dimensional input matrix and need a quick answer, full batch size is the way to go. If your dimensional input is large, you won't be able to use a complete batch size since your RAM will jump off. I believe it is important to study the underlying data and choose a batch size that is appropriate for it. For example, in the blog article I cited at the beginning of the chapter, some situations may require a dynamic batch size that lowers over time to provide the best outcomes.

\hypertarget{appendix-complete-code-4}{%
\section{Appendix (complete code)}\label{appendix-complete-code-4}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ pyplot}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ confusion\_matrix}
\ImportTok{import}\NormalTok{ math }\ImportTok{as}\NormalTok{ ma}
\NormalTok{np.random.seed(}\DecValTok{0}\NormalTok{)}
\NormalTok{np.warnings.filterwarnings(}\StringTok{\textquotesingle{}ignore\textquotesingle{}}\NormalTok{, category}\OperatorTok{=}\NormalTok{np.VisibleDeprecationWarning) }

\NormalTok{data }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"example\_data/credit\_risk\_dataset.csv"}\NormalTok{).fillna(}\DecValTok{0}\NormalTok{)}
\NormalTok{data }\OperatorTok{=}\NormalTok{ data.replace(\{}\StringTok{"Y"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"N"}\NormalTok{:}\DecValTok{0}\NormalTok{\})}

\NormalTok{data[}\StringTok{"person\_home\_ownership"}\NormalTok{] }\OperatorTok{=}\NormalTok{ data[}\StringTok{"person\_home\_ownership"}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}OWN\textquotesingle{}}\NormalTok{:}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}RENT\textquotesingle{}}\NormalTok{:}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}MORTGAGE\textquotesingle{}}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}OTHER\textquotesingle{}}\NormalTok{:}\DecValTok{4}\NormalTok{\})}
\NormalTok{data[}\StringTok{"loan\_intent"}\NormalTok{] }\OperatorTok{=}\NormalTok{ data[}\StringTok{"loan\_intent"}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}PERSONAL\textquotesingle{}}\NormalTok{:}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}EDUCATION\textquotesingle{}}\NormalTok{:}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}MEDICAL\textquotesingle{}}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}VENTURE\textquotesingle{}}\NormalTok{:}\DecValTok{4}\NormalTok{, }\StringTok{\textquotesingle{}HOMEIMPROVEMENT\textquotesingle{}}\NormalTok{:}\DecValTok{5}\NormalTok{,}\StringTok{\textquotesingle{}DEBTCONSOLIDATION\textquotesingle{}}\NormalTok{:}\DecValTok{6}\NormalTok{\})}
\NormalTok{data[}\StringTok{"loan\_grade"}\NormalTok{] }\OperatorTok{=}\NormalTok{ data[}\StringTok{"loan\_grade"}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{:}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{:}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}D\textquotesingle{}}\NormalTok{:}\DecValTok{4}\NormalTok{, }\StringTok{\textquotesingle{}E\textquotesingle{}}\NormalTok{:}\DecValTok{5}\NormalTok{, }\StringTok{\textquotesingle{}F\textquotesingle{}}\NormalTok{:}\DecValTok{6}\NormalTok{, }\StringTok{\textquotesingle{}G\textquotesingle{}}\NormalTok{:}\DecValTok{7}\NormalTok{\})}


\KeywordTok{def}\NormalTok{ NormalizeData(np\_arr):}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(np\_arr.shape[}\DecValTok{1}\NormalTok{]):}
\NormalTok{    np\_arr[:,i] }\OperatorTok{=}\NormalTok{ (np\_arr[:,i] }\OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(np\_arr[:,i])) }\OperatorTok{/}\NormalTok{ (np.}\BuiltInTok{max}\NormalTok{(np\_arr[:,i]) }\OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(np\_arr[:,i]))}
  \ControlFlowTok{return}\NormalTok{(np\_arr)}

\NormalTok{training\_n }\OperatorTok{=} \DecValTok{2000}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ NormalizeData( data.loc[}\DecValTok{0}\NormalTok{:(training\_n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{), data.columns }\OperatorTok{!=} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy() )}
\NormalTok{Y\_train }\OperatorTok{=}\NormalTok{ data.loc[}\DecValTok{0}\NormalTok{:(training\_n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{), data.columns }\OperatorTok{==} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy()}

\NormalTok{X\_test }\OperatorTok{=}\NormalTok{ NormalizeData( data.loc[training\_n:, data.columns }\OperatorTok{!=} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy() )}
\NormalTok{Y\_test }\OperatorTok{=}\NormalTok{ data.loc[training\_n:, data.columns }\OperatorTok{==} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy()}



\KeywordTok{def}\NormalTok{ generate\_weights(n\_input, n\_output, hidden\_layer\_neurons):}
\NormalTok{  W }\OperatorTok{=}\NormalTok{ []}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(hidden\_layer\_neurons)}\OperatorTok{+}\DecValTok{1}\NormalTok{):}
    \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{==} \DecValTok{0}\NormalTok{: }\CommentTok{\# first layer}
\NormalTok{      W.append(np.random.random((n\_input}\OperatorTok{+}\DecValTok{1}\NormalTok{, hidden\_layer\_neurons[i])))}
    \ControlFlowTok{elif}\NormalTok{ i }\OperatorTok{==} \BuiltInTok{len}\NormalTok{(hidden\_layer\_neurons): }\CommentTok{\# last layer}
\NormalTok{      W.append(np.random.random((hidden\_layer\_neurons[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{, n\_output)))}
    \ControlFlowTok{else}\NormalTok{: }\CommentTok{\# middle layers}
\NormalTok{      W.append(np.random.random((hidden\_layer\_neurons[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{, hidden\_layer\_neurons[i])))}
  \ControlFlowTok{return}\NormalTok{(W)}

\KeywordTok{def}\NormalTok{ add\_ones\_to\_input(x):}
  \ControlFlowTok{return}\NormalTok{(np.append(x, np.array([np.ones(}\BuiltInTok{len}\NormalTok{(x))]).T, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{))}



\KeywordTok{def}\NormalTok{ sigmoid(x):}
  \ControlFlowTok{return} \FloatTok{1.0} \OperatorTok{/}\NormalTok{ (}\FloatTok{1.0} \OperatorTok{+}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{x))}

\KeywordTok{def}\NormalTok{ deriv\_sigmoid(x):}
  \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x)}


\KeywordTok{def}\NormalTok{ forward(x, w):}
  \ControlFlowTok{return}\NormalTok{( sigmoid(x }\OperatorTok{@}\NormalTok{ w) )}

\KeywordTok{def}\NormalTok{ backward(IN, OUT, W, Y, grad, k):}
  \ControlFlowTok{if}\NormalTok{ k }\OperatorTok{==} \BuiltInTok{len}\NormalTok{(grad)}\OperatorTok{{-}}\DecValTok{1}\NormalTok{:}
\NormalTok{    grad[k] }\OperatorTok{=}\NormalTok{ deriv\_sigmoid(OUT[k]) }\OperatorTok{*}\NormalTok{ (Y}\OperatorTok{{-}}\NormalTok{OUT[k])}
  \ControlFlowTok{else}\NormalTok{:}
\NormalTok{    grad[k] }\OperatorTok{=}\NormalTok{ deriv\_sigmoid(OUT[k]) }\OperatorTok{*}\NormalTok{(grad[k}\OperatorTok{+}\DecValTok{1}\NormalTok{] }\OperatorTok{@}\NormalTok{ W[k}\OperatorTok{+}\DecValTok{1}\NormalTok{][}\DecValTok{0}\NormalTok{:}\BuiltInTok{len}\NormalTok{(W[k}\OperatorTok{+}\DecValTok{1}\NormalTok{])}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].T)}
  \ControlFlowTok{return}\NormalTok{(grad)}

\KeywordTok{def}\NormalTok{ generate\_random\_batches(batch\_size, full\_batch\_size):}
\NormalTok{  batches }\OperatorTok{=}\NormalTok{ np.arange(full\_batch\_size)}
\NormalTok{  np.random.shuffle(batches)}
  \ControlFlowTok{return}\NormalTok{(np.array\_split(batches, ma.ceil(full\_batch\_size}\OperatorTok{/}\NormalTok{batch\_size)))}

\KeywordTok{def}\NormalTok{ train(X, Y, hidden\_layer\_neurons, alpha, epochs, batch\_size):}
\NormalTok{  n\_input }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(X[}\DecValTok{0}\NormalTok{])}
\NormalTok{  n\_output }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(Y[}\DecValTok{0}\NormalTok{])}
\NormalTok{  W }\OperatorTok{=}\NormalTok{ generate\_weights(n\_input, n\_output, hidden\_layer\_neurons)}
\NormalTok{  errors }\OperatorTok{=}\NormalTok{ []}
\NormalTok{  batches }\OperatorTok{=}\NormalTok{ generate\_random\_batches(batch\_size, full\_batch\_size }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(X))}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(epochs):}
\NormalTok{    error\_temp }\OperatorTok{=}\NormalTok{ np.array([])}
    \ControlFlowTok{for}\NormalTok{ z }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(batches)):}
\NormalTok{      IN }\OperatorTok{=}\NormalTok{ []}
\NormalTok{      OUT }\OperatorTok{=}\NormalTok{ []}
\NormalTok{      grad }\OperatorTok{=}\NormalTok{ [}\VariableTok{None}\NormalTok{]}\OperatorTok{*}\BuiltInTok{len}\NormalTok{(W)}
      \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
        \ControlFlowTok{if}\NormalTok{ k}\OperatorTok{==}\DecValTok{0}\NormalTok{:}
\NormalTok{          IN.append(add\_ones\_to\_input(X[batches[z],:]))}
        \ControlFlowTok{else}\NormalTok{:}
\NormalTok{          IN.append(add\_ones\_to\_input(OUT[k}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]))}
\NormalTok{        OUT.append(forward(x}\OperatorTok{=}\NormalTok{IN[k], w}\OperatorTok{=}\NormalTok{W[k]))}
        
\NormalTok{      error\_temp }\OperatorTok{=}\NormalTok{ np.append(error\_temp, Y[batches[z],:] }\OperatorTok{{-}}\NormalTok{ OUT[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
        
      \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
\NormalTok{        grad }\OperatorTok{=}\NormalTok{ backward(IN, OUT, W, Y[batches[z],:], grad, k) }
        
      \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
\NormalTok{        W[k] }\OperatorTok{=}\NormalTok{ W[k] }\OperatorTok{+}\NormalTok{ alpha }\OperatorTok{*}\NormalTok{ (IN[k].T }\OperatorTok{@}\NormalTok{ grad[k])}
\NormalTok{    errors.append(error\_temp)}
    
  \ControlFlowTok{return}\NormalTok{ W, errors}



\NormalTok{W\_train, errors\_train }\OperatorTok{=}\NormalTok{ train(X }\OperatorTok{=}\NormalTok{ X\_train, Y }\OperatorTok{=}\NormalTok{ Y\_train, hidden\_layer\_neurons }\OperatorTok{=}\NormalTok{ [}\DecValTok{11}\NormalTok{,}\DecValTok{4}\NormalTok{], alpha }\OperatorTok{=} \FloatTok{0.01}\NormalTok{, epochs }\OperatorTok{=} \DecValTok{2000}\NormalTok{, batch\_size }\OperatorTok{=} \DecValTok{2000}\NormalTok{)}


\KeywordTok{def}\NormalTok{ mean\_square\_error(error):}
  \ControlFlowTok{return}\NormalTok{( }\FloatTok{0.5} \OperatorTok{*}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(error }\OperatorTok{**} \DecValTok{2}\NormalTok{) )}

\NormalTok{ms\_errors\_train }\OperatorTok{=}\NormalTok{ np.array(}\BuiltInTok{list}\NormalTok{(}\BuiltInTok{map}\NormalTok{(mean\_square\_error, errors\_train)))}

\KeywordTok{def}\NormalTok{ plot\_error(errors, title):}
\NormalTok{  x }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(errors)))}
\NormalTok{  y }\OperatorTok{=}\NormalTok{ np.array(errors)}
\NormalTok{  pyplot.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{))}
\NormalTok{  pyplot.plot(x, y, }\StringTok{"g"}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{  pyplot.xlabel(}\StringTok{"Iterations"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.ylabel(}\StringTok{"Mean Square Error"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{  pyplot.title(title)}
\NormalTok{  pyplot.ylim(}\DecValTok{0}\NormalTok{,}\BuiltInTok{max}\NormalTok{(errors)}\OperatorTok{*}\FloatTok{1.1}\NormalTok{)}
\NormalTok{  pyplot.show()}
  
\NormalTok{plot\_error(ms\_errors\_train, }\StringTok{"MLP Credit Default"}\NormalTok{)}



\KeywordTok{def}\NormalTok{ test(X\_test, W):}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
\NormalTok{    X\_test }\OperatorTok{=}\NormalTok{ forward(add\_ones\_to\_input(X\_test), W[i])}
  \ControlFlowTok{return}\NormalTok{(X\_test)}
  

\NormalTok{result\_test }\OperatorTok{=}\NormalTok{ test(X\_test, W\_train)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean Square error over all testdata: "}\NormalTok{, mean\_square\_error(Y\_test }\OperatorTok{{-}}\NormalTok{ result\_test))}


\KeywordTok{def}\NormalTok{ classify(Y\_approx):}
  \ControlFlowTok{return}\NormalTok{( np.}\BuiltInTok{round}\NormalTok{(Y\_approx,}\DecValTok{0}\NormalTok{) )}

\NormalTok{classified\_error }\OperatorTok{=}\NormalTok{ Y\_test }\OperatorTok{{-}}\NormalTok{ classify(result\_test)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Mean Square error over all classified testdata: "}\NormalTok{, mean\_square\_error(classified\_error))}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Probability of a wrong output: "}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{(np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(classified\_error)) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(classified\_error) }\OperatorTok{*} \DecValTok{100}\NormalTok{, }\DecValTok{2}\NormalTok{), }\StringTok{"\%"}\NormalTok{ )}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Probability of a right output: "}\NormalTok{, np.}\BuiltInTok{round}\NormalTok{((}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(classified\_error)) }\OperatorTok{/} \BuiltInTok{len}\NormalTok{(classified\_error))}\OperatorTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{2}\NormalTok{),}\StringTok{"\%"}\NormalTok{ )}


\NormalTok{confusion\_matrix(Y\_test, classify(result\_test))}
\end{Highlighting}
\end{Shaded}

\includegraphics{gitbook-demo_files/figure-latex/unnamed-chunk-23-19.pdf}

\begin{verbatim}
## Mean Square error over all testdata:  2330.8698549481937
## Mean Square error over all classified testdata:  2940.0
## Probability of a wrong output:  19.23 %
## Probability of a right output:  80.77 %
## array([[20516,  3790],
##        [ 2090,  4185]], dtype=int64)
\end{verbatim}

\hypertarget{class-mlp}{%
\chapter{Class MLP}\label{class-mlp}}

Finally, can we create a `MLP' class that contains all of the functions from the preview chapters. You can use this class to play around with different settings and learn new things about the datasets you've chosen.

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ pyplot}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ confusion\_matrix}
\ImportTok{import}\NormalTok{ math }\ImportTok{as}\NormalTok{ ma}
\ImportTok{import}\NormalTok{ time}
\NormalTok{np.warnings.filterwarnings(}\StringTok{\textquotesingle{}ignore\textquotesingle{}}\NormalTok{, category}\OperatorTok{=}\NormalTok{np.VisibleDeprecationWarning) }

\KeywordTok{class}\NormalTok{ MLP:}
  \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, X\_train, Y\_train, X\_test, Y\_test, hidden\_layer\_neurons, alpha, epochs, batch\_size):}
    \VariableTok{self}\NormalTok{.X\_train }\OperatorTok{=}\NormalTok{ X\_train}
    \VariableTok{self}\NormalTok{.Y\_train }\OperatorTok{=}\NormalTok{ Y\_train}
    \VariableTok{self}\NormalTok{.X\_test }\OperatorTok{=}\NormalTok{ X\_test}
    \VariableTok{self}\NormalTok{.Y\_test }\OperatorTok{=}\NormalTok{ Y\_test}
    \VariableTok{self}\NormalTok{.hidden\_layer\_neurons }\OperatorTok{=}\NormalTok{ hidden\_layer\_neurons}
    \VariableTok{self}\NormalTok{.alpha }\OperatorTok{=}\NormalTok{ alpha}
    \VariableTok{self}\NormalTok{.epochs }\OperatorTok{=}\NormalTok{ epochs}
    \VariableTok{self}\NormalTok{.batch\_size }\OperatorTok{=}\NormalTok{ batch\_size}
   
  \KeywordTok{def}\NormalTok{ generate\_weights(}\VariableTok{self}\NormalTok{):}
\NormalTok{    W }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.hidden\_layer\_neurons)}\OperatorTok{+}\DecValTok{1}\NormalTok{):}
      \ControlFlowTok{if}\NormalTok{ i }\OperatorTok{==} \DecValTok{0}\NormalTok{: }\CommentTok{\# first layer}
\NormalTok{        W.append(np.random.random((}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.X\_train)}\OperatorTok{+}\DecValTok{1}\NormalTok{, }\VariableTok{self}\NormalTok{.hidden\_layer\_neurons[i])))}
      \ControlFlowTok{elif}\NormalTok{ i }\OperatorTok{==} \BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.hidden\_layer\_neurons): }\CommentTok{\# last layer}
\NormalTok{        W.append(np.random.random((}\VariableTok{self}\NormalTok{.hidden\_layer\_neurons[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{, }\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.Y\_train))))}
      \ControlFlowTok{else}\NormalTok{: }\CommentTok{\# middle layers}
\NormalTok{        W.append(np.random.random((}\VariableTok{self}\NormalTok{.hidden\_layer\_neurons[i}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}\OperatorTok{+}\DecValTok{1}\NormalTok{, }\VariableTok{self}\NormalTok{.hidden\_layer\_neurons[i])))}
    \ControlFlowTok{return}\NormalTok{(W)}
    
  \AttributeTok{@staticmethod}
  \KeywordTok{def}\NormalTok{ add\_ones\_to\_input(x):}
    \ControlFlowTok{return}\NormalTok{(np.append(x, np.array([np.ones(}\BuiltInTok{len}\NormalTok{(x))]).T, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{))}
  \AttributeTok{@staticmethod}
  \KeywordTok{def}\NormalTok{ sigmoid(x):}
    \ControlFlowTok{return} \FloatTok{1.0} \OperatorTok{/}\NormalTok{ (}\FloatTok{1.0} \OperatorTok{+}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{x))}
  \AttributeTok{@staticmethod}
  \KeywordTok{def}\NormalTok{ deriv\_sigmoid(x):}
    \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ x)}
   
  \AttributeTok{@staticmethod}
  \KeywordTok{def}\NormalTok{ forward(x, w):}
    \ControlFlowTok{return}\NormalTok{( sigmoid(x }\OperatorTok{@}\NormalTok{ w) )}
  
  \AttributeTok{@staticmethod}
  \KeywordTok{def}\NormalTok{ backward(IN, OUT, W, Y, grad, k):}
    \ControlFlowTok{if}\NormalTok{ k }\OperatorTok{==} \BuiltInTok{len}\NormalTok{(grad)}\OperatorTok{{-}}\DecValTok{1}\NormalTok{:}
\NormalTok{      grad[k] }\OperatorTok{=}\NormalTok{ deriv\_sigmoid(OUT[k]) }\OperatorTok{*}\NormalTok{ (Y}\OperatorTok{{-}}\NormalTok{OUT[k])}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{      grad[k] }\OperatorTok{=}\NormalTok{ deriv\_sigmoid(OUT[k]) }\OperatorTok{*}\NormalTok{(grad[k}\OperatorTok{+}\DecValTok{1}\NormalTok{] }\OperatorTok{@}\NormalTok{ W[k}\OperatorTok{+}\DecValTok{1}\NormalTok{][}\DecValTok{0}\NormalTok{:}\BuiltInTok{len}\NormalTok{(W[k}\OperatorTok{+}\DecValTok{1}\NormalTok{])}\OperatorTok{{-}}\DecValTok{1}\NormalTok{].T)}
    \ControlFlowTok{return}\NormalTok{(grad)}
    
  \KeywordTok{def}\NormalTok{ generate\_random\_batches(}\VariableTok{self}\NormalTok{):}
\NormalTok{    batches }\OperatorTok{=}\NormalTok{ np.arange(}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.X\_train))}
\NormalTok{    np.random.shuffle(batches)}
    \ControlFlowTok{return}\NormalTok{(np.array\_split(batches, ma.ceil(}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.X\_train)}\OperatorTok{/}\VariableTok{self}\NormalTok{.batch\_size)))}
    
  \KeywordTok{def}\NormalTok{ train(}\VariableTok{self}\NormalTok{):}
\NormalTok{    W }\OperatorTok{=} \VariableTok{self}\NormalTok{.generate\_weights()}
\NormalTok{    errors }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    batches }\OperatorTok{=} \VariableTok{self}\NormalTok{.generate\_random\_batches()}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\VariableTok{self}\NormalTok{.epochs):}
\NormalTok{      error\_temp }\OperatorTok{=}\NormalTok{ np.array([])}
      \ControlFlowTok{for}\NormalTok{ z }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(batches)):}
\NormalTok{        IN }\OperatorTok{=}\NormalTok{ []}
\NormalTok{        OUT }\OperatorTok{=}\NormalTok{ []}
\NormalTok{        grad }\OperatorTok{=}\NormalTok{ [}\VariableTok{None}\NormalTok{]}\OperatorTok{*}\BuiltInTok{len}\NormalTok{(W)}
        \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
          \ControlFlowTok{if}\NormalTok{ k}\OperatorTok{==}\DecValTok{0}\NormalTok{:}
\NormalTok{            IN.append(}\VariableTok{self}\NormalTok{.add\_ones\_to\_input(x }\OperatorTok{=} \VariableTok{self}\NormalTok{.X\_train[batches[z],:]))}
          \ControlFlowTok{else}\NormalTok{:}
\NormalTok{            IN.append(}\VariableTok{self}\NormalTok{.add\_ones\_to\_input(x }\OperatorTok{=}\NormalTok{ OUT[k}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]))}
\NormalTok{          OUT.append(}\VariableTok{self}\NormalTok{.forward(IN[k], W[k]))}
          
\NormalTok{        error\_temp }\OperatorTok{=}\NormalTok{ np.append(error\_temp, }\VariableTok{self}\NormalTok{.Y\_train[batches[z],:] }\OperatorTok{{-}}\NormalTok{ OUT[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{])}
          
        \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)}\OperatorTok{{-}}\DecValTok{1}\NormalTok{,}\OperatorTok{{-}}\DecValTok{1}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{):}
\NormalTok{          grad }\OperatorTok{=} \VariableTok{self}\NormalTok{.backward(IN, OUT, W, }\VariableTok{self}\NormalTok{.Y\_train[batches[z],:], grad, k) }
          
        \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(W)):}
\NormalTok{          W[k] }\OperatorTok{=}\NormalTok{ W[k] }\OperatorTok{+} \VariableTok{self}\NormalTok{.alpha }\OperatorTok{*}\NormalTok{ (IN[k].T }\OperatorTok{@}\NormalTok{ grad[k])}
\NormalTok{      errors.append(error\_temp)}
    \VariableTok{self}\NormalTok{.W }\OperatorTok{=}\NormalTok{ W}
    \VariableTok{self}\NormalTok{.errors }\OperatorTok{=}\NormalTok{ errors}
  \AttributeTok{@staticmethod}
  \KeywordTok{def}\NormalTok{ mean\_square\_error(error):}
    \ControlFlowTok{return}\NormalTok{( }\FloatTok{0.5} \OperatorTok{*}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(error }\OperatorTok{**} \DecValTok{2}\NormalTok{) )}
  \AttributeTok{@staticmethod}
  \KeywordTok{def}\NormalTok{ plot\_error(errors, title):}
\NormalTok{    x }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(errors)))}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ np.array(errors)}
\NormalTok{    pyplot.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{))}
\NormalTok{    pyplot.plot(x, y, }\StringTok{"g"}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{    pyplot.xlabel(}\StringTok{"Iterations"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{    pyplot.ylabel(}\StringTok{"Mean Square Error"}\NormalTok{, fontsize }\OperatorTok{=} \DecValTok{16}\NormalTok{)}
\NormalTok{    pyplot.title(title)}
\NormalTok{    pyplot.ylim(}\DecValTok{0}\NormalTok{,}\BuiltInTok{max}\NormalTok{(errors)}\OperatorTok{*}\FloatTok{1.1}\NormalTok{)}
\NormalTok{    pyplot.show()}
    
  \KeywordTok{def}\NormalTok{ test(}\VariableTok{self}\NormalTok{):}
\NormalTok{    X\_test }\OperatorTok{=} \VariableTok{self}\NormalTok{.X\_test}
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.W)):}
\NormalTok{      X\_test }\OperatorTok{=} \VariableTok{self}\NormalTok{.forward(}\VariableTok{self}\NormalTok{.add\_ones\_to\_input(X\_test), }\VariableTok{self}\NormalTok{.W[i])}
    \ControlFlowTok{return}\NormalTok{(X\_test)}
  \AttributeTok{@staticmethod}
  \KeywordTok{def}\NormalTok{ classify(Y\_approx):}
    \ControlFlowTok{return}\NormalTok{( np.}\BuiltInTok{round}\NormalTok{(Y\_approx,}\DecValTok{0}\NormalTok{) )}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"example\_data/credit\_risk\_dataset.csv"}\NormalTok{).fillna(}\DecValTok{0}\NormalTok{)}
\NormalTok{data }\OperatorTok{=}\NormalTok{ data.replace(\{}\StringTok{"Y"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"N"}\NormalTok{:}\DecValTok{0}\NormalTok{\})}

\NormalTok{data[}\StringTok{"person\_home\_ownership"}\NormalTok{] }\OperatorTok{=}\NormalTok{ data[}\StringTok{"person\_home\_ownership"}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}OWN\textquotesingle{}}\NormalTok{:}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}RENT\textquotesingle{}}\NormalTok{:}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}MORTGAGE\textquotesingle{}}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}OTHER\textquotesingle{}}\NormalTok{:}\DecValTok{4}\NormalTok{\})}
\NormalTok{data[}\StringTok{"loan\_intent"}\NormalTok{] }\OperatorTok{=}\NormalTok{ data[}\StringTok{"loan\_intent"}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}PERSONAL\textquotesingle{}}\NormalTok{:}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}EDUCATION\textquotesingle{}}\NormalTok{:}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}MEDICAL\textquotesingle{}}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}VENTURE\textquotesingle{}}\NormalTok{:}\DecValTok{4}\NormalTok{, }\StringTok{\textquotesingle{}HOMEIMPROVEMENT\textquotesingle{}}\NormalTok{:}\DecValTok{5}\NormalTok{,}\StringTok{\textquotesingle{}DEBTCONSOLIDATION\textquotesingle{}}\NormalTok{:}\DecValTok{6}\NormalTok{\})}
\NormalTok{data[}\StringTok{"loan\_grade"}\NormalTok{] }\OperatorTok{=}\NormalTok{ data[}\StringTok{"loan\_grade"}\NormalTok{].replace(\{}\StringTok{\textquotesingle{}A\textquotesingle{}}\NormalTok{:}\DecValTok{1}\NormalTok{, }\StringTok{\textquotesingle{}B\textquotesingle{}}\NormalTok{:}\DecValTok{2}\NormalTok{, }\StringTok{\textquotesingle{}C\textquotesingle{}}\NormalTok{:}\DecValTok{3}\NormalTok{, }\StringTok{\textquotesingle{}D\textquotesingle{}}\NormalTok{:}\DecValTok{4}\NormalTok{, }\StringTok{\textquotesingle{}E\textquotesingle{}}\NormalTok{:}\DecValTok{5}\NormalTok{, }\StringTok{\textquotesingle{}F\textquotesingle{}}\NormalTok{:}\DecValTok{6}\NormalTok{, }\StringTok{\textquotesingle{}G\textquotesingle{}}\NormalTok{:}\DecValTok{7}\NormalTok{\})}

\KeywordTok{def}\NormalTok{ NormalizeData(np\_arr):}
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(np\_arr.shape[}\DecValTok{1}\NormalTok{]):}
\NormalTok{    np\_arr[:,i] }\OperatorTok{=}\NormalTok{ (np\_arr[:,i] }\OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(np\_arr[:,i])) }\OperatorTok{/}\NormalTok{ (np.}\BuiltInTok{max}\NormalTok{(np\_arr[:,i]) }\OperatorTok{{-}}\NormalTok{ np.}\BuiltInTok{min}\NormalTok{(np\_arr[:,i]))}
  \ControlFlowTok{return}\NormalTok{(np\_arr)}

\NormalTok{training\_n }\OperatorTok{=} \DecValTok{2000}
\NormalTok{X\_train }\OperatorTok{=}\NormalTok{ NormalizeData( data.loc[}\DecValTok{0}\NormalTok{:(training\_n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{), data.columns }\OperatorTok{!=} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy() )}
\NormalTok{Y\_train }\OperatorTok{=}\NormalTok{ data.loc[}\DecValTok{0}\NormalTok{:(training\_n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{), data.columns }\OperatorTok{==} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy()}

\NormalTok{X\_test }\OperatorTok{=}\NormalTok{ NormalizeData( data.loc[training\_n:, data.columns }\OperatorTok{!=} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy() )}
\NormalTok{Y\_test }\OperatorTok{=}\NormalTok{ data.loc[training\_n:, data.columns }\OperatorTok{==} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy()}


\CommentTok{\# mlp = MLP(X\_train, Y\_train, X\_test, Y\_test, hidden\_layer\_neurons = [11,4], alpha = 0.01, epochs = 2000, batch\_size = 2000)}
\CommentTok{\# mlp.train()}
\end{Highlighting}
\end{Shaded}

\hypertarget{decision-trees}{%
\chapter{Decision Trees}\label{decision-trees}}

The goal of Decision Trees (DT) and Neuronal Netwoks (NN) is the same: to analyze data and provide answers to previously unknown data. Nonetheless, there are significant differences in working with each of these methods. As a result, I've included a link to a thorough \href{https://www.kdnuggets.com/2019/06/random-forest-vs-neural-network.html}{comparison} of these two approaches. As a result, Decision Trees are much simpler than Neuronal Networks, with the added benefit of being easier to interpret the way a certain answer is given. In comparison, Neuronal Networks can easily process large datasets, and you can fine-tune the learning behavior using the hyper-parameter you choose, resulting in high accuracy. If you consider everything, it's better to consider Decision Trees before creating a Neuronal Network because it's a simpler approach. If your data is more complicated or large, Neuronal Networks are the way to go. They frequently use hybrids of Decision Trees and Neuronal Networks to archive understandable results with high accuracy in very complex situations (for example \href{https://bair.berkeley.edu/blog/2020/04/23/decisions/}{Neural-Backed Decision Trees}).

\hypertarget{entropy}{%
\section{Entropy}\label{entropy}}

The impurity of the data is measured by entropy. Low impurity results in improved classification and accuracy. The goal of Decision Trees is to split the data with the highest purity gain at each node. The \href{https://en.wikipedia.org/wiki/Concave_function}{concave} Entropy-Formula can be used to calculate the purity of a dataset:
\[
  E = -p \cdot log_2(p) - (1-p) \cdot log_2(1-p) 
\]

with \(p\) as the probability of having no default in the credit default dataset. If the dataset contains 50\% defaults and 50\% no defaults, the Entropy increases to 1, and it decreases to 0 if the dataset contains only defaults or only no defaults. The following is the python function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ calc\_entropy(df, decision\_on }\OperatorTok{=} \StringTok{"loan\_status"}\NormalTok{):}
  \ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(df)}\OperatorTok{==}\DecValTok{0}\NormalTok{:}
    \ControlFlowTok{return}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{  p }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(df[decision\_on] }\OperatorTok{==} \DecValTok{0}\NormalTok{)}\OperatorTok{/}\BuiltInTok{len}\NormalTok{(df)}
  \ControlFlowTok{if}\NormalTok{ p }\OperatorTok{==} \DecValTok{0} \KeywordTok{or}\NormalTok{ p }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
    \ControlFlowTok{return}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{  result }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{p }\OperatorTok{*}\NormalTok{ ma.log(p,}\DecValTok{2}\NormalTok{) }\OperatorTok{{-}}\NormalTok{ (}\DecValTok{1}\OperatorTok{{-}}\NormalTok{p)}\OperatorTok{*}\NormalTok{ma.log(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{p,}\DecValTok{2}\NormalTok{)}
  \ControlFlowTok{return}\NormalTok{ result}
\end{Highlighting}
\end{Shaded}

Now can we calculate the initial Entropy of the credit default dataset as shown in the next code snippet:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ math }\ImportTok{as}\NormalTok{ ma}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}

\NormalTok{data }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"example\_data/credit\_risk\_dataset.csv"}\NormalTok{).fillna(}\DecValTok{0}\NormalTok{)}
\NormalTok{data }\OperatorTok{=}\NormalTok{ data.replace(\{}\StringTok{"Y"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"N"}\NormalTok{:}\DecValTok{0}\NormalTok{\})}

\NormalTok{entropy\_of\_data }\OperatorTok{=}\NormalTok{ calc\_entropy(df}\OperatorTok{=}\NormalTok{data)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Initial Inpurity/Entropy of data: "}\NormalTok{, entropy\_of\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Initial Inpurity/Entropy of data:  0.7568007498467375
\end{verbatim}

Unfortunately, do we face the same problem with categorical data as we had with the NN. We could convert categorical data to numerical as shown in the Credit Default chapter, but I'd like to remove these columns to make things easier. We'll also limit it to four columns plus the answer to make it more readable and understandable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OperatorTok{=}\NormalTok{ data.loc[:, [}\StringTok{"person\_age"}\NormalTok{, }\StringTok{"loan\_percent\_income"}\NormalTok{, }\StringTok{"loan\_int\_rate"}\NormalTok{, }\StringTok{"cb\_person\_default\_on\_file"}\NormalTok{ ,}\StringTok{"loan\_status"}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

Now we must create the tree's nodes based on the decisions that result in the lowest Entropy. The purity obtained by splitting the data into two subsets by a single column condition (value \(z\)) can be calculated as follows:
\[
  p_1(z) = \text{proportion of column-value} > z \\
  p_2(z) = \text{proportion of no default and column-value} > z \\
  p_3(z) = \text{proportion of no default and column-value} \leq z \\
  E_{split} = p_1(z) \cdot [-p_2(z) \cdot log_2(p_2(z)) - (1-p_2(z)) \cdot log_2(1-p_2(z))] \\
  +(1-p_1(z)) \cdot [-p_3(z) \cdot log_2(p_3(z)) - (1-p_3(z)) \cdot log_2(1-p_3(z))]
\]

and in python can we use the \texttt{calc\_entropy} function to make it even simpler:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ calc\_splitted\_entropy(df, col, val, decision\_on }\OperatorTok{=} \StringTok{"loan\_status"}\NormalTok{):}
\NormalTok{  w }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(df[col] }\OperatorTok{\textgreater{}}\NormalTok{ val)}\OperatorTok{/}\BuiltInTok{len}\NormalTok{(df)}
\NormalTok{  result }\OperatorTok{=}\NormalTok{ w }\OperatorTok{*}\NormalTok{ calc\_entropy(df.loc[df[col] }\OperatorTok{\textgreater{}}\NormalTok{ val], decision\_on) }\OperatorTok{+}\NormalTok{ (}\DecValTok{1}\OperatorTok{{-}}\NormalTok{w) }\OperatorTok{*}\NormalTok{ calc\_entropy(df.loc[df[col] }\OperatorTok{\textless{}=}\NormalTok{ val], decision\_on)}
  \ControlFlowTok{return}\NormalTok{ result}
\end{Highlighting}
\end{Shaded}

For example can we split the dataset by column \texttt{loan\_percent\_income} and value \(z=0.3\) to archive an decrease in Entropy:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{entropy\_of\_splitted\_data }\OperatorTok{=}\NormalTok{ calc\_splitted\_entropy(df}\OperatorTok{=}\NormalTok{data, col}\OperatorTok{=}\StringTok{"loan\_percent\_income"}\NormalTok{, val}\OperatorTok{=}\FloatTok{0.3}\NormalTok{, decision\_on }\OperatorTok{=} \StringTok{"loan\_status"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Splitted Inpurity/Entropy of data: "}\NormalTok{, entropy\_of\_splitted\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Splitted Inpurity/Entropy of data:  0.648938734265563
\end{verbatim}

This split results in a decrease of \texttt{python\ round(entropy\_of\_data-entropy\_of\_splitted\_data,5)} in the overall Entropy.

\hypertarget{constructing-the-tree}{%
\section{Constructing the Tree}\label{constructing-the-tree}}

First of all do we need to find the column and the value that results in the highest decrease of Entropy and splitt the dataset by it. Afterwards do we pass the resulting subsets into the same function (recursion). We will save the given conditions for the splitting. If the given subset contains less than \texttt{min\_size} of rows or reached the \texttt{max\_depth} it will turn into a leaf. We need to analyze the function properties to find the minimal value for the optimal splitting. The \texttt{calc\_splitted\_entropy} function is concave (``\href{https://en.wikipedia.org/wiki/Entropy_(information_theory)}{The Entropy is concave in the probability mass function}''). We can use this property to write a simple minimiza that checks if the next evaluation is smaller than the previous and steps along the input value. If the evaluation is bigger, it will change the direction and decrease the step distance. It repeats this process until the change in the evaluation is stagnating.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ find\_minima(df, col, decision\_on }\OperatorTok{=} \StringTok{"loan\_status"}\NormalTok{, round\_at }\OperatorTok{=} \DecValTok{5}\NormalTok{):}
\NormalTok{  direction }\OperatorTok{=} \DecValTok{1}
\NormalTok{  step }\OperatorTok{=}\NormalTok{ (df[col].}\BuiltInTok{max}\NormalTok{()}\OperatorTok{{-}}\NormalTok{df[col].}\BuiltInTok{min}\NormalTok{()) }\OperatorTok{*} \FloatTok{0.1}
\NormalTok{  val }\OperatorTok{=}\NormalTok{ df[col].}\BuiltInTok{min}\NormalTok{() }\OperatorTok{+}\NormalTok{ step}
\NormalTok{  best\_entropy }\OperatorTok{=} \DecValTok{1}
\NormalTok{  stagnation }\OperatorTok{=} \DecValTok{0}
  
  \ControlFlowTok{while}\NormalTok{ stagnation }\OperatorTok{\textless{}=} \DecValTok{15}\NormalTok{:}
\NormalTok{    temp }\OperatorTok{=}\NormalTok{ calc\_splitted\_entropy(df, col, val)}
    \ControlFlowTok{if}\NormalTok{ temp }\OperatorTok{\textgreater{}}\NormalTok{ best\_entropy:}
\NormalTok{      direction }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{direction}
\NormalTok{      step }\OperatorTok{=} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ step}
\NormalTok{      stagnation }\OperatorTok{+=} \DecValTok{1}
    \ControlFlowTok{elif} \BuiltInTok{round}\NormalTok{(temp,round\_at) }\OperatorTok{\textless{}} \BuiltInTok{round}\NormalTok{(best\_entropy,round\_at):}
\NormalTok{      stagnation }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{      stagnation }\OperatorTok{+=} \DecValTok{1}
\NormalTok{    best\_entropy }\OperatorTok{=}\NormalTok{ temp}
\NormalTok{    val }\OperatorTok{=}\NormalTok{ val }\OperatorTok{+}\NormalTok{ direction }\OperatorTok{*}\NormalTok{ step}
    
  \ControlFlowTok{return}\NormalTok{ best\_entropy, val}
\end{Highlighting}
\end{Shaded}

This minimizer is written by my self and it only works for convex functions. I dont know if there do exist better approaches, but this one works.

Now do we need to find the best decease in Entropy of all columns with the next function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ find\_best\_col(df, decision\_on }\OperatorTok{=} \StringTok{"loan\_status"}\NormalTok{, round\_at }\OperatorTok{=} \DecValTok{5}\NormalTok{):}
\NormalTok{  cols }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(df.columns[df.columns }\OperatorTok{!=}\NormalTok{ decision\_on])}
\NormalTok{  entropys }\OperatorTok{=}\NormalTok{ np.ones(}\BuiltInTok{len}\NormalTok{(cols))}
\NormalTok{  vals }\OperatorTok{=}\NormalTok{ np.ones(}\BuiltInTok{len}\NormalTok{(cols))}
  
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(cols)):}
\NormalTok{    entropys[i], vals[i] }\OperatorTok{=}\NormalTok{ find\_minima(df, col}\OperatorTok{=}\NormalTok{cols[i], decision\_on }\OperatorTok{=} \StringTok{"loan\_status"}\NormalTok{, round\_at }\OperatorTok{=} \DecValTok{5}\NormalTok{)}
  
\NormalTok{  best\_i }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(np.where(entropys }\OperatorTok{==} \BuiltInTok{min}\NormalTok{(entropys))[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{])}
  \ControlFlowTok{return}\NormalTok{ cols[best\_i], entropys[best\_i], vals[best\_i]}
\end{Highlighting}
\end{Shaded}

The following is an example of the output for the initial dataste:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{find\_best\_col(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ('loan_percent_income', 0.6564119496913492, 0.29990234374999997)
\end{verbatim}

It's now time to build the tree and save everything that ends in a leaf:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ make\_node\_and\_leafs(df, decision\_on }\OperatorTok{=} \StringTok{"loan\_status"}\NormalTok{, round\_at }\OperatorTok{=} \DecValTok{5}\NormalTok{, path }\OperatorTok{=} \StringTok{"I"}\NormalTok{, condition }\OperatorTok{=} \StringTok{""}\NormalTok{, min\_size }\OperatorTok{=} \DecValTok{1000}\NormalTok{, max\_depth }\OperatorTok{=} \DecValTok{4}\NormalTok{, leafs }\OperatorTok{=}\NormalTok{ pd.DataFrame(columns}\OperatorTok{=}\NormalTok{[}\StringTok{"path"}\NormalTok{, }\StringTok{"condition"}\NormalTok{, }\StringTok{"rows"}\NormalTok{, }\StringTok{"P\_of\_no\_default"}\NormalTok{, }\StringTok{"entropy"}\NormalTok{])):}
  \ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(df) }\OperatorTok{\textless{}}\NormalTok{ min\_size }\KeywordTok{or}\NormalTok{ (path.count(}\StringTok{"{-}"}\NormalTok{)}\OperatorTok{{-}}\DecValTok{1}\NormalTok{) }\OperatorTok{\textgreater{}=}\NormalTok{ max\_depth }\KeywordTok{or} \BuiltInTok{len}\NormalTok{(df.columns) }\OperatorTok{\textless{}=} \DecValTok{1}\NormalTok{:}
\NormalTok{    leafs }\OperatorTok{=}\NormalTok{ leafs.append(\{}\StringTok{"path"}\NormalTok{:path}\OperatorTok{+}\StringTok{"\}"}\NormalTok{, }\StringTok{"condition"}\NormalTok{:condition[}\DecValTok{0}\NormalTok{:(}\BuiltInTok{len}\NormalTok{(condition)}\OperatorTok{{-}}\DecValTok{5}\NormalTok{)], }\StringTok{"rows"}\NormalTok{:}\BuiltInTok{len}\NormalTok{(df), }\StringTok{"P\_of\_no\_default"}\NormalTok{:np.}\BuiltInTok{sum}\NormalTok{(df[decision\_on] }\OperatorTok{==} \DecValTok{0}\NormalTok{)}\OperatorTok{/}\BuiltInTok{len}\NormalTok{(df), }\StringTok{"entropy"}\NormalTok{:calc\_entropy(df)\}, ignore\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
  \ControlFlowTok{else}\NormalTok{:}
\NormalTok{    col, entropy, val }\OperatorTok{=}\NormalTok{ find\_best\_col(df, decision\_on, round\_at)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"path:"}\NormalTok{, path, }\StringTok{"   entropy:"}\NormalTok{, entropy, }\StringTok{"  col:"}\NormalTok{, col, }\StringTok{"   val:"}\NormalTok{, val, }\StringTok{"  rows:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(df))}
\NormalTok{    leafs }\OperatorTok{=}\NormalTok{ make\_node\_and\_leafs( df.loc[df[col] }\OperatorTok{\textgreater{}}\NormalTok{ val, df.columns }\OperatorTok{!=}\NormalTok{ col], decision\_on, round\_at, path}\OperatorTok{+}\StringTok{"{-}R"}\NormalTok{, condition}\OperatorTok{+}\NormalTok{col}\OperatorTok{+}\StringTok{" \textgreater{} "}\OperatorTok{+}\BuiltInTok{str}\NormalTok{(}\BuiltInTok{float}\NormalTok{(}\BuiltInTok{round}\NormalTok{(val,}\DecValTok{5}\NormalTok{)))}\OperatorTok{+}\StringTok{" and "}\NormalTok{, min\_size, max\_depth, leafs)}
\NormalTok{    leafs }\OperatorTok{=}\NormalTok{ make\_node\_and\_leafs( df.loc[df[col] }\OperatorTok{\textless{}=}\NormalTok{ val, df.columns }\OperatorTok{!=}\NormalTok{ col], decision\_on, round\_at, path }\OperatorTok{+} \StringTok{"{-}L"}\NormalTok{, condition}\OperatorTok{+}\NormalTok{col}\OperatorTok{+}\StringTok{" \textless{}= "}\OperatorTok{+}\BuiltInTok{str}\NormalTok{(}\BuiltInTok{float}\NormalTok{(}\BuiltInTok{round}\NormalTok{(val,}\DecValTok{5}\NormalTok{)))}\OperatorTok{+}\StringTok{" and "}\NormalTok{, min\_size, max\_depth, leafs)}
  \ControlFlowTok{return}\NormalTok{(leafs)}

\NormalTok{leafs }\OperatorTok{=}\NormalTok{ make\_node\_and\_leafs(df}\OperatorTok{=}\NormalTok{data, decision\_on }\OperatorTok{=} \StringTok{"loan\_status"}\NormalTok{, round\_at }\OperatorTok{=} \DecValTok{5}\NormalTok{, min\_size }\OperatorTok{=} \DecValTok{1000}\NormalTok{, max\_depth }\OperatorTok{=} \DecValTok{4}\NormalTok{)}
\NormalTok{leafs[}\StringTok{"entropy"}\NormalTok{] }\OperatorTok{=}\NormalTok{ (leafs[}\StringTok{"entropy"}\NormalTok{]}\OperatorTok{*}\NormalTok{leafs[}\StringTok{"rows"}\NormalTok{])}\OperatorTok{/}\BuiltInTok{len}\NormalTok{(data)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Entropy in data: "}\NormalTok{, calc\_entropy(data))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Entropy in all leafs: "}\NormalTok{, np.}\BuiltInTok{sum}\NormalTok{(leafs[}\StringTok{"entropy"}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## path: I    entropy: 0.6564119496913492   col: loan_percent_income    val: 0.29990234374999997   rows: 32581
## path: I-R    entropy: 0.9103981172376336   col: loan_int_rate    val: 14.01635546875   rows: 4285
## path: I-R-L    entropy: 0.9607335396537932   col: cb_person_default_on_file    val: 0.14999999999999947   rows: 3361
## path: I-R-L-L    entropy: 0.9648046914552115   col: person_age    val: 34.296875   rows: 2977
## path: I-L    entropy: 0.531844917392527   col: loan_int_rate    val: 14.280073242187505   rows: 28296
## path: I-L-R    entropy: 0.998669387657678   col: person_age    val: 22.3046875   rows: 4057
## path: I-L-R-R    entropy: 0.9992350724472542   col: cb_person_default_on_file    val: 0.14999999999999947   rows: 3488
## path: I-L-L    entropy: 0.44498607709157834   col: cb_person_default_on_file    val: 0.14999999999999947   rows: 24239
## path: I-L-L-R    entropy: 0.704963233218175   col: person_age    val: 33.9921875   rows: 2838
## path: I-L-L-L    entropy: 0.40973705382045866   col: person_age    val: 21.840624999999967   rows: 21401
## Entropy in data:  0.7568007498467375
## Entropy in all leafs:  0.574488597974446
\end{verbatim}

We can observe that the Entropy of all leafs is significantly smaller than the initial Entropy.

\hypertarget{forcast-credit-defaults-with-dt}{%
\section{Forcast Credit Defaults with DT}\label{forcast-credit-defaults-with-dt}}

We must set a limit that divides all leafs by default and none default. We've set the restriction at 0.65, which indicates that leaves with a chance of none default of less than 65 percent are considered as defaults. The conditions column in the \texttt{leafs} table can be used to acquire these rows. The following analysis demonstrates the forecast's accuracy:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_temp }\OperatorTok{=}\NormalTok{ data.copy()}
\NormalTok{data\_temp[}\StringTok{"ID"}\NormalTok{] }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(data\_temp)))}
\NormalTok{conditions }\OperatorTok{=} \StringTok{"("}\OperatorTok{+} \StringTok{") | ("}\NormalTok{.join(}\BuiltInTok{list}\NormalTok{(leafs.loc[leafs[}\StringTok{"P\_of\_no\_default"}\NormalTok{] }\OperatorTok{\textless{}} \FloatTok{0.65}\NormalTok{, leafs.columns }\OperatorTok{==} \StringTok{"condition"}\NormalTok{][}\StringTok{"condition"}\NormalTok{].replace(}\StringTok{"and"}\NormalTok{,}\StringTok{"\&"}\NormalTok{)))}\OperatorTok{+}\StringTok{")"}
\NormalTok{data\_temp }\OperatorTok{=}\NormalTok{ data\_temp.query(conditions)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.zeros(}\BuiltInTok{len}\NormalTok{(data))}
\NormalTok{X[}\BuiltInTok{list}\NormalTok{(data\_temp[}\StringTok{"ID"}\NormalTok{])] }\OperatorTok{=} \DecValTok{1}

\NormalTok{Y }\OperatorTok{=}\NormalTok{ data.loc[:, data.columns }\OperatorTok{==} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy()[:,}\DecValTok{0}\NormalTok{]}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Wrong answers of the decission tree: "}\NormalTok{,np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(Y}\OperatorTok{{-}}\NormalTok{X))}\OperatorTok{/}\BuiltInTok{len}\NormalTok{(Y) }\OperatorTok{*} \DecValTok{100}\NormalTok{, }\StringTok{"\%"}\NormalTok{)}
\NormalTok{confusion\_matrix(Y,X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Wrong answers of the decission tree:  17.94911144532089 %
## array([[21932,  3541],
##        [ 2307,  4801]], dtype=int64)
\end{verbatim}

We can compare the results to the previews created NN (with \texttt{hidden\_layer\_neurons\ =\ {[}4,4{]},\ alpha\ =\ 0.01,\ epochs\ =\ 500,\ batch\_size\ =\ 2000}):

\hypertarget{extern-packages}{%
\section{Extern Packages}\label{extern-packages}}

There do exist some packages to create DTs for example sklearn, but i wasnt able to get the conditions out of it like in my own code:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ sklearn.tree }\ImportTok{import}\NormalTok{ DecisionTreeClassifier, plot\_tree, export\_graphviz, export\_text}
\NormalTok{X }\OperatorTok{=}\NormalTok{ data.drop(}\StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{,axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ data[}\StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{]}
\NormalTok{clf }\OperatorTok{=}\NormalTok{ DecisionTreeClassifier(criterion}\OperatorTok{=}\StringTok{\textquotesingle{}entropy\textquotesingle{}}\NormalTok{,max\_depth}\OperatorTok{=}\DecValTok{4}\NormalTok{,min\_samples\_split}\OperatorTok{=}\DecValTok{1000}\NormalTok{,min\_samples\_leaf}\OperatorTok{=}\DecValTok{200}\NormalTok{,random\_state}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\NormalTok{clf }\OperatorTok{=}\NormalTok{ clf.fit(X,y)}
\NormalTok{pyplot.figure(figsize}\OperatorTok{=}\NormalTok{(}\DecValTok{16}\NormalTok{,}\DecValTok{8}\NormalTok{))}
\NormalTok{plot\_tree(clf, filled}\OperatorTok{=}\VariableTok{True}\NormalTok{, feature\_names}\OperatorTok{=}\NormalTok{X.columns, proportion}\OperatorTok{=}\VariableTok{False}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{6}\NormalTok{)}
\NormalTok{pyplot.show()}

\NormalTok{r }\OperatorTok{=}\NormalTok{ export\_text(clf, feature\_names}\OperatorTok{=}\BuiltInTok{list}\NormalTok{(X.columns))}
\BuiltInTok{print}\NormalTok{(r)}
\end{Highlighting}
\end{Shaded}

\includegraphics{gitbook-demo_files/figure-latex/unnamed-chunk-37-22.pdf}

\begin{verbatim}
## |--- loan_percent_income <= 0.31
## |   |--- loan_int_rate <= 14.03
## |   |   |--- cb_person_default_on_file <= 0.50
## |   |   |   |--- loan_percent_income <= 0.16
## |   |   |   |   |--- class: 0
## |   |   |   |--- loan_percent_income >  0.16
## |   |   |   |   |--- class: 0
## |   |   |--- cb_person_default_on_file >  0.50
## |   |   |   |--- loan_int_rate <= 10.97
## |   |   |   |   |--- class: 0
## |   |   |   |--- loan_int_rate >  10.97
## |   |   |   |   |--- class: 0
## |   |--- loan_int_rate >  14.03
## |   |   |--- loan_int_rate <= 15.28
## |   |   |   |--- loan_int_rate <= 14.37
## |   |   |   |   |--- class: 0
## |   |   |   |--- loan_int_rate >  14.37
## |   |   |   |   |--- class: 0
## |   |   |--- loan_int_rate >  15.28
## |   |   |   |--- loan_int_rate <= 16.85
## |   |   |   |   |--- class: 1
## |   |   |   |--- loan_int_rate >  16.85
## |   |   |   |   |--- class: 1
## |--- loan_percent_income >  0.31
## |   |--- loan_int_rate <= 12.76
## |   |   |--- loan_int_rate <= 9.97
## |   |   |   |--- loan_percent_income <= 0.38
## |   |   |   |   |--- class: 1
## |   |   |   |--- loan_percent_income >  0.38
## |   |   |   |   |--- class: 1
## |   |   |--- loan_int_rate >  9.97
## |   |   |   |--- person_age <= 22.50
## |   |   |   |   |--- class: 1
## |   |   |   |--- person_age >  22.50
## |   |   |   |   |--- class: 1
## |   |--- loan_int_rate >  12.76
## |   |   |--- loan_int_rate <= 14.31
## |   |   |   |--- class: 1
## |   |   |--- loan_int_rate >  14.31
## |   |   |   |--- class: 1
\end{verbatim}

\hypertarget{appendix-complete-code-5}{%
\section{Appendix (complete code)}\label{appendix-complete-code-5}}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{from}\NormalTok{ sklearn.metrics }\ImportTok{import}\NormalTok{ confusion\_matrix}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ pyplot}
\ImportTok{import}\NormalTok{ math }\ImportTok{as}\NormalTok{ ma}


\NormalTok{data }\OperatorTok{=}\NormalTok{ pd.read\_csv(}\StringTok{"example\_data/credit\_risk\_dataset.csv"}\NormalTok{).fillna(}\DecValTok{0}\NormalTok{)}
\NormalTok{data }\OperatorTok{=}\NormalTok{ data.replace(\{}\StringTok{"Y"}\NormalTok{: }\DecValTok{1}\NormalTok{, }\StringTok{"N"}\NormalTok{:}\DecValTok{0}\NormalTok{\})}
\NormalTok{data }\OperatorTok{=}\NormalTok{ data.loc[:, [}\StringTok{"person\_age"}\NormalTok{, }\StringTok{"loan\_percent\_income"}\NormalTok{, }\StringTok{"loan\_int\_rate"}\NormalTok{, }\StringTok{"cb\_person\_default\_on\_file"}\NormalTok{ ,}\StringTok{"loan\_status"}\NormalTok{]]}



\KeywordTok{def}\NormalTok{ calc\_entropy(df, decision\_on }\OperatorTok{=} \StringTok{"loan\_status"}\NormalTok{):}
  \ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(df)}\OperatorTok{==}\DecValTok{0}\NormalTok{:}
    \ControlFlowTok{return}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{  p }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(df[decision\_on] }\OperatorTok{==} \DecValTok{0}\NormalTok{)}\OperatorTok{/}\BuiltInTok{len}\NormalTok{(df)}
  \ControlFlowTok{if}\NormalTok{ p }\OperatorTok{==} \DecValTok{0} \KeywordTok{or}\NormalTok{ p }\OperatorTok{==} \DecValTok{1}\NormalTok{:}
    \ControlFlowTok{return}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{  result }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{p }\OperatorTok{*}\NormalTok{ ma.log(p,}\DecValTok{2}\NormalTok{) }\OperatorTok{{-}}\NormalTok{ (}\DecValTok{1}\OperatorTok{{-}}\NormalTok{p)}\OperatorTok{*}\NormalTok{ma.log(}\DecValTok{1}\OperatorTok{{-}}\NormalTok{p,}\DecValTok{2}\NormalTok{)}
  \ControlFlowTok{return}\NormalTok{ result}

\KeywordTok{def}\NormalTok{ calc\_splitted\_entropy(df, col, val, decision\_on }\OperatorTok{=} \StringTok{"loan\_status"}\NormalTok{):}
\NormalTok{  w }\OperatorTok{=}\NormalTok{ np.}\BuiltInTok{sum}\NormalTok{(df[col] }\OperatorTok{\textgreater{}}\NormalTok{ val)}\OperatorTok{/}\BuiltInTok{len}\NormalTok{(df)}
\NormalTok{  result }\OperatorTok{=}\NormalTok{ w }\OperatorTok{*}\NormalTok{ calc\_entropy(df.loc[df[col] }\OperatorTok{\textgreater{}}\NormalTok{ val], decision\_on) }\OperatorTok{+}\NormalTok{ (}\DecValTok{1}\OperatorTok{{-}}\NormalTok{w) }\OperatorTok{*}\NormalTok{ calc\_entropy(df.loc[df[col] }\OperatorTok{\textless{}=}\NormalTok{ val], decision\_on)}
  \ControlFlowTok{return}\NormalTok{ result}

\KeywordTok{def}\NormalTok{ find\_minima(df, col, decision\_on }\OperatorTok{=} \StringTok{"loan\_status"}\NormalTok{, round\_at }\OperatorTok{=} \DecValTok{5}\NormalTok{):}
\NormalTok{  direction }\OperatorTok{=} \DecValTok{1}
\NormalTok{  step }\OperatorTok{=}\NormalTok{ (df[col].}\BuiltInTok{max}\NormalTok{()}\OperatorTok{{-}}\NormalTok{df[col].}\BuiltInTok{min}\NormalTok{()) }\OperatorTok{*} \FloatTok{0.1}
\NormalTok{  val }\OperatorTok{=}\NormalTok{ df[col].}\BuiltInTok{min}\NormalTok{() }\OperatorTok{+}\NormalTok{ step}
\NormalTok{  best\_entropy }\OperatorTok{=} \DecValTok{1}
\NormalTok{  stagnation }\OperatorTok{=} \DecValTok{0}
  
  \ControlFlowTok{while}\NormalTok{ stagnation }\OperatorTok{\textless{}=} \DecValTok{15}\NormalTok{:}
\NormalTok{    temp }\OperatorTok{=}\NormalTok{ calc\_splitted\_entropy(df, col, val)}
    \ControlFlowTok{if}\NormalTok{ temp }\OperatorTok{\textgreater{}}\NormalTok{ best\_entropy:}
\NormalTok{      direction }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{direction}
\NormalTok{      step }\OperatorTok{=} \FloatTok{0.5} \OperatorTok{*}\NormalTok{ step}
\NormalTok{      stagnation }\OperatorTok{+=} \DecValTok{1}
    \ControlFlowTok{elif} \BuiltInTok{round}\NormalTok{(temp,round\_at) }\OperatorTok{\textless{}} \BuiltInTok{round}\NormalTok{(best\_entropy,round\_at):}
\NormalTok{      stagnation }\OperatorTok{=} \DecValTok{0}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{      stagnation }\OperatorTok{+=} \DecValTok{1}
\NormalTok{    best\_entropy }\OperatorTok{=}\NormalTok{ temp}
\NormalTok{    val }\OperatorTok{=}\NormalTok{ val }\OperatorTok{+}\NormalTok{ direction }\OperatorTok{*}\NormalTok{ step}
    
  \ControlFlowTok{return}\NormalTok{ best\_entropy, val}


\KeywordTok{def}\NormalTok{ find\_best\_col(df, decision\_on }\OperatorTok{=} \StringTok{"loan\_status"}\NormalTok{, round\_at }\OperatorTok{=} \DecValTok{5}\NormalTok{):}
\NormalTok{  cols }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(df.columns[df.columns }\OperatorTok{!=}\NormalTok{ decision\_on])}
\NormalTok{  entropys }\OperatorTok{=}\NormalTok{ np.ones(}\BuiltInTok{len}\NormalTok{(cols))}
\NormalTok{  vals }\OperatorTok{=}\NormalTok{ np.ones(}\BuiltInTok{len}\NormalTok{(cols))}
  
  \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(cols)):}
\NormalTok{    entropys[i], vals[i] }\OperatorTok{=}\NormalTok{ find\_minima(df, col}\OperatorTok{=}\NormalTok{cols[i], decision\_on }\OperatorTok{=} \StringTok{"loan\_status"}\NormalTok{, round\_at }\OperatorTok{=} \DecValTok{5}\NormalTok{)}
  
\NormalTok{  best\_i }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(np.where(entropys }\OperatorTok{==} \BuiltInTok{min}\NormalTok{(entropys))[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{])}
  \ControlFlowTok{return}\NormalTok{ cols[best\_i], entropys[best\_i], vals[best\_i]}




\KeywordTok{def}\NormalTok{ make\_node\_and\_leafs(df, decision\_on }\OperatorTok{=} \StringTok{"loan\_status"}\NormalTok{, round\_at }\OperatorTok{=} \DecValTok{5}\NormalTok{, path }\OperatorTok{=} \StringTok{"I"}\NormalTok{, condition }\OperatorTok{=} \StringTok{""}\NormalTok{, min\_size }\OperatorTok{=} \DecValTok{1000}\NormalTok{, max\_depth }\OperatorTok{=} \DecValTok{4}\NormalTok{, leafs }\OperatorTok{=}\NormalTok{ pd.DataFrame(columns}\OperatorTok{=}\NormalTok{[}\StringTok{"path"}\NormalTok{, }\StringTok{"condition"}\NormalTok{, }\StringTok{"rows"}\NormalTok{, }\StringTok{"P\_of\_no\_default"}\NormalTok{, }\StringTok{"entropy"}\NormalTok{])):}
  \ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(df) }\OperatorTok{\textless{}}\NormalTok{ min\_size }\KeywordTok{or}\NormalTok{ (path.count(}\StringTok{"{-}"}\NormalTok{)}\OperatorTok{{-}}\DecValTok{1}\NormalTok{) }\OperatorTok{\textgreater{}=}\NormalTok{ max\_depth }\KeywordTok{or} \BuiltInTok{len}\NormalTok{(df.columns) }\OperatorTok{\textless{}=} \DecValTok{1}\NormalTok{:}
\NormalTok{    leafs }\OperatorTok{=}\NormalTok{ leafs.append(\{}\StringTok{"path"}\NormalTok{:path}\OperatorTok{+}\StringTok{"\}"}\NormalTok{, }\StringTok{"condition"}\NormalTok{:condition[}\DecValTok{0}\NormalTok{:(}\BuiltInTok{len}\NormalTok{(condition)}\OperatorTok{{-}}\DecValTok{5}\NormalTok{)], }\StringTok{"rows"}\NormalTok{:}\BuiltInTok{len}\NormalTok{(df), }\StringTok{"P\_of\_no\_default"}\NormalTok{:np.}\BuiltInTok{sum}\NormalTok{(df[decision\_on] }\OperatorTok{==} \DecValTok{0}\NormalTok{)}\OperatorTok{/}\BuiltInTok{len}\NormalTok{(df), }\StringTok{"entropy"}\NormalTok{:calc\_entropy(df)\}, ignore\_index}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
  \ControlFlowTok{else}\NormalTok{:}
\NormalTok{    col, entropy, val }\OperatorTok{=}\NormalTok{ find\_best\_col(df, decision\_on, round\_at)}
    \BuiltInTok{print}\NormalTok{(}\StringTok{"path:"}\NormalTok{, path, }\StringTok{"   entropy:"}\NormalTok{, entropy, }\StringTok{"  col:"}\NormalTok{, col, }\StringTok{"   val:"}\NormalTok{, val, }\StringTok{"  rows:"}\NormalTok{, }\BuiltInTok{len}\NormalTok{(df))}
\NormalTok{    leafs }\OperatorTok{=}\NormalTok{ make\_node\_and\_leafs( df.loc[df[col] }\OperatorTok{\textgreater{}}\NormalTok{ val, df.columns }\OperatorTok{!=}\NormalTok{ col], decision\_on, round\_at, path}\OperatorTok{+}\StringTok{"{-}R"}\NormalTok{, condition}\OperatorTok{+}\NormalTok{col}\OperatorTok{+}\StringTok{" \textgreater{} "}\OperatorTok{+}\BuiltInTok{str}\NormalTok{(}\BuiltInTok{float}\NormalTok{(}\BuiltInTok{round}\NormalTok{(val,}\DecValTok{5}\NormalTok{)))}\OperatorTok{+}\StringTok{" and "}\NormalTok{, min\_size, max\_depth, leafs)}
\NormalTok{    leafs }\OperatorTok{=}\NormalTok{ make\_node\_and\_leafs( df.loc[df[col] }\OperatorTok{\textless{}=}\NormalTok{ val, df.columns }\OperatorTok{!=}\NormalTok{ col], decision\_on, round\_at, path }\OperatorTok{+} \StringTok{"{-}L"}\NormalTok{, condition}\OperatorTok{+}\NormalTok{col}\OperatorTok{+}\StringTok{" \textless{}= "}\OperatorTok{+}\BuiltInTok{str}\NormalTok{(}\BuiltInTok{float}\NormalTok{(}\BuiltInTok{round}\NormalTok{(val,}\DecValTok{5}\NormalTok{)))}\OperatorTok{+}\StringTok{" and "}\NormalTok{, min\_size, max\_depth, leafs)}
  \ControlFlowTok{return}\NormalTok{(leafs)}
  
  
  


\NormalTok{leafs }\OperatorTok{=}\NormalTok{ make\_node\_and\_leafs(df}\OperatorTok{=}\NormalTok{data, decision\_on }\OperatorTok{=} \StringTok{"loan\_status"}\NormalTok{, round\_at }\OperatorTok{=} \DecValTok{5}\NormalTok{, min\_size }\OperatorTok{=} \DecValTok{1000}\NormalTok{, max\_depth }\OperatorTok{=} \DecValTok{4}\NormalTok{)}
\NormalTok{leafs[}\StringTok{"entropy"}\NormalTok{] }\OperatorTok{=}\NormalTok{ (leafs[}\StringTok{"entropy"}\NormalTok{]}\OperatorTok{*}\NormalTok{leafs[}\StringTok{"rows"}\NormalTok{])}\OperatorTok{/}\BuiltInTok{len}\NormalTok{(data)}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Entropy in data: "}\NormalTok{, calc\_entropy(data))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Entropy in all leafs: "}\NormalTok{, np.}\BuiltInTok{sum}\NormalTok{(leafs[}\StringTok{"entropy"}\NormalTok{]))}


\NormalTok{data\_temp }\OperatorTok{=}\NormalTok{ data.copy()}
\NormalTok{data\_temp[}\StringTok{"ID"}\NormalTok{] }\OperatorTok{=} \BuiltInTok{list}\NormalTok{(}\BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(data\_temp)))}
\NormalTok{conditions }\OperatorTok{=} \StringTok{"("}\OperatorTok{+} \StringTok{") | ("}\NormalTok{.join(}\BuiltInTok{list}\NormalTok{(leafs.loc[leafs[}\StringTok{"P\_of\_no\_default"}\NormalTok{] }\OperatorTok{\textless{}} \FloatTok{0.65}\NormalTok{, leafs.columns }\OperatorTok{==} \StringTok{"condition"}\NormalTok{][}\StringTok{"condition"}\NormalTok{].replace(}\StringTok{"and"}\NormalTok{,}\StringTok{"\&"}\NormalTok{)))}\OperatorTok{+}\StringTok{")"}
\NormalTok{data\_temp }\OperatorTok{=}\NormalTok{ data\_temp.query(conditions)}
\NormalTok{X }\OperatorTok{=}\NormalTok{ np.zeros(}\BuiltInTok{len}\NormalTok{(data))}
\NormalTok{X[}\BuiltInTok{list}\NormalTok{(data\_temp[}\StringTok{"ID"}\NormalTok{])] }\OperatorTok{=} \DecValTok{1}

\NormalTok{Y }\OperatorTok{=}\NormalTok{ data.loc[:, data.columns }\OperatorTok{==} \StringTok{\textquotesingle{}loan\_status\textquotesingle{}}\NormalTok{].to\_numpy()[:,}\DecValTok{0}\NormalTok{]}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Wrong answers of the decission tree: "}\NormalTok{,np.}\BuiltInTok{sum}\NormalTok{(np.}\BuiltInTok{abs}\NormalTok{(Y}\OperatorTok{{-}}\NormalTok{X))}\OperatorTok{/}\BuiltInTok{len}\NormalTok{(Y) }\OperatorTok{*} \DecValTok{100}\NormalTok{, }\StringTok{"\%"}\NormalTok{)}
\NormalTok{confusion\_matrix(Y,X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## path: I    entropy: 0.6564119496913492   col: loan_percent_income    val: 0.29990234374999997   rows: 32581
## path: I-R    entropy: 0.9103981172376336   col: loan_int_rate    val: 14.01635546875   rows: 4285
## path: I-R-L    entropy: 0.9607335396537932   col: cb_person_default_on_file    val: 0.14999999999999947   rows: 3361
## path: I-R-L-L    entropy: 0.9648046914552115   col: person_age    val: 34.296875   rows: 2977
## path: I-L    entropy: 0.531844917392527   col: loan_int_rate    val: 14.280073242187505   rows: 28296
## path: I-L-R    entropy: 0.998669387657678   col: person_age    val: 22.3046875   rows: 4057
## path: I-L-R-R    entropy: 0.9992350724472542   col: cb_person_default_on_file    val: 0.14999999999999947   rows: 3488
## path: I-L-L    entropy: 0.44498607709157834   col: cb_person_default_on_file    val: 0.14999999999999947   rows: 24239
## path: I-L-L-R    entropy: 0.704963233218175   col: person_age    val: 33.9921875   rows: 2838
## path: I-L-L-L    entropy: 0.40973705382045866   col: person_age    val: 21.840624999999967   rows: 21401
## Entropy in data:  0.7568007498467375
## Entropy in all leafs:  0.574488597974446
## Wrong answers of the decission tree:  17.94911144532089 %
## array([[21932,  3541],
##        [ 2307,  4801]], dtype=int64)
\end{verbatim}

  \bibliography{book.bib,packages.bib}

\end{document}
