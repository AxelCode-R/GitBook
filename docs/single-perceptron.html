<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Single Perceptron | My First Steps in Neuronal Networks (Beginners Guide)</title>
  <meta name="description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Single Perceptron | My First Steps in Neuronal Networks (Beginners Guide)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  <meta name="github-repo" content="https://github.com/AxelCode-R/GitBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Single Perceptron | My First Steps in Neuronal Networks (Beginners Guide)" />
  
  <meta name="twitter:description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  

<meta name="author" content="Axel Roth" />


<meta name="date" content="2021-11-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="adding-trainable-bias.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A GitBook for Teaching</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#me"><i class="fa fa-check"></i><b>1.1</b> Me</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-book"><i class="fa fa-check"></i><b>1.2</b> The Book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#how-it-works"><i class="fa fa-check"></i><b>1.3</b> How it works</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="single-perceptron.html"><a href="single-perceptron.html"><i class="fa fa-check"></i><b>2</b> Single Perceptron</a>
<ul>
<li class="chapter" data-level="2.1" data-path="single-perceptron.html"><a href="single-perceptron.html#neural-network-basics"><i class="fa fa-check"></i><b>2.1</b> Neural Network Basics</a></li>
<li class="chapter" data-level="2.2" data-path="single-perceptron.html"><a href="single-perceptron.html#forward-pass"><i class="fa fa-check"></i><b>2.2</b> Forward pass</a></li>
<li class="chapter" data-level="2.3" data-path="single-perceptron.html"><a href="single-perceptron.html#backward-pass"><i class="fa fa-check"></i><b>2.3</b> backward pass</a></li>
<li class="chapter" data-level="2.4" data-path="single-perceptron.html"><a href="single-perceptron.html#single-perceptron-1"><i class="fa fa-check"></i><b>2.4</b> Single Perceptron</a></li>
<li class="chapter" data-level="2.5" data-path="single-perceptron.html"><a href="single-perceptron.html#appendix-full-code"><i class="fa fa-check"></i><b>2.5</b> Appendix (full code)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="adding-trainable-bias.html"><a href="adding-trainable-bias.html"><i class="fa fa-check"></i><b>3</b> Adding trainable Bias</a></li>
<li class="chapter" data-level="4" data-path="multi-layer-perceptrons.html"><a href="multi-layer-perceptrons.html"><i class="fa fa-check"></i><b>4</b> Multi Layer Perceptrons</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">My First Steps in Neuronal Networks (Beginners Guide)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="single-perceptron" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Single Perceptron</h1>
<p>In this chapter i will teach you how to code a single Perceptron in python with only the numpy package. Numpy uses a vectorizible math structure in which you can easily calculate elementwise or do stuff like normal matrix multiplications with just a symbol (i always interpret Vectors as one dimensional matrices!). At the most of the time, its just translating math formulas into python code without changing its structure.<br />
First of all we are starting with the needed parameters, that are explained later:</p>
<p>Number of runs over the training data := <code>n_train</code><br />
Learning rate := <span class="math inline">\(\alpha =\)</span> <code>alpha</code><br />
Bias value := <span class="math inline">\(\beta =\)</span> <code>bias</code>
and the activation function:<br />
<span class="math display">\[ 
step(s)= 
\begin{cases}
    1,&amp; s   \geq \beta\\
    0,&amp; s &lt; \beta
\end{cases}
\]</span>
This function is named the heavyside-function and should be the easiest activation function to start with. If the weighted sum is smaller than the <code>bias</code> it will send the value zero to the next neuron. Our brain works with the same behavior. If the electricity is too small, the neuron will not activate and the next one dosent get any electricity.</p>
<p>The traings dataset is the following:
<span class="math display">\[
\left[
\begin{array}{cc|c}
x_i,_1 &amp; x_i,_2 &amp; y_i \\
\end{array}
\right]
\]</span>
<span class="math display">\[
\left[
\begin{array}{cc|c}
0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 1 \\
\end{array}
\right]
\]</span>
The provided training dataset contains the <code>X</code> matrix with two inputs for each scenario and the <code>Y</code> matrix with the correct output. If your looking exactly you can see that this is the OR-Gate. Later you will see why these type of problems are the only suitable things to do with a single neuron.</p>
<p>The needed python imports and default options are the following:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="single-perceptron.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="single-perceptron.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random <span class="im">as</span> ra</span>
<span id="cb2-3"><a href="single-perceptron.html#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-4"><a href="single-perceptron.html#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> pyplot</span>
<span id="cb2-5"><a href="single-perceptron.html#cb2-5" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">&#39;display.max_rows&#39;</span>, <span class="dv">500</span>)</span>
<span id="cb2-6"><a href="single-perceptron.html#cb2-6" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">&#39;display.max_columns&#39;</span>, <span class="dv">500</span>)</span>
<span id="cb2-7"><a href="single-perceptron.html#cb2-7" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">&#39;display.width&#39;</span>, <span class="dv">1000</span>)</span></code></pre></div>
<p>( Do you see more imports than only the numpy package? <a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">Yes</a> or No )</p>
<p>Now that we have all the needed parameters and settings, i can give you a quick overview of the algorithm.</p>
<div id="neural-network-basics" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Neural Network Basics</h2>
<p>In a NN we are having two basic parts, the forward pass and the backward pass. In the forward pass we are calculating the weighted sum of each input with its weights of the layer and evaluating the activation function with it, to calculate the output. In the backward pass we are analyzing the error to adjust the weights accordingly. This is it! This is all a NN will do. I explained everything to you. Have a good life…<br />
Ahh no no ok we have a look deeper into it :)</p>
<p>Whats exactly is the forward pass in a single Perceptron? Its just the evaluation of the acvtivation function with the weighted sum like i said, so you have (with the matrix dimensions included) for one scenario out of the training dataset, the following:
<span class="math display">\[
  step(W^{(1,2)} \cdot x^{(2,1)}) = y^{(1,1)}
\]</span>
That is the normal approach to iterate over all scenarios in the training dataset…<br />
But i think its not the right way to describe it, because it gets very confusing to interpret it for all scenarios.</p>
<p>My next approach is to consider all scenarios in the training dataset in one formula. If your data isnt that huge, its a much faster approach as well. First of all we need to interpret the new dimensions of <span class="math inline">\(W\)</span> and <span class="math inline">\(X\)</span>.<br />
We have <span class="math inline">\(X\)</span> as:
<span class="math display">\[
  X = \left[
  \begin{array}{cc}
  0 &amp; 0 \\
  0 &amp; 1 \\
  1 &amp; 0 \\
  1 &amp; 1 \\
  \end{array}
  \right]
\]</span>
each row describes the inputs for each neuron in the scenario <span class="math inline">\(i\)</span>.<br />
For the weights <span class="math inline">\(W\)</span> we have for example:
<span class="math display">\[
  W =\left[
  \begin{array}{c}
  0.1 \\ 
  0.2 \\ 
  \end{array}
  \right]
\]</span>
The new formular looks like this:
<span class="math display">\[
  step(X \cdot W) = Y
\]</span>
For example if you take a look at the <span class="math inline">\(i\)</span>-th row or scenario of <span class="math inline">\(X\)</span> you will see the following:
<span class="math display">\[
  Y_i,_0 = step([X_i,_0 \cdot W_0,_0 + X_i,_1 \cdot W_1,_0 ])
\]</span>
and <span class="math inline">\(Y_i,_0\)</span> is the approximated output of the <span class="math inline">\(i\)</span>-th scenario. Now we can look at the NN and compare the formula with it:<br />
<img src="img/NN_01_v4.png" style="width:50.0%" alt="Made with the awesome chart website lucidecharts" /><br></p>
<p>Yes it is the same, its the weighted sum of the inputs and evaluated the activation function with it to calculate the output of the scenario <span class="math inline">\(i\)</span>.</p>
</div>
<div id="forward-pass" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Forward pass</h2>
<p>Now we create the so called <code>forward()</code> function in python:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="single-perceptron.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(X, W):</span>
<span id="cb3-2"><a href="single-perceptron.html#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( step(X <span class="op">@</span> W) )</span></code></pre></div>
<p>(Numpy provides us with the <code>@</code> symbol to make a matrix multiplication and the <code>.T</code> to transpose)</p>
<p>Because we want to put one dimensional matrices into the <code>step()</code> function we need to use numpy for the if-else statement:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="single-perceptron.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step(s):</span>
<span id="cb4-2"><a href="single-perceptron.html#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( np.where(s <span class="op">&gt;=</span> bias, <span class="dv">1</span>, <span class="dv">0</span>) )</span></code></pre></div>
<p>Here an small example for the forward pass:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="single-perceptron.html#cb5-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([</span>
<span id="cb5-2"><a href="single-perceptron.html#cb5-2" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">0</span>],</span>
<span id="cb5-3"><a href="single-perceptron.html#cb5-3" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb5-4"><a href="single-perceptron.html#cb5-4" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">0</span>],</span>
<span id="cb5-5"><a href="single-perceptron.html#cb5-5" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb5-6"><a href="single-perceptron.html#cb5-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb5-7"><a href="single-perceptron.html#cb5-7" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.array([</span>
<span id="cb5-8"><a href="single-perceptron.html#cb5-8" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.1</span>], </span>
<span id="cb5-9"><a href="single-perceptron.html#cb5-9" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.2</span>]</span>
<span id="cb5-10"><a href="single-perceptron.html#cb5-10" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb5-11"><a href="single-perceptron.html#cb5-11" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-12"><a href="single-perceptron.html#cb5-12" aria-hidden="true" tabindex="-1"></a>Y_approx <span class="op">=</span> forward(X, W)</span>
<span id="cb5-13"><a href="single-perceptron.html#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(Y_approx)</span></code></pre></div>
<pre><code>## [[0]
##  [0]
##  [0]
##  [0]]</code></pre>
<p>And these are all the generated outputs of our NN over all scenarios. Now we need to calculate the error and adjust the weights accordingly.</p>
</div>
<div id="backward-pass" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> backward pass</h2>
<p>To adjust the weights in a single Perceptron, we need the Delta-Rule:
<span class="math display">\[
  W(t+1) = W(t) + \Delta W(t)
\]</span>
with
<span class="math display">\[
  \Delta W(t) = \alpha \cdot X^{T} \cdot (Y - \hat{Y})
\]</span>
and <span class="math inline">\(\hat{Y}\)</span> is the output of the NN.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="single-perceptron.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward(W, X, Y, alpha, Y_approx):</span>
<span id="cb7-2"><a href="single-perceptron.html#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(W <span class="op">+</span> alpha <span class="op">*</span> X.T <span class="op">@</span> (Y <span class="op">-</span> Y_approx))</span></code></pre></div>
<p>with the result of the forward pass and example data we have the following:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="single-perceptron.html#cb8-1" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([</span>
<span id="cb8-2"><a href="single-perceptron.html#cb8-2" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>],</span>
<span id="cb8-3"><a href="single-perceptron.html#cb8-3" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb8-4"><a href="single-perceptron.html#cb8-4" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb8-5"><a href="single-perceptron.html#cb8-5" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>]</span>
<span id="cb8-6"><a href="single-perceptron.html#cb8-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb8-7"><a href="single-perceptron.html#cb8-7" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb8-8"><a href="single-perceptron.html#cb8-8" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> backward(W, X, Y, alpha, Y_approx)</span>
<span id="cb8-9"><a href="single-perceptron.html#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(W)</span></code></pre></div>
<pre><code>## [[0.12]
##  [0.22]]</code></pre>
<p>and this is the new weight.</p>
</div>
<div id="single-perceptron-1" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Single Perceptron</h2>
<p>Now we want to do the same process multiple times, to train the NN:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="single-perceptron.html#cb10-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([</span>
<span id="cb10-2"><a href="single-perceptron.html#cb10-2" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">0</span>],</span>
<span id="cb10-3"><a href="single-perceptron.html#cb10-3" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb10-4"><a href="single-perceptron.html#cb10-4" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">0</span>],</span>
<span id="cb10-5"><a href="single-perceptron.html#cb10-5" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb10-6"><a href="single-perceptron.html#cb10-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb10-7"><a href="single-perceptron.html#cb10-7" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([</span>
<span id="cb10-8"><a href="single-perceptron.html#cb10-8" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>],</span>
<span id="cb10-9"><a href="single-perceptron.html#cb10-9" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb10-10"><a href="single-perceptron.html#cb10-10" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb10-11"><a href="single-perceptron.html#cb10-11" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>]</span>
<span id="cb10-12"><a href="single-perceptron.html#cb10-12" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb10-13"><a href="single-perceptron.html#cb10-13" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.array([</span>
<span id="cb10-14"><a href="single-perceptron.html#cb10-14" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.1</span>], </span>
<span id="cb10-15"><a href="single-perceptron.html#cb10-15" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.2</span>]</span>
<span id="cb10-16"><a href="single-perceptron.html#cb10-16" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb10-17"><a href="single-perceptron.html#cb10-17" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb10-18"><a href="single-perceptron.html#cb10-18" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb10-19"><a href="single-perceptron.html#cb10-19" aria-hidden="true" tabindex="-1"></a>train_n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb10-20"><a href="single-perceptron.html#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="single-perceptron.html#cb10-21" aria-hidden="true" tabindex="-1"></a>errors <span class="op">=</span> []</span>
<span id="cb10-22"><a href="single-perceptron.html#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(train_n):</span>
<span id="cb10-23"><a href="single-perceptron.html#cb10-23" aria-hidden="true" tabindex="-1"></a>  Y_approx <span class="op">=</span> forward(X, W)</span>
<span id="cb10-24"><a href="single-perceptron.html#cb10-24" aria-hidden="true" tabindex="-1"></a>  errors.append(Y <span class="op">-</span> Y_approx)</span>
<span id="cb10-25"><a href="single-perceptron.html#cb10-25" aria-hidden="true" tabindex="-1"></a>  W <span class="op">=</span> backward(W, X, Y, alpha, Y_approx)</span></code></pre></div>
<p>The KNN is trained now. Now we want to look at the error. We want to measure the mean-square-error with the following formula:
<span class="math display">\[
  Error_i = \frac{1}{2} \cdot \sum(Y-\hat{Y})^2
\]</span>
or as python code:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="single-perceptron.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mean_square_error(error):</span>
<span id="cb11-2"><a href="single-perceptron.html#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(error <span class="op">**</span> <span class="dv">2</span>) )</span></code></pre></div>
<p>Now we need to calculate the mean-square-error for each element in the list <code>errors</code> which is made with <code>map()</code>:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="single-perceptron.html#cb12-1" aria-hidden="true" tabindex="-1"></a>mean_square_errors <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(mean_square_error, errors)))</span></code></pre></div>
<p>To plot the errors, im using the following function:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="single-perceptron.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_error(errors, title):</span>
<span id="cb13-2"><a href="single-perceptron.html#cb13-2" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(errors)))</span>
<span id="cb13-3"><a href="single-perceptron.html#cb13-3" aria-hidden="true" tabindex="-1"></a>  y <span class="op">=</span> np.array(errors)</span>
<span id="cb13-4"><a href="single-perceptron.html#cb13-4" aria-hidden="true" tabindex="-1"></a>  pyplot.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb13-5"><a href="single-perceptron.html#cb13-5" aria-hidden="true" tabindex="-1"></a>  pyplot.plot(x, y, <span class="st">&quot;g&quot;</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-6"><a href="single-perceptron.html#cb13-6" aria-hidden="true" tabindex="-1"></a>  pyplot.xlabel(<span class="st">&quot;Iterations&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb13-7"><a href="single-perceptron.html#cb13-7" aria-hidden="true" tabindex="-1"></a>  pyplot.ylabel(<span class="st">&quot;Mean Square Error&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb13-8"><a href="single-perceptron.html#cb13-8" aria-hidden="true" tabindex="-1"></a>  pyplot.title(title)</span>
<span id="cb13-9"><a href="single-perceptron.html#cb13-9" aria-hidden="true" tabindex="-1"></a>  pyplot.ylim(<span class="op">-</span><span class="fl">0.01</span>,<span class="bu">max</span>(errors)<span class="op">*</span><span class="fl">1.2</span>)</span>
<span id="cb13-10"><a href="single-perceptron.html#cb13-10" aria-hidden="true" tabindex="-1"></a>  pyplot.show()</span>
<span id="cb13-11"><a href="single-perceptron.html#cb13-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-12"><a href="single-perceptron.html#cb13-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-13"><a href="single-perceptron.html#cb13-13" aria-hidden="true" tabindex="-1"></a>plot_error(mean_square_errors, <span class="st">&quot;Mean-Square-Errors of a single Perceptron&quot;</span>)</span></code></pre></div>
<p><img src="gitbook-demo_files/figure-html/unnamed-chunk-11-1.png" width="576" /></p>
<p>If you survived until now, you have learned how you can program a single Perceptron!</p>
</div>
<div id="appendix-full-code" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Appendix (full code)</h2>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="single-perceptron.html#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-2"><a href="single-perceptron.html#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random <span class="im">as</span> ra</span>
<span id="cb14-3"><a href="single-perceptron.html#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb14-4"><a href="single-perceptron.html#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> pyplot</span>
<span id="cb14-5"><a href="single-perceptron.html#cb14-5" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">&#39;display.max_rows&#39;</span>, <span class="dv">500</span>)</span>
<span id="cb14-6"><a href="single-perceptron.html#cb14-6" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">&#39;display.max_columns&#39;</span>, <span class="dv">500</span>)</span>
<span id="cb14-7"><a href="single-perceptron.html#cb14-7" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">&#39;display.width&#39;</span>, <span class="dv">1000</span>)</span>
<span id="cb14-8"><a href="single-perceptron.html#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="single-perceptron.html#cb14-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([</span>
<span id="cb14-10"><a href="single-perceptron.html#cb14-10" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">0</span>],</span>
<span id="cb14-11"><a href="single-perceptron.html#cb14-11" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb14-12"><a href="single-perceptron.html#cb14-12" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">0</span>],</span>
<span id="cb14-13"><a href="single-perceptron.html#cb14-13" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb14-14"><a href="single-perceptron.html#cb14-14" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb14-15"><a href="single-perceptron.html#cb14-15" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([</span>
<span id="cb14-16"><a href="single-perceptron.html#cb14-16" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>],</span>
<span id="cb14-17"><a href="single-perceptron.html#cb14-17" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb14-18"><a href="single-perceptron.html#cb14-18" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb14-19"><a href="single-perceptron.html#cb14-19" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>]</span>
<span id="cb14-20"><a href="single-perceptron.html#cb14-20" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb14-21"><a href="single-perceptron.html#cb14-21" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.array([</span>
<span id="cb14-22"><a href="single-perceptron.html#cb14-22" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.1</span>], </span>
<span id="cb14-23"><a href="single-perceptron.html#cb14-23" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.2</span>]</span>
<span id="cb14-24"><a href="single-perceptron.html#cb14-24" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb14-25"><a href="single-perceptron.html#cb14-25" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb14-26"><a href="single-perceptron.html#cb14-26" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb14-27"><a href="single-perceptron.html#cb14-27" aria-hidden="true" tabindex="-1"></a>train_n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb14-28"><a href="single-perceptron.html#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="single-perceptron.html#cb14-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step(s):</span>
<span id="cb14-30"><a href="single-perceptron.html#cb14-30" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( np.where(s <span class="op">&gt;=</span> bias, <span class="dv">1</span>, <span class="dv">0</span>) )</span>
<span id="cb14-31"><a href="single-perceptron.html#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="single-perceptron.html#cb14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-33"><a href="single-perceptron.html#cb14-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(X, W):</span>
<span id="cb14-34"><a href="single-perceptron.html#cb14-34" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( step(X <span class="op">@</span> W) )</span>
<span id="cb14-35"><a href="single-perceptron.html#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="single-perceptron.html#cb14-36" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward(W, X, Y, alpha, Y_approx):</span>
<span id="cb14-37"><a href="single-perceptron.html#cb14-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(W <span class="op">+</span> alpha <span class="op">*</span> X.T <span class="op">@</span> (Y <span class="op">-</span> Y_approx))</span>
<span id="cb14-38"><a href="single-perceptron.html#cb14-38" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-39"><a href="single-perceptron.html#cb14-39" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-40"><a href="single-perceptron.html#cb14-40" aria-hidden="true" tabindex="-1"></a>errors <span class="op">=</span> []</span>
<span id="cb14-41"><a href="single-perceptron.html#cb14-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(train_n<span class="op">*</span><span class="dv">5</span>):</span>
<span id="cb14-42"><a href="single-perceptron.html#cb14-42" aria-hidden="true" tabindex="-1"></a>  Y_approx <span class="op">=</span> forward(X, W)</span>
<span id="cb14-43"><a href="single-perceptron.html#cb14-43" aria-hidden="true" tabindex="-1"></a>  errors.append(Y <span class="op">-</span> Y_approx)</span>
<span id="cb14-44"><a href="single-perceptron.html#cb14-44" aria-hidden="true" tabindex="-1"></a>  W <span class="op">=</span> backward(W, X, Y, alpha, Y_approx)</span>
<span id="cb14-45"><a href="single-perceptron.html#cb14-45" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-46"><a href="single-perceptron.html#cb14-46" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-47"><a href="single-perceptron.html#cb14-47" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-48"><a href="single-perceptron.html#cb14-48" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mean_square_error(error):</span>
<span id="cb14-49"><a href="single-perceptron.html#cb14-49" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(error <span class="op">**</span> <span class="dv">2</span>) )</span>
<span id="cb14-50"><a href="single-perceptron.html#cb14-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-51"><a href="single-perceptron.html#cb14-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-52"><a href="single-perceptron.html#cb14-52" aria-hidden="true" tabindex="-1"></a>mean_square_errors <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(mean_square_error, errors)))</span>
<span id="cb14-53"><a href="single-perceptron.html#cb14-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-54"><a href="single-perceptron.html#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="single-perceptron.html#cb14-55" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_error(errors, title):</span>
<span id="cb14-56"><a href="single-perceptron.html#cb14-56" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(errors)))</span>
<span id="cb14-57"><a href="single-perceptron.html#cb14-57" aria-hidden="true" tabindex="-1"></a>  y <span class="op">=</span> np.array(errors)</span>
<span id="cb14-58"><a href="single-perceptron.html#cb14-58" aria-hidden="true" tabindex="-1"></a>  pyplot.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb14-59"><a href="single-perceptron.html#cb14-59" aria-hidden="true" tabindex="-1"></a>  pyplot.plot(x, y, <span class="st">&quot;g&quot;</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-60"><a href="single-perceptron.html#cb14-60" aria-hidden="true" tabindex="-1"></a>  pyplot.xlabel(<span class="st">&quot;Iterations&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb14-61"><a href="single-perceptron.html#cb14-61" aria-hidden="true" tabindex="-1"></a>  pyplot.ylabel(<span class="st">&quot;Mean Square Error&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb14-62"><a href="single-perceptron.html#cb14-62" aria-hidden="true" tabindex="-1"></a>  pyplot.title(title)</span>
<span id="cb14-63"><a href="single-perceptron.html#cb14-63" aria-hidden="true" tabindex="-1"></a>  pyplot.ylim(<span class="op">-</span><span class="fl">0.01</span>,<span class="bu">max</span>(errors)<span class="op">*</span><span class="fl">1.2</span>)</span>
<span id="cb14-64"><a href="single-perceptron.html#cb14-64" aria-hidden="true" tabindex="-1"></a>  pyplot.show()</span>
<span id="cb14-65"><a href="single-perceptron.html#cb14-65" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-66"><a href="single-perceptron.html#cb14-66" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-67"><a href="single-perceptron.html#cb14-67" aria-hidden="true" tabindex="-1"></a>plot_error(mean_square_errors, <span class="st">&quot;Mean-Square-Errors of a single Perceptron&quot;</span>)</span></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="adding-trainable-bias.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/cjvanlissa/gitbook-demo/edit/master/01_Single_Perceptron.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["gitbook-demo.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
