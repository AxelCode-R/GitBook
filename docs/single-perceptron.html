<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Single Perceptron | gitbook-demo.knit</title>
  <meta name="description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Single Perceptron | gitbook-demo.knit" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  <meta name="github-repo" content="https://github.com/AxelCode-R/GitBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Single Perceptron | gitbook-demo.knit" />
  
  <meta name="twitter:description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  

<meta name="author" content="Axel Roth" />


<meta name="date" content="2021-11-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="adding-trainable-bias.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A GitBook for Teaching</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#me"><i class="fa fa-check"></i><b>1.1</b> Me</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-book"><i class="fa fa-check"></i><b>1.2</b> The Book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#how-it-works"><i class="fa fa-check"></i><b>1.3</b> How it works</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="single-perceptron.html"><a href="single-perceptron.html"><i class="fa fa-check"></i><b>2</b> Single Perceptron</a>
<ul>
<li class="chapter" data-level="2.1" data-path="single-perceptron.html"><a href="single-perceptron.html#neural-network-basics"><i class="fa fa-check"></i><b>2.1</b> Neural Network Basics</a></li>
<li class="chapter" data-level="2.2" data-path="single-perceptron.html"><a href="single-perceptron.html#forward-pass"><i class="fa fa-check"></i><b>2.2</b> Forward pass</a></li>
<li class="chapter" data-level="2.3" data-path="single-perceptron.html"><a href="single-perceptron.html#backward-pass"><i class="fa fa-check"></i><b>2.3</b> backward pass</a></li>
<li class="chapter" data-level="2.4" data-path="single-perceptron.html"><a href="single-perceptron.html#single-perceptron-1"><i class="fa fa-check"></i><b>2.4</b> Single Perceptron</a></li>
<li class="chapter" data-level="2.5" data-path="single-perceptron.html"><a href="single-perceptron.html#appendix-complete-code"><i class="fa fa-check"></i><b>2.5</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="adding-trainable-bias.html"><a href="adding-trainable-bias.html"><i class="fa fa-check"></i><b>3</b> Adding trainable Bias</a>
<ul>
<li class="chapter" data-level="3.1" data-path="adding-trainable-bias.html"><a href="adding-trainable-bias.html#generalising-the-bias"><i class="fa fa-check"></i><b>3.1</b> Generalising the Bias</a></li>
<li class="chapter" data-level="3.2" data-path="adding-trainable-bias.html"><a href="adding-trainable-bias.html#appendix-complete-code-1"><i class="fa fa-check"></i><b>3.2</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multi-layer-perceptrons-mlp.html"><a href="multi-layer-perceptrons-mlp.html"><i class="fa fa-check"></i><b>4</b> Multi Layer Perceptrons (MLP)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multi-layer-perceptrons-mlp.html"><a href="multi-layer-perceptrons-mlp.html#forward-pass-1"><i class="fa fa-check"></i><b>4.1</b> forward pass</a></li>
<li class="chapter" data-level="4.2" data-path="multi-layer-perceptrons-mlp.html"><a href="multi-layer-perceptrons-mlp.html#backward-pass-1"><i class="fa fa-check"></i><b>4.2</b> backward pass</a></li>
<li class="chapter" data-level="4.3" data-path="multi-layer-perceptrons-mlp.html"><a href="multi-layer-perceptrons-mlp.html#appendix-complete-code-2"><i class="fa fa-check"></i><b>4.3</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html"><i class="fa fa-check"></i><b>5</b> MLP example (Credit Default)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html#loading-and-analysing-the-data"><i class="fa fa-check"></i><b>5.1</b> Loading and analysing the data</a></li>
<li class="chapter" data-level="5.2" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html#train-and-test-phase"><i class="fa fa-check"></i><b>5.2</b> Train and test phase</a></li>
<li class="chapter" data-level="5.3" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html#appendix-complete-code-3"><i class="fa fa-check"></i><b>5.3</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="effect-of-batch-size.html"><a href="effect-of-batch-size.html"><i class="fa fa-check"></i><b>6</b> Effect of batch size</a>
<ul>
<li class="chapter" data-level="6.1" data-path="effect-of-batch-size.html"><a href="effect-of-batch-size.html#impact-of-diffrent-hyperparameters"><i class="fa fa-check"></i><b>6.1</b> Impact of diffrent hyperparameters</a></li>
<li class="chapter" data-level="6.2" data-path="effect-of-batch-size.html"><a href="effect-of-batch-size.html#appendix-complete-code-4"><i class="fa fa-check"></i><b>6.2</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="class-mlp.html"><a href="class-mlp.html"><i class="fa fa-check"></i><b>7</b> Class MLP</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">My First Steps in Neuronal Networks<br />
(Beginners Guide)</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="single-perceptron" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Single Perceptron</h1>
<p>In this chapter i will teach you how to code a single Perceptron in python with only the numpy package. Numpy uses a vectorizible math structure in which you can easily calculate elementwise or do stuff like normal matrix multiplications with just a symbol (i always interpret vectors as one dimensional matrices!). At the most of the time, its just translating math formulas into python code without changing its structure.<br />
First of all do we start with the necessary parameters, that are explained later:</p>
<p>Number of iterations over all the training dataset := <code>epochs</code><br />
Learning rate := <span class="math inline">\(\alpha =\)</span> <code>alpha</code><br />
Bias value := <span class="math inline">\(\beta =\)</span> <code>bias</code>
and the activation function:<br />
<span class="math display">\[ 
step(s)= 
\begin{cases}
    1,&amp; s   \geq \beta\\
    0,&amp; s &lt; \beta
\end{cases}
\]</span>
This function is named the heavyside-function and should be the easiest activation function to start with. If the weighted sum is smaller than the bias <span class="math inline">\(\beta\)</span>, it will send the value zero to the next neuron. Our brain works with the same behavior. If the electricity is too small, the neuron will not activate and the next one dosent get any electricity.</p>
<p>The training dataset is the following:
<span class="math display">\[
\left[
\begin{array}{cc|c}
x_i,_1 &amp; x_i,_2 &amp; y_i \\
\end{array}
\right]
\]</span>
<span class="math display">\[
\left[
\begin{array}{cc|c}
0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 1 \\
\end{array}
\right]
\]</span>
The provided training dataset contains the <code>X</code> matrix with two inputs for each scenario and the <code>Y</code> matrix with the correct output (each row contains the input and output of one scenario). If your looking exactly you can see that this is the OR-Gate. Later you will see why these type of problems are the only suitable things to do with a single neuron.</p>
<p>The needed python imports are the following:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="single-perceptron.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="single-perceptron.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> pyplot</span></code></pre></div>
<p>( Do you see more imports than only the numpy package? <a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">Yes</a> or No )</p>
<p>Now that we have all the needed parameters and settings, i can give you a quick overview of the algorithm.</p>
<div id="neural-network-basics" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Neural Network Basics</h2>
<p>In a NN we are having two basic parts, the forward pass and the backward pass. In the forward pass do we calculate the weighted sum of each input neuron with its weights of the layer and evaluating the activation function with it, to calculate the output. In the backward pass do we analyzing the error to adjust the weights accordingly. This is it! This is all a NN will do. I explained everything to you. Have a good life…<br />
Ahh no no ok we have a deeper look into it :)</p>
<p>What exactly is the forward pass in a single Perceptron? Its just the evaluation of the acvtivation function with the weighted sum like i said, so you have for one scenario of the training dataset, the following:
<span class="math display">\[
  step(W \cdot x^T_i) = y_i
\]</span>
This is the normal approach to use this formula to iterate over all scenarios in the training dataset…<br />
But i think its not the right way to describe it, because it gets very confusing to interpret it for all scenarios at the same time.</p>
<p>My next approach is to consider all scenarios in the training dataset in one formula. If your data isnt that huge, its a much faster approach as well. First of all do we need to interpret the new dimensions of <span class="math inline">\(W\)</span> and <span class="math inline">\(X\)</span>.<br />
We have <span class="math inline">\(X\)</span> as:
<span class="math display">\[
  X = \left[
  \begin{array}{cc}
  0 &amp; 0 \\
  0 &amp; 1 \\
  1 &amp; 0 \\
  1 &amp; 1 \\
  \end{array}
  \right]
\]</span>
each row describes the inputs for each neuron in the scenario <span class="math inline">\(i\)</span>.<br />
For the weights <span class="math inline">\(W\)</span> do we have for example:
<span class="math display">\[
  W =\left[
  \begin{array}{c}
  0.1 \\ 
  0.2 \\ 
  \end{array}
  \right]
\]</span>
The new formula looks like this:
<span class="math display">\[
  step(X * W) = Y
\]</span>
The <span class="math inline">\(*\)</span> symbol defines a matrix to matrix multiplication. For example if you take a look at the <span class="math inline">\(i\)</span>-th row (scenario) of <span class="math inline">\(X\)</span> you will see the following:
<span class="math display">\[
  Y_i,_0 = step([X_i,_0 \cdot W_0,_0 + X_i,_1 \cdot W_1,_0 ])
\]</span>
and <span class="math inline">\(Y_i,_0\)</span> is the approximated output of the <span class="math inline">\(i\)</span>-th scenario. Now can we look at the NN and compare the formula with it:<br />
<img src="img/NN_01_v4.png" style="width:50.0%" alt="https://app.diagrams.net/" /><br></p>
<p>Yes it is the same, its the weighted sum of the inputs and evaluated the activation function with it, to calculate the output of the scenario <span class="math inline">\(i\)</span>.</p>
</div>
<div id="forward-pass" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Forward pass</h2>
<p>Now can we create the so called <code>forward()</code> function in python:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="single-perceptron.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(x, w):</span>
<span id="cb3-2"><a href="single-perceptron.html#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( step(x <span class="op">@</span> w) )</span></code></pre></div>
<p>(Numpy provides us with the <code>@</code> symbol to make a matrix to matrix multiplication and the <code>.T</code> to transpose)</p>
<p>Because we want to put one dimensional matrices into the <code>step()</code> function, its necessary to use numpy for the if-else statement:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="single-perceptron.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step(s):</span>
<span id="cb4-2"><a href="single-perceptron.html#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( np.where(s <span class="op">&gt;=</span> bias, <span class="dv">1</span>, <span class="dv">0</span>) )</span></code></pre></div>
<p>In the next step will we create an small example for the forward pass:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="single-perceptron.html#cb5-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([</span>
<span id="cb5-2"><a href="single-perceptron.html#cb5-2" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">0</span>],</span>
<span id="cb5-3"><a href="single-perceptron.html#cb5-3" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb5-4"><a href="single-perceptron.html#cb5-4" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">0</span>],</span>
<span id="cb5-5"><a href="single-perceptron.html#cb5-5" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb5-6"><a href="single-perceptron.html#cb5-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb5-7"><a href="single-perceptron.html#cb5-7" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.array([</span>
<span id="cb5-8"><a href="single-perceptron.html#cb5-8" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.1</span>], </span>
<span id="cb5-9"><a href="single-perceptron.html#cb5-9" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.2</span>]</span>
<span id="cb5-10"><a href="single-perceptron.html#cb5-10" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb5-11"><a href="single-perceptron.html#cb5-11" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-12"><a href="single-perceptron.html#cb5-12" aria-hidden="true" tabindex="-1"></a>Y_approx <span class="op">=</span> forward(X, W)</span>
<span id="cb5-13"><a href="single-perceptron.html#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(Y_approx)</span></code></pre></div>
<pre><code>## [[0]
##  [0]
##  [0]
##  [0]]</code></pre>
<p>And these are all the generated outputs of our NN over all scenarios. Now do we need to calculate the error and adjust the weights accordingly.</p>
</div>
<div id="backward-pass" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> backward pass</h2>
<p>We need the Delta-Rule to adjust the weights in a single Perceptron:
<span class="math display">\[
  W(t+1) = W(t) + \Delta W(t)
\]</span>
with
<span class="math display">\[
  \Delta W(t) = \alpha \cdot X^{T} * (Y - \hat{Y})
\]</span>
and <span class="math inline">\(\hat{Y}\)</span> is the output of the NN. Translatet to code it is:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="single-perceptron.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward(W, X, Y, alpha, Y_approx):</span>
<span id="cb7-2"><a href="single-perceptron.html#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(W <span class="op">+</span> alpha <span class="op">*</span> X.T <span class="op">@</span> (Y <span class="op">-</span> Y_approx))</span></code></pre></div>
<p>With the result of the forward pass and and the correct outputs, do we have the following:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="single-perceptron.html#cb8-1" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([</span>
<span id="cb8-2"><a href="single-perceptron.html#cb8-2" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>],</span>
<span id="cb8-3"><a href="single-perceptron.html#cb8-3" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb8-4"><a href="single-perceptron.html#cb8-4" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb8-5"><a href="single-perceptron.html#cb8-5" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>]</span>
<span id="cb8-6"><a href="single-perceptron.html#cb8-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb8-7"><a href="single-perceptron.html#cb8-7" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb8-8"><a href="single-perceptron.html#cb8-8" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> backward(W, X, Y, alpha, Y_approx)</span>
<span id="cb8-9"><a href="single-perceptron.html#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(W)</span></code></pre></div>
<pre><code>## [[0.12]
##  [0.22]]</code></pre>
<p>and these are the new weight.</p>
</div>
<div id="single-perceptron-1" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Single Perceptron</h2>
<p>Now do we want to do the same process multiple times, to train the NN:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="single-perceptron.html#cb10-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([</span>
<span id="cb10-2"><a href="single-perceptron.html#cb10-2" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">0</span>],</span>
<span id="cb10-3"><a href="single-perceptron.html#cb10-3" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb10-4"><a href="single-perceptron.html#cb10-4" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">0</span>],</span>
<span id="cb10-5"><a href="single-perceptron.html#cb10-5" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb10-6"><a href="single-perceptron.html#cb10-6" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb10-7"><a href="single-perceptron.html#cb10-7" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([</span>
<span id="cb10-8"><a href="single-perceptron.html#cb10-8" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>],</span>
<span id="cb10-9"><a href="single-perceptron.html#cb10-9" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb10-10"><a href="single-perceptron.html#cb10-10" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb10-11"><a href="single-perceptron.html#cb10-11" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>]</span>
<span id="cb10-12"><a href="single-perceptron.html#cb10-12" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb10-13"><a href="single-perceptron.html#cb10-13" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.array([</span>
<span id="cb10-14"><a href="single-perceptron.html#cb10-14" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.1</span>], </span>
<span id="cb10-15"><a href="single-perceptron.html#cb10-15" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.2</span>]</span>
<span id="cb10-16"><a href="single-perceptron.html#cb10-16" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb10-17"><a href="single-perceptron.html#cb10-17" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb10-18"><a href="single-perceptron.html#cb10-18" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb10-19"><a href="single-perceptron.html#cb10-19" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb10-20"><a href="single-perceptron.html#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="single-perceptron.html#cb10-21" aria-hidden="true" tabindex="-1"></a>errors <span class="op">=</span> []</span>
<span id="cb10-22"><a href="single-perceptron.html#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb10-23"><a href="single-perceptron.html#cb10-23" aria-hidden="true" tabindex="-1"></a>  Y_approx <span class="op">=</span> forward(X, W)</span>
<span id="cb10-24"><a href="single-perceptron.html#cb10-24" aria-hidden="true" tabindex="-1"></a>  errors.append(Y <span class="op">-</span> Y_approx)</span>
<span id="cb10-25"><a href="single-perceptron.html#cb10-25" aria-hidden="true" tabindex="-1"></a>  W <span class="op">=</span> backward(W, X, Y, alpha, Y_approx)</span></code></pre></div>
<p>The KNN is trained. In the next step do we analyze the errors of each epoch. The best way to consider all the errors of each scenario is to measure the mean-square-error with the following formula:
<span class="math display">\[
  Errors = \frac{1}{2} \cdot \sum(Y-\hat{Y})^2
\]</span>
or as python code:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="single-perceptron.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mean_square_error(error):</span>
<span id="cb11-2"><a href="single-perceptron.html#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(error <span class="op">**</span> <span class="dv">2</span>) )</span></code></pre></div>
<p>Now do we need to calculate the mean-square-error for each element in the list <code>errors</code> which can be performed with <code>map()</code>:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="single-perceptron.html#cb12-1" aria-hidden="true" tabindex="-1"></a>mean_square_errors <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(mean_square_error, errors)))</span></code></pre></div>
<p>To plot the errors, im using the following function:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="single-perceptron.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_error(errors, title):</span>
<span id="cb13-2"><a href="single-perceptron.html#cb13-2" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(errors)))</span>
<span id="cb13-3"><a href="single-perceptron.html#cb13-3" aria-hidden="true" tabindex="-1"></a>  y <span class="op">=</span> np.array(errors)</span>
<span id="cb13-4"><a href="single-perceptron.html#cb13-4" aria-hidden="true" tabindex="-1"></a>  pyplot.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb13-5"><a href="single-perceptron.html#cb13-5" aria-hidden="true" tabindex="-1"></a>  pyplot.plot(x, y, <span class="st">&quot;g&quot;</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-6"><a href="single-perceptron.html#cb13-6" aria-hidden="true" tabindex="-1"></a>  pyplot.xlabel(<span class="st">&quot;Iterations&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb13-7"><a href="single-perceptron.html#cb13-7" aria-hidden="true" tabindex="-1"></a>  pyplot.ylabel(<span class="st">&quot;Mean Square Error&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb13-8"><a href="single-perceptron.html#cb13-8" aria-hidden="true" tabindex="-1"></a>  pyplot.title(title)</span>
<span id="cb13-9"><a href="single-perceptron.html#cb13-9" aria-hidden="true" tabindex="-1"></a>  pyplot.ylim(<span class="op">-</span><span class="fl">0.01</span>,<span class="bu">max</span>(errors)<span class="op">*</span><span class="fl">1.2</span>)</span>
<span id="cb13-10"><a href="single-perceptron.html#cb13-10" aria-hidden="true" tabindex="-1"></a>  pyplot.show()</span>
<span id="cb13-11"><a href="single-perceptron.html#cb13-11" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-12"><a href="single-perceptron.html#cb13-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb13-13"><a href="single-perceptron.html#cb13-13" aria-hidden="true" tabindex="-1"></a>plot_error(mean_square_errors, <span class="st">&quot;Single Perceptron&quot;</span>)</span></code></pre></div>
<p><img src="gitbook-demo_files/figure-html/0_9-1.png" width="576" /></p>
<p>If you survived until now, you have learned how to program a single Perceptron!</p>
</div>
<div id="appendix-complete-code" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Appendix (complete code)</h2>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="single-perceptron.html#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-2"><a href="single-perceptron.html#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> pyplot</span>
<span id="cb14-3"><a href="single-perceptron.html#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="single-perceptron.html#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="single-perceptron.html#cb14-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([</span>
<span id="cb14-6"><a href="single-perceptron.html#cb14-6" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">0</span>],</span>
<span id="cb14-7"><a href="single-perceptron.html#cb14-7" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb14-8"><a href="single-perceptron.html#cb14-8" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">0</span>],</span>
<span id="cb14-9"><a href="single-perceptron.html#cb14-9" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb14-10"><a href="single-perceptron.html#cb14-10" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb14-11"><a href="single-perceptron.html#cb14-11" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([</span>
<span id="cb14-12"><a href="single-perceptron.html#cb14-12" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>],</span>
<span id="cb14-13"><a href="single-perceptron.html#cb14-13" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb14-14"><a href="single-perceptron.html#cb14-14" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb14-15"><a href="single-perceptron.html#cb14-15" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>]</span>
<span id="cb14-16"><a href="single-perceptron.html#cb14-16" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb14-17"><a href="single-perceptron.html#cb14-17" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.array([</span>
<span id="cb14-18"><a href="single-perceptron.html#cb14-18" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.1</span>], </span>
<span id="cb14-19"><a href="single-perceptron.html#cb14-19" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.2</span>]</span>
<span id="cb14-20"><a href="single-perceptron.html#cb14-20" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb14-21"><a href="single-perceptron.html#cb14-21" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb14-22"><a href="single-perceptron.html#cb14-22" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb14-23"><a href="single-perceptron.html#cb14-23" aria-hidden="true" tabindex="-1"></a>train_n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb14-24"><a href="single-perceptron.html#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="single-perceptron.html#cb14-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step(s):</span>
<span id="cb14-26"><a href="single-perceptron.html#cb14-26" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( np.where(s <span class="op">&gt;=</span> bias, <span class="dv">1</span>, <span class="dv">0</span>) )</span>
<span id="cb14-27"><a href="single-perceptron.html#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="single-perceptron.html#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="single-perceptron.html#cb14-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(X, W):</span>
<span id="cb14-30"><a href="single-perceptron.html#cb14-30" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( step(X <span class="op">@</span> W) )</span>
<span id="cb14-31"><a href="single-perceptron.html#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="single-perceptron.html#cb14-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward(W, X, Y, alpha, Y_approx):</span>
<span id="cb14-33"><a href="single-perceptron.html#cb14-33" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(W <span class="op">+</span> alpha <span class="op">*</span> X.T <span class="op">@</span> (Y <span class="op">-</span> Y_approx))</span>
<span id="cb14-34"><a href="single-perceptron.html#cb14-34" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-35"><a href="single-perceptron.html#cb14-35" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-36"><a href="single-perceptron.html#cb14-36" aria-hidden="true" tabindex="-1"></a>errors <span class="op">=</span> []</span>
<span id="cb14-37"><a href="single-perceptron.html#cb14-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(train_n):</span>
<span id="cb14-38"><a href="single-perceptron.html#cb14-38" aria-hidden="true" tabindex="-1"></a>  Y_approx <span class="op">=</span> forward(X, W)</span>
<span id="cb14-39"><a href="single-perceptron.html#cb14-39" aria-hidden="true" tabindex="-1"></a>  errors.append(Y <span class="op">-</span> Y_approx)</span>
<span id="cb14-40"><a href="single-perceptron.html#cb14-40" aria-hidden="true" tabindex="-1"></a>  W <span class="op">=</span> backward(W, X, Y, alpha, Y_approx)</span>
<span id="cb14-41"><a href="single-perceptron.html#cb14-41" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-42"><a href="single-perceptron.html#cb14-42" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-43"><a href="single-perceptron.html#cb14-43" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-44"><a href="single-perceptron.html#cb14-44" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mean_square_error(error):</span>
<span id="cb14-45"><a href="single-perceptron.html#cb14-45" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(error <span class="op">**</span> <span class="dv">2</span>) )</span>
<span id="cb14-46"><a href="single-perceptron.html#cb14-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-47"><a href="single-perceptron.html#cb14-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-48"><a href="single-perceptron.html#cb14-48" aria-hidden="true" tabindex="-1"></a>mean_square_errors <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(mean_square_error, errors)))</span>
<span id="cb14-49"><a href="single-perceptron.html#cb14-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-50"><a href="single-perceptron.html#cb14-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-51"><a href="single-perceptron.html#cb14-51" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_error(errors, title):</span>
<span id="cb14-52"><a href="single-perceptron.html#cb14-52" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(errors)))</span>
<span id="cb14-53"><a href="single-perceptron.html#cb14-53" aria-hidden="true" tabindex="-1"></a>  y <span class="op">=</span> np.array(errors)</span>
<span id="cb14-54"><a href="single-perceptron.html#cb14-54" aria-hidden="true" tabindex="-1"></a>  pyplot.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb14-55"><a href="single-perceptron.html#cb14-55" aria-hidden="true" tabindex="-1"></a>  pyplot.plot(x, y, <span class="st">&quot;g&quot;</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-56"><a href="single-perceptron.html#cb14-56" aria-hidden="true" tabindex="-1"></a>  pyplot.xlabel(<span class="st">&quot;Iterations&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb14-57"><a href="single-perceptron.html#cb14-57" aria-hidden="true" tabindex="-1"></a>  pyplot.ylabel(<span class="st">&quot;Mean Square Error&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb14-58"><a href="single-perceptron.html#cb14-58" aria-hidden="true" tabindex="-1"></a>  pyplot.title(title)</span>
<span id="cb14-59"><a href="single-perceptron.html#cb14-59" aria-hidden="true" tabindex="-1"></a>  pyplot.ylim(<span class="op">-</span><span class="fl">0.01</span>,<span class="bu">max</span>(errors)<span class="op">*</span><span class="fl">1.2</span>)</span>
<span id="cb14-60"><a href="single-perceptron.html#cb14-60" aria-hidden="true" tabindex="-1"></a>  pyplot.show()</span>
<span id="cb14-61"><a href="single-perceptron.html#cb14-61" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-62"><a href="single-perceptron.html#cb14-62" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb14-63"><a href="single-perceptron.html#cb14-63" aria-hidden="true" tabindex="-1"></a>plot_error(mean_square_errors, <span class="st">&quot;Single Perceptron&quot;</span>)</span></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="adding-trainable-bias.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/cjvanlissa/gitbook-demo/edit/master/01_Single_Perceptron.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["gitbook-demo.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
