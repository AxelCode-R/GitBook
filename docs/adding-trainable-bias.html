<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Adding trainable Bias | gitbook-demo.knit</title>
  <meta name="description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Adding trainable Bias | gitbook-demo.knit" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  <meta name="github-repo" content="https://github.com/AxelCode-R/GitBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Adding trainable Bias | gitbook-demo.knit" />
  
  <meta name="twitter:description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  

<meta name="author" content="Axel Roth" />


<meta name="date" content="2021-11-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="single-perceptron.html"/>
<link rel="next" href="multi-layer-perceptrons.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A GitBook for Teaching</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#me"><i class="fa fa-check"></i><b>1.1</b> Me</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-book"><i class="fa fa-check"></i><b>1.2</b> The Book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#how-it-works"><i class="fa fa-check"></i><b>1.3</b> How it works</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="single-perceptron.html"><a href="single-perceptron.html"><i class="fa fa-check"></i><b>2</b> Single Perceptron</a>
<ul>
<li class="chapter" data-level="2.1" data-path="single-perceptron.html"><a href="single-perceptron.html#neural-network-basics"><i class="fa fa-check"></i><b>2.1</b> Neural Network Basics</a></li>
<li class="chapter" data-level="2.2" data-path="single-perceptron.html"><a href="single-perceptron.html#forward-pass"><i class="fa fa-check"></i><b>2.2</b> Forward pass</a></li>
<li class="chapter" data-level="2.3" data-path="single-perceptron.html"><a href="single-perceptron.html#backward-pass"><i class="fa fa-check"></i><b>2.3</b> backward pass</a></li>
<li class="chapter" data-level="2.4" data-path="single-perceptron.html"><a href="single-perceptron.html#single-perceptron-1"><i class="fa fa-check"></i><b>2.4</b> Single Perceptron</a></li>
<li class="chapter" data-level="2.5" data-path="single-perceptron.html"><a href="single-perceptron.html#appendix-full-code"><i class="fa fa-check"></i><b>2.5</b> Appendix (full code)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="adding-trainable-bias.html"><a href="adding-trainable-bias.html"><i class="fa fa-check"></i><b>3</b> Adding trainable Bias</a></li>
<li class="chapter" data-level="4" data-path="multi-layer-perceptrons.html"><a href="multi-layer-perceptrons.html"><i class="fa fa-check"></i><b>4</b> Multi Layer Perceptrons</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">My First Steps in Neuronal Networks<br />
(Beginners Guide)</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="adding-trainable-bias" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Adding trainable Bias</h1>
<p>The single Perceptron, you saw in the previews chapter had the following activation function:
<span class="math display">\[ 
step(s)= 
\begin{cases}
    1,&amp; s   \geq \beta\\
    0,&amp; s &lt; \beta
\end{cases}
\]</span>
with <span class="math inline">\(\beta = 1\)</span> and that is just the right <span class="math inline">\(\beta\)</span> for the given training dataset. But what happens if you shift the training data for example adding <span class="math inline">\(5\)</span> to the <span class="math inline">\(X\)</span> matrix? It is still the OR-Gate but now it will never give you the correct answer. That is because you need to select the <span class="math inline">\(\beta\)</span> accordingly. But this wouldnt be intelligent to search for each dataset the optimal shift by hand. That is why we now generalise the weighted sum step by step. First we generalise the activation function:
<span class="math display">\[ 
step(s)= 
\begin{cases}
    1,&amp; s   \geq 0\\
    0,&amp; s &lt; 0
\end{cases}
\]</span>
Now we can list it in the weighted sum:
<span class="math display">\[
  step(X \cdot W - \beta) = Y
\]</span>
But now we have the same problem as previews, because we need to specify the <span class="math inline">\(\beta\)</span> explicit. To add it to the training process we add a column with ones on the right side of <span class="math inline">\(X\)</span> and add the <span class="math inline">\(-\beta\)</span> to the last row of <span class="math inline">\(W\)</span>. The output of one scenario is now caluclated as the following:
<span class="math display">\[
  Y_i,_0 = step([X_i,_0 \cdot W_0,_0 + X_i,_1 \cdot W_1,_0 + X_i,_2 \cdot W_2,_0]) = step([X_i,_0 \cdot W_0,_0 + X_i,_1 \cdot W_1,_0 - \beta])
\]</span>
Furthermore because the <span class="math inline">\(W\)</span> is holding the <span class="math inline">\(\beta\)</span> it now gets re-adjusted in the backward pass so it is involved in the training process. Thats why we now generate a random number for it, because it gets corrected anyway.</p>
<p>As python code we now did the following:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="adding-trainable-bias.html#cb15-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([</span>
<span id="cb15-2"><a href="adding-trainable-bias.html#cb15-2" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">0</span>],</span>
<span id="cb15-3"><a href="adding-trainable-bias.html#cb15-3" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb15-4"><a href="adding-trainable-bias.html#cb15-4" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">0</span>],</span>
<span id="cb15-5"><a href="adding-trainable-bias.html#cb15-5" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb15-6"><a href="adding-trainable-bias.html#cb15-6" aria-hidden="true" tabindex="-1"></a>])<span class="op">+</span><span class="dv">5</span></span>
<span id="cb15-7"><a href="adding-trainable-bias.html#cb15-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.append(X, np.array([np.ones(<span class="bu">len</span>(X))]).T, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-8"><a href="adding-trainable-bias.html#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="adding-trainable-bias.html#cb15-9" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.array([</span>
<span id="cb15-10"><a href="adding-trainable-bias.html#cb15-10" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.1</span>], </span>
<span id="cb15-11"><a href="adding-trainable-bias.html#cb15-11" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.2</span>]</span>
<span id="cb15-12"><a href="adding-trainable-bias.html#cb15-12" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb15-13"><a href="adding-trainable-bias.html#cb15-13" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.append(W, <span class="op">-</span>np.array([np.random.random(<span class="bu">len</span>(W[<span class="dv">0</span>]))]).T, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-14"><a href="adding-trainable-bias.html#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="co">#W = np.append(W, np.array([[-5]]), axis=0)</span></span></code></pre></div>
<p>We added <span class="math inline">\(+5\)</span> to the <span class="math inline">\(X\)</span> matrix to simulate the problem of shifted data, added ones on the left side of <span class="math inline">\(X\)</span> and added negative random numbers between <span class="math inline">\((0,1)\)</span> to the weights. Yes if you would have a clue, what <span class="math inline">\(\beta\)</span> would be great for the given problem, its better to choose it explicit. The new problem needs more iterations, because it needs to find a good <span class="math inline">\(\beta\)</span> by its self.<br />
The complete code is the following:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="adding-trainable-bias.html#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-2"><a href="adding-trainable-bias.html#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random <span class="im">as</span> ra</span>
<span id="cb16-3"><a href="adding-trainable-bias.html#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-4"><a href="adding-trainable-bias.html#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> pyplot</span>
<span id="cb16-5"><a href="adding-trainable-bias.html#cb16-5" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">&#39;display.max_rows&#39;</span>, <span class="dv">500</span>)</span>
<span id="cb16-6"><a href="adding-trainable-bias.html#cb16-6" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">&#39;display.max_columns&#39;</span>, <span class="dv">500</span>)</span>
<span id="cb16-7"><a href="adding-trainable-bias.html#cb16-7" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">&#39;display.width&#39;</span>, <span class="dv">1000</span>)</span>
<span id="cb16-8"><a href="adding-trainable-bias.html#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="adding-trainable-bias.html#cb16-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([</span>
<span id="cb16-10"><a href="adding-trainable-bias.html#cb16-10" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">0</span>],</span>
<span id="cb16-11"><a href="adding-trainable-bias.html#cb16-11" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb16-12"><a href="adding-trainable-bias.html#cb16-12" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">0</span>],</span>
<span id="cb16-13"><a href="adding-trainable-bias.html#cb16-13" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb16-14"><a href="adding-trainable-bias.html#cb16-14" aria-hidden="true" tabindex="-1"></a>]) <span class="op">+</span> <span class="dv">5</span></span>
<span id="cb16-15"><a href="adding-trainable-bias.html#cb16-15" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.append(X, np.array([np.ones(<span class="bu">len</span>(X))]).T, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-16"><a href="adding-trainable-bias.html#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="adding-trainable-bias.html#cb16-17" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.array([</span>
<span id="cb16-18"><a href="adding-trainable-bias.html#cb16-18" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.1</span>], </span>
<span id="cb16-19"><a href="adding-trainable-bias.html#cb16-19" aria-hidden="true" tabindex="-1"></a>  [<span class="fl">0.2</span>]</span>
<span id="cb16-20"><a href="adding-trainable-bias.html#cb16-20" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb16-21"><a href="adding-trainable-bias.html#cb16-21" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.append(W, <span class="op">-</span>np.array([np.random.random(<span class="bu">len</span>(W[<span class="dv">0</span>]))]).T, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-22"><a href="adding-trainable-bias.html#cb16-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-23"><a href="adding-trainable-bias.html#cb16-23" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([</span>
<span id="cb16-24"><a href="adding-trainable-bias.html#cb16-24" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>],</span>
<span id="cb16-25"><a href="adding-trainable-bias.html#cb16-25" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb16-26"><a href="adding-trainable-bias.html#cb16-26" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb16-27"><a href="adding-trainable-bias.html#cb16-27" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>]</span>
<span id="cb16-28"><a href="adding-trainable-bias.html#cb16-28" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb16-29"><a href="adding-trainable-bias.html#cb16-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-30"><a href="adding-trainable-bias.html#cb16-30" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb16-31"><a href="adding-trainable-bias.html#cb16-31" aria-hidden="true" tabindex="-1"></a>train_n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb16-32"><a href="adding-trainable-bias.html#cb16-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-33"><a href="adding-trainable-bias.html#cb16-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step(s):</span>
<span id="cb16-34"><a href="adding-trainable-bias.html#cb16-34" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( np.where(s <span class="op">&gt;=</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>) )</span>
<span id="cb16-35"><a href="adding-trainable-bias.html#cb16-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-36"><a href="adding-trainable-bias.html#cb16-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-37"><a href="adding-trainable-bias.html#cb16-37" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(X, W):</span>
<span id="cb16-38"><a href="adding-trainable-bias.html#cb16-38" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( step(X <span class="op">@</span> W) )</span>
<span id="cb16-39"><a href="adding-trainable-bias.html#cb16-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-40"><a href="adding-trainable-bias.html#cb16-40" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward(W, X, Y, alpha, Y_approx):</span>
<span id="cb16-41"><a href="adding-trainable-bias.html#cb16-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(W <span class="op">+</span> alpha <span class="op">*</span> X.T <span class="op">@</span> (Y <span class="op">-</span> Y_approx))</span>
<span id="cb16-42"><a href="adding-trainable-bias.html#cb16-42" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-43"><a href="adding-trainable-bias.html#cb16-43" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-44"><a href="adding-trainable-bias.html#cb16-44" aria-hidden="true" tabindex="-1"></a>errors <span class="op">=</span> []</span>
<span id="cb16-45"><a href="adding-trainable-bias.html#cb16-45" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(train_n):</span>
<span id="cb16-46"><a href="adding-trainable-bias.html#cb16-46" aria-hidden="true" tabindex="-1"></a>  Y_approx <span class="op">=</span> forward(X, W)</span>
<span id="cb16-47"><a href="adding-trainable-bias.html#cb16-47" aria-hidden="true" tabindex="-1"></a>  errors.append(Y <span class="op">-</span> Y_approx)</span>
<span id="cb16-48"><a href="adding-trainable-bias.html#cb16-48" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(W)</span>
<span id="cb16-49"><a href="adding-trainable-bias.html#cb16-49" aria-hidden="true" tabindex="-1"></a>  W <span class="op">=</span> backward(W, X, Y, alpha, Y_approx)</span>
<span id="cb16-50"><a href="adding-trainable-bias.html#cb16-50" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-51"><a href="adding-trainable-bias.html#cb16-51" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb16-52"><a href="adding-trainable-bias.html#cb16-52" aria-hidden="true" tabindex="-1"></a>  </span></code></pre></div>
<pre><code>## [[ 0.1       ]
##  [ 0.2       ]
##  [-0.85455638]]
## [[ 0.05      ]
##  [ 0.15      ]
##  [-0.86455638]]
## [[ 0.        ]
##  [ 0.1       ]
##  [-0.87455638]]
## [[ 0.17      ]
##  [ 0.27      ]
##  [-0.84455638]]
## [[ 0.12      ]
##  [ 0.22      ]
##  [-0.85455638]]
## [[ 0.07      ]
##  [ 0.17      ]
##  [-0.86455638]]
## [[ 0.02      ]
##  [ 0.12      ]
##  [-0.87455638]]
## [[ 0.19      ]
##  [ 0.29      ]
##  [-0.84455638]]
## [[ 0.14      ]
##  [ 0.24      ]
##  [-0.85455638]]
## [[ 0.09      ]
##  [ 0.19      ]
##  [-0.86455638]]
## [[ 0.04      ]
##  [ 0.14      ]
##  [-0.87455638]]
## [[-0.01      ]
##  [ 0.09      ]
##  [-0.88455638]]
## [[ 0.16      ]
##  [ 0.26      ]
##  [-0.85455638]]
## [[ 0.11      ]
##  [ 0.21      ]
##  [-0.86455638]]
## [[ 0.06      ]
##  [ 0.16      ]
##  [-0.87455638]]
## [[ 0.01      ]
##  [ 0.11      ]
##  [-0.88455638]]
## [[ 0.18      ]
##  [ 0.28      ]
##  [-0.85455638]]
## [[ 0.13      ]
##  [ 0.23      ]
##  [-0.86455638]]
## [[ 0.08      ]
##  [ 0.18      ]
##  [-0.87455638]]
## [[ 0.03      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.09      ]
##  [ 0.18      ]
##  [-0.87455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]
## [[ 0.04      ]
##  [ 0.13      ]
##  [-0.88455638]]</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="adding-trainable-bias.html#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mean_square_error(error):</span>
<span id="cb18-2"><a href="adding-trainable-bias.html#cb18-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(error <span class="op">**</span> <span class="dv">2</span>) )</span>
<span id="cb18-3"><a href="adding-trainable-bias.html#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="adding-trainable-bias.html#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="adding-trainable-bias.html#cb18-5" aria-hidden="true" tabindex="-1"></a>mean_square_errors <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(mean_square_error, errors)))</span>
<span id="cb18-6"><a href="adding-trainable-bias.html#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="adding-trainable-bias.html#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="adding-trainable-bias.html#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_error(errors, title):</span>
<span id="cb18-9"><a href="adding-trainable-bias.html#cb18-9" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(errors)))</span>
<span id="cb18-10"><a href="adding-trainable-bias.html#cb18-10" aria-hidden="true" tabindex="-1"></a>  y <span class="op">=</span> np.array(errors)</span>
<span id="cb18-11"><a href="adding-trainable-bias.html#cb18-11" aria-hidden="true" tabindex="-1"></a>  pyplot.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb18-12"><a href="adding-trainable-bias.html#cb18-12" aria-hidden="true" tabindex="-1"></a>  pyplot.plot(x, y, <span class="st">&quot;g&quot;</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-13"><a href="adding-trainable-bias.html#cb18-13" aria-hidden="true" tabindex="-1"></a>  pyplot.xlabel(<span class="st">&quot;Iterations&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb18-14"><a href="adding-trainable-bias.html#cb18-14" aria-hidden="true" tabindex="-1"></a>  pyplot.ylabel(<span class="st">&quot;Mean Square Error&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb18-15"><a href="adding-trainable-bias.html#cb18-15" aria-hidden="true" tabindex="-1"></a>  pyplot.title(title)</span>
<span id="cb18-16"><a href="adding-trainable-bias.html#cb18-16" aria-hidden="true" tabindex="-1"></a>  pyplot.ylim(<span class="op">-</span><span class="fl">0.01</span>,<span class="bu">max</span>(errors)<span class="op">*</span><span class="fl">1.2</span>)</span>
<span id="cb18-17"><a href="adding-trainable-bias.html#cb18-17" aria-hidden="true" tabindex="-1"></a>  pyplot.show()</span>
<span id="cb18-18"><a href="adding-trainable-bias.html#cb18-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb18-19"><a href="adding-trainable-bias.html#cb18-19" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb18-20"><a href="adding-trainable-bias.html#cb18-20" aria-hidden="true" tabindex="-1"></a>plot_error(mean_square_errors, <span class="st">&quot;Mean-Square-Errors of a single Perceptron&quot;</span>)</span></code></pre></div>
<p><img src="gitbook-demo_files/figure-html/unnamed-chunk-14-3.png" width="576" /></p>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="single-perceptron.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multi-layer-perceptrons.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/cjvanlissa/gitbook-demo/edit/master/02_Adding_trainable_Bias.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["gitbook-demo.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
