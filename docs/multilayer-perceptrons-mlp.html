<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Multilayer Perceptrons (MLP) | gitbook-demo.knit</title>
  <meta name="description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Multilayer Perceptrons (MLP) | gitbook-demo.knit" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  <meta name="github-repo" content="https://github.com/AxelCode-R/GitBook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Multilayer Perceptrons (MLP) | gitbook-demo.knit" />
  
  <meta name="twitter:description" content="This GitBook should provide an slide insight into my own learning process of coding Neuronal Networks (NN). Additionaly its a good chance for me to learn how to write a GitBook." />
  

<meta name="author" content="Axel Roth" />


<meta name="date" content="2021-11-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="adding-trainable-bias.html"/>
<link rel="next" href="mlp-example-credit-default.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A GitBook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#me"><i class="fa fa-check"></i><b>1.1</b> Me</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-book"><i class="fa fa-check"></i><b>1.2</b> The Book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#how-it-works"><i class="fa fa-check"></i><b>1.3</b> How it works</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="single-perceptron.html"><a href="single-perceptron.html"><i class="fa fa-check"></i><b>2</b> Single Perceptron</a>
<ul>
<li class="chapter" data-level="2.1" data-path="single-perceptron.html"><a href="single-perceptron.html#neural-network-basics"><i class="fa fa-check"></i><b>2.1</b> Neural Network Basics</a></li>
<li class="chapter" data-level="2.2" data-path="single-perceptron.html"><a href="single-perceptron.html#forward-pass"><i class="fa fa-check"></i><b>2.2</b> Forward pass</a></li>
<li class="chapter" data-level="2.3" data-path="single-perceptron.html"><a href="single-perceptron.html#backward-pass"><i class="fa fa-check"></i><b>2.3</b> Backward pass</a></li>
<li class="chapter" data-level="2.4" data-path="single-perceptron.html"><a href="single-perceptron.html#single-perceptron-1"><i class="fa fa-check"></i><b>2.4</b> Single Perceptron</a></li>
<li class="chapter" data-level="2.5" data-path="single-perceptron.html"><a href="single-perceptron.html#why-does-it-work"><i class="fa fa-check"></i><b>2.5</b> Why does it work?</a></li>
<li class="chapter" data-level="2.6" data-path="single-perceptron.html"><a href="single-perceptron.html#appendix-complete-code"><i class="fa fa-check"></i><b>2.6</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="adding-trainable-bias.html"><a href="adding-trainable-bias.html"><i class="fa fa-check"></i><b>3</b> Adding trainable Bias</a>
<ul>
<li class="chapter" data-level="3.1" data-path="adding-trainable-bias.html"><a href="adding-trainable-bias.html#generalising-the-bias"><i class="fa fa-check"></i><b>3.1</b> Generalising the Bias</a></li>
<li class="chapter" data-level="3.2" data-path="adding-trainable-bias.html"><a href="adding-trainable-bias.html#appendix-complete-code-1"><i class="fa fa-check"></i><b>3.2</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multilayer-perceptrons-mlp.html"><a href="multilayer-perceptrons-mlp.html"><i class="fa fa-check"></i><b>4</b> Multilayer Perceptrons (MLP)</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multilayer-perceptrons-mlp.html"><a href="multilayer-perceptrons-mlp.html#forward-pass-1"><i class="fa fa-check"></i><b>4.1</b> Forward pass</a></li>
<li class="chapter" data-level="4.2" data-path="multilayer-perceptrons-mlp.html"><a href="multilayer-perceptrons-mlp.html#backward-pass-1"><i class="fa fa-check"></i><b>4.2</b> Backward pass</a></li>
<li class="chapter" data-level="4.3" data-path="multilayer-perceptrons-mlp.html"><a href="multilayer-perceptrons-mlp.html#appendix-complete-code-2"><i class="fa fa-check"></i><b>4.3</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html"><i class="fa fa-check"></i><b>5</b> MLP example (Credit Default)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html#loading-and-analysing-the-data"><i class="fa fa-check"></i><b>5.1</b> Loading and analysing the data</a></li>
<li class="chapter" data-level="5.2" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html#train-and-test-phase"><i class="fa fa-check"></i><b>5.2</b> Train and test phase</a></li>
<li class="chapter" data-level="5.3" data-path="mlp-example-credit-default.html"><a href="mlp-example-credit-default.html#appendix-complete-code-3"><i class="fa fa-check"></i><b>5.3</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="effect-of-batch-size.html"><a href="effect-of-batch-size.html"><i class="fa fa-check"></i><b>6</b> Effect of batch size</a>
<ul>
<li class="chapter" data-level="6.1" data-path="effect-of-batch-size.html"><a href="effect-of-batch-size.html#impact-of-diffrent-hyperparameters"><i class="fa fa-check"></i><b>6.1</b> Impact of diffrent hyperparameters</a></li>
<li class="chapter" data-level="6.2" data-path="effect-of-batch-size.html"><a href="effect-of-batch-size.html#appendix-complete-code-4"><i class="fa fa-check"></i><b>6.2</b> Appendix (complete code)</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="class-mlp.html"><a href="class-mlp.html"><i class="fa fa-check"></i><b>7</b> Class MLP</a></li>
<li class="chapter" data-level="8" data-path="decision-tree.html"><a href="decision-tree.html"><i class="fa fa-check"></i><b>8</b> Decision Tree</a>
<ul>
<li class="chapter" data-level="8.1" data-path="decision-tree.html"><a href="decision-tree.html#entropy"><i class="fa fa-check"></i><b>8.1</b> Entropy</a></li>
<li class="chapter" data-level="8.2" data-path="decision-tree.html"><a href="decision-tree.html#constructing-the-tree"><i class="fa fa-check"></i><b>8.2</b> Constructing the Tree</a></li>
<li class="chapter" data-level="8.3" data-path="decision-tree.html"><a href="decision-tree.html#forcast-credit-defaults-with-dt"><i class="fa fa-check"></i><b>8.3</b> Forcast Credit Defaults with DT</a></li>
<li class="chapter" data-level="8.4" data-path="decision-tree.html"><a href="decision-tree.html#extern-packages"><i class="fa fa-check"></i><b>8.4</b> Extern Packages</a></li>
<li class="chapter" data-level="8.5" data-path="decision-tree.html"><a href="decision-tree.html#appendix-complete-code-5"><i class="fa fa-check"></i><b>8.5</b> Appendix (complete code)</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><div class="line-block">My First Steps in Neuronal Networks<br />
(Beginners Guide)</div></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multilayer-perceptrons-mlp" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Multilayer Perceptrons (MLP)</h1>
<p>Multiple layers of neurons make up a multilayer Perceptron. As a result, we must calculate the forward pass several times, as well as the backward pass. First and foremost, do we need to broaden some definitions in order to support this behavior? The NN of the following image is what we want to do:<br />
<img src="img/NN_03_new.png" style="width:100.0%" alt="https://app.diagrams.net/" /><br></p>
<p>It’s my own definition of layers since I believed it would be easier to transition from <span class="math inline">\(n\)</span> to <span class="math inline">\(n+1\)</span> hidden layers if they were displayed as indicated in the image. By evaluating <span class="math inline">\(f(IN^{layer} \cdot W^{layer}) = OUT^{layer}\)</span> and just transferring the result to the next layer like <span class="math inline">\(OUT^{layer} = IN^{layer+1}\)</span>, with <span class="math inline">\(f\)</span> as the chosen activation function, you can see that each layer has the identical procedure in the forward pass.
We’ll choose the sigmoid function as the activation function <span class="math inline">\(f\)</span> because it has a simple deviation <span class="math inline">\(f&#39;\)</span> which is used for the backward pass and behaves similarly to the heavyside function with an output between 0 and 1.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="multilayer-perceptrons-mlp.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb17-2"><a href="multilayer-perceptrons-mlp.html#cb17-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb17-3"><a href="multilayer-perceptrons-mlp.html#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="multilayer-perceptrons-mlp.html#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> deriv_sigmoid(x):</span>
<span id="cb17-5"><a href="multilayer-perceptrons-mlp.html#cb17-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> x)</span></code></pre></div>
<p>Additionaly will we choose the XOR-Gate as training dataset and generate random weights in a very generic approach:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="multilayer-perceptrons-mlp.html#cb18-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([</span>
<span id="cb18-2"><a href="multilayer-perceptrons-mlp.html#cb18-2" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">0</span>],</span>
<span id="cb18-3"><a href="multilayer-perceptrons-mlp.html#cb18-3" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb18-4"><a href="multilayer-perceptrons-mlp.html#cb18-4" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">0</span>],</span>
<span id="cb18-5"><a href="multilayer-perceptrons-mlp.html#cb18-5" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb18-6"><a href="multilayer-perceptrons-mlp.html#cb18-6" aria-hidden="true" tabindex="-1"></a>]) </span>
<span id="cb18-7"><a href="multilayer-perceptrons-mlp.html#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="multilayer-perceptrons-mlp.html#cb18-8" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([</span>
<span id="cb18-9"><a href="multilayer-perceptrons-mlp.html#cb18-9" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>],</span>
<span id="cb18-10"><a href="multilayer-perceptrons-mlp.html#cb18-10" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb18-11"><a href="multilayer-perceptrons-mlp.html#cb18-11" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb18-12"><a href="multilayer-perceptrons-mlp.html#cb18-12" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>]</span>
<span id="cb18-13"><a href="multilayer-perceptrons-mlp.html#cb18-13" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb18-14"><a href="multilayer-perceptrons-mlp.html#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="multilayer-perceptrons-mlp.html#cb18-15" aria-hidden="true" tabindex="-1"></a>n_input <span class="op">=</span> <span class="bu">len</span>(X[<span class="dv">0</span>])</span>
<span id="cb18-16"><a href="multilayer-perceptrons-mlp.html#cb18-16" aria-hidden="true" tabindex="-1"></a>n_output <span class="op">=</span> <span class="bu">len</span>(Y[<span class="dv">0</span>])</span>
<span id="cb18-17"><a href="multilayer-perceptrons-mlp.html#cb18-17" aria-hidden="true" tabindex="-1"></a>hidden_layer_neurons <span class="op">=</span> [<span class="dv">2</span>] <span class="co"># the 2 means that there is one hidden layer with 2 neurons</span></span>
<span id="cb18-18"><a href="multilayer-perceptrons-mlp.html#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="multilayer-perceptrons-mlp.html#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_weights(n_input, n_output, hidden_layer_neurons):</span>
<span id="cb18-20"><a href="multilayer-perceptrons-mlp.html#cb18-20" aria-hidden="true" tabindex="-1"></a>  W <span class="op">=</span> []</span>
<span id="cb18-21"><a href="multilayer-perceptrons-mlp.html#cb18-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(hidden_layer_neurons)<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb18-22"><a href="multilayer-perceptrons-mlp.html#cb18-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>: <span class="co"># first layer</span></span>
<span id="cb18-23"><a href="multilayer-perceptrons-mlp.html#cb18-23" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((n_input<span class="op">+</span><span class="dv">1</span>, hidden_layer_neurons[i])))</span>
<span id="cb18-24"><a href="multilayer-perceptrons-mlp.html#cb18-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">==</span> <span class="bu">len</span>(hidden_layer_neurons): <span class="co"># last layer</span></span>
<span id="cb18-25"><a href="multilayer-perceptrons-mlp.html#cb18-25" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((hidden_layer_neurons[i<span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>, n_output)))</span>
<span id="cb18-26"><a href="multilayer-perceptrons-mlp.html#cb18-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: <span class="co"># middle layers</span></span>
<span id="cb18-27"><a href="multilayer-perceptrons-mlp.html#cb18-27" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((hidden_layer_neurons[i<span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>, hidden_layer_neurons[i])))</span>
<span id="cb18-28"><a href="multilayer-perceptrons-mlp.html#cb18-28" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb18-29"><a href="multilayer-perceptrons-mlp.html#cb18-29" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(W)</span>
<span id="cb18-30"><a href="multilayer-perceptrons-mlp.html#cb18-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-31"><a href="multilayer-perceptrons-mlp.html#cb18-31" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> generate_weights(n_input, n_output, hidden_layer_neurons)</span>
<span id="cb18-32"><a href="multilayer-perceptrons-mlp.html#cb18-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-33"><a href="multilayer-perceptrons-mlp.html#cb18-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;W[0]: </span><span class="ch">\n</span><span class="st">&quot;</span>, W[<span class="dv">0</span>])</span>
<span id="cb18-34"><a href="multilayer-perceptrons-mlp.html#cb18-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;W[1]: </span><span class="ch">\n</span><span class="st">&quot;</span>, W[<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## W[0]: 
##  [[0.5488135  0.71518937]
##  [0.60276338 0.54488318]
##  [0.4236548  0.64589411]]
## W[1]: 
##  [[0.43758721]
##  [0.891773  ]
##  [0.96366276]]</code></pre>
<p>The neurons for the hidden layers are generated using the <code>hidden_layer_neurons</code> list, while the input and output layer neurons are calculated from the training dataset. <code>hidden_layer_neurons = [4,2]</code>, for example, can construct two hidden layers with 4 and 2 neurons. I didn’t choose the bias because it is automatically rectified.
Is it now necessary to construct a helper function to add the biases to the last column of the inputs, like follows:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="multilayer-perceptrons-mlp.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_ones_to_input(x):</span>
<span id="cb20-2"><a href="multilayer-perceptrons-mlp.html#cb20-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(np.append(x, np.array([np.ones(<span class="bu">len</span>(x))]).T, axis<span class="op">=</span><span class="dv">1</span>))</span></code></pre></div>
<div id="forward-pass-1" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Forward pass</h2>
<p>The structur of the new forward function looks exactly like in the single Perceptron:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="multilayer-perceptrons-mlp.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(x, w):</span>
<span id="cb21-2"><a href="multilayer-perceptrons-mlp.html#cb21-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( sigmoid(x <span class="op">@</span> w) )</span></code></pre></div>
<p>Now we have everything to calculate the forward pass of the NN from above with the generated weights step by step:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="multilayer-perceptrons-mlp.html#cb22-1" aria-hidden="true" tabindex="-1"></a>IN <span class="op">=</span> []</span>
<span id="cb22-2"><a href="multilayer-perceptrons-mlp.html#cb22-2" aria-hidden="true" tabindex="-1"></a>OUT <span class="op">=</span> []</span>
<span id="cb22-3"><a href="multilayer-perceptrons-mlp.html#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="multilayer-perceptrons-mlp.html#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># layer 0</span></span>
<span id="cb22-5"><a href="multilayer-perceptrons-mlp.html#cb22-5" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-6"><a href="multilayer-perceptrons-mlp.html#cb22-6" aria-hidden="true" tabindex="-1"></a>IN.append( add_ones_to_input(X) )</span>
<span id="cb22-7"><a href="multilayer-perceptrons-mlp.html#cb22-7" aria-hidden="true" tabindex="-1"></a>OUT.append( forward(IN[i], W[i]) )</span>
<span id="cb22-8"><a href="multilayer-perceptrons-mlp.html#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="multilayer-perceptrons-mlp.html#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co"># layer 1</span></span>
<span id="cb22-10"><a href="multilayer-perceptrons-mlp.html#cb22-10" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb22-11"><a href="multilayer-perceptrons-mlp.html#cb22-11" aria-hidden="true" tabindex="-1"></a>IN.append( add_ones_to_input(OUT[i<span class="op">-</span><span class="dv">1</span>]) )</span>
<span id="cb22-12"><a href="multilayer-perceptrons-mlp.html#cb22-12" aria-hidden="true" tabindex="-1"></a>OUT.append( forward(IN[i], W[i]) )</span>
<span id="cb22-13"><a href="multilayer-perceptrons-mlp.html#cb22-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-14"><a href="multilayer-perceptrons-mlp.html#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="co"># error</span></span>
<span id="cb22-15"><a href="multilayer-perceptrons-mlp.html#cb22-15" aria-hidden="true" tabindex="-1"></a>Y<span class="op">-</span>OUT[<span class="op">-</span><span class="dv">1</span>]</span></code></pre></div>
<pre><code>## array([[-0.85974823],
##        [ 0.12242041],
##        [ 0.12015376],
##        [-0.89115202]])</code></pre>
<p>Thats all! We calculated the forward pass in a very generic way for the NN with 2 input neurons, 2 hidden neurons and 1 output neuron for all 4 scenarios at the same time. Sadly is the forward pass the easiest part of the multilayer Perceptron :)</p>
</div>
<div id="backward-pass-1" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Backward pass</h2>
<p>The weights will be adjusted using the backpropagation algorithm, which is a variant of the descent gradient algorithm. Calculating the sensitives of the outputs according to the activation function multiplied by the error that occurred is done in the output layer. On all other layers, it’s calculated by reversing the previously calculated gradient, splitting it up on each neuron by the previews weights, and multiplying the sensitivity of that layer’s outputs by the activation function’s sensitivity. The following is the formula:
<span class="math display">\[
  grad^i= 
\begin{cases}
    f^`(OUT^i) \cdot (Y-OUT^i),&amp; i =\text{last layer}\\
    f^`(OUT^i) \cdot (grad^{i+1} * \widetilde{W}^{i+1\ T}),&amp; \text{else}
\end{cases}
\]</span>
with <span class="math inline">\(\widetilde{W}\)</span> as the weights of the layer <span class="math inline">\(i\)</span>, without the connection to the bias neuron. You can calculate <span class="math inline">\(\widetilde{W}\)</span> in our datastructure by removing the last row.<br />
The gradients for the backward pass are calculated using the following code:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="multilayer-perceptrons-mlp.html#cb24-1" aria-hidden="true" tabindex="-1"></a>grad <span class="op">=</span> [<span class="va">None</span>] <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb24-2"><a href="multilayer-perceptrons-mlp.html#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="multilayer-perceptrons-mlp.html#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co"># layer 1</span></span>
<span id="cb24-4"><a href="multilayer-perceptrons-mlp.html#cb24-4" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb24-5"><a href="multilayer-perceptrons-mlp.html#cb24-5" aria-hidden="true" tabindex="-1"></a>grad[i] <span class="op">=</span> deriv_sigmoid(OUT[i]) <span class="op">*</span> (Y<span class="op">-</span>OUT[i])</span>
<span id="cb24-6"><a href="multilayer-perceptrons-mlp.html#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="multilayer-perceptrons-mlp.html#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># layer 0</span></span>
<span id="cb24-8"><a href="multilayer-perceptrons-mlp.html#cb24-8" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-9"><a href="multilayer-perceptrons-mlp.html#cb24-9" aria-hidden="true" tabindex="-1"></a>grad[i] <span class="op">=</span> deriv_sigmoid(OUT[i]) <span class="op">*</span>(grad[i<span class="op">+</span><span class="dv">1</span>] <span class="op">@</span> W[i<span class="op">+</span><span class="dv">1</span>][<span class="dv">0</span>:<span class="bu">len</span>(W[i<span class="op">+</span><span class="dv">1</span>])<span class="op">-</span><span class="dv">1</span>].T) <span class="co"># without bias weights</span></span></code></pre></div>
<p>You can now examine the gradient of the last layer and the direction in which it is displayed:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="multilayer-perceptrons-mlp.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Y: </span><span class="ch">\n</span><span class="st">&quot;</span>,Y)</span>
<span id="cb25-2"><a href="multilayer-perceptrons-mlp.html#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;OUT: </span><span class="ch">\n</span><span class="st">&quot;</span>,OUT[<span class="dv">1</span>])</span>
<span id="cb25-3"><a href="multilayer-perceptrons-mlp.html#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;grad: </span><span class="ch">\n</span><span class="st">&quot;</span>,grad[<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## Y: 
##  [[0]
##  [1]
##  [1]
##  [0]]
## OUT: 
##  [[0.85974823]
##  [0.87757959]
##  [0.87984624]
##  [0.89115202]]
## grad: 
##  [[-0.10366948]
##  [ 0.01315207]
##  [ 0.01270227]
##  [-0.08644184]]</code></pre>
<p>The gradient appears to be pointing in the direction of drifting the output <span class="math inline">\(OUT\)</span> closer to the desired output <span class="math inline">\(Y\)</span>. The gradient descent algorithm accomplishes exactly that.
The weights with the gradients and the learningrate <span class="math inline">\(\alpha\)</span> must then be adjusted according to the direction of the gradients using the following formula:
<span class="math display">\[
  W^i_{new} = W^i_{old} + \alpha \cdot ( IN^{i\ T} * grad^i) 
\]</span>
After the first epoch, we get the following results for the adjusted weights in the example above:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="multilayer-perceptrons-mlp.html#cb27-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.03</span></span>
<span id="cb27-2"><a href="multilayer-perceptrons-mlp.html#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="multilayer-perceptrons-mlp.html#cb27-3" aria-hidden="true" tabindex="-1"></a>W[<span class="dv">1</span>] <span class="op">=</span> W[<span class="dv">1</span>] <span class="op">+</span> alpha <span class="op">*</span> (IN[<span class="dv">1</span>].T <span class="op">@</span> grad[<span class="dv">1</span>]) </span>
<span id="cb27-4"><a href="multilayer-perceptrons-mlp.html#cb27-4" aria-hidden="true" tabindex="-1"></a>W[<span class="dv">0</span>] <span class="op">=</span> W[<span class="dv">0</span>] <span class="op">+</span> alpha <span class="op">*</span> (IN[<span class="dv">0</span>].T <span class="op">@</span> grad[<span class="dv">0</span>]) </span></code></pre></div>
<p>This was the forward and backward pass process in a multilayer Perceptron with two input neurons, two hidden layer neurons, and one output neuron for one epoch. It’s simple to use the above code to make a more generic NN with a variable number of hidden layers and variable training datasets for a given number of epochs, as shown in the appendix below.</p>
</div>
<div id="appendix-complete-code-2" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Appendix (complete code)</h2>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="multilayer-perceptrons-mlp.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb28-2"><a href="multilayer-perceptrons-mlp.html#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> pyplot</span>
<span id="cb28-3"><a href="multilayer-perceptrons-mlp.html#cb28-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb28-4"><a href="multilayer-perceptrons-mlp.html#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="multilayer-perceptrons-mlp.html#cb28-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([</span>
<span id="cb28-6"><a href="multilayer-perceptrons-mlp.html#cb28-6" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb28-7"><a href="multilayer-perceptrons-mlp.html#cb28-7" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">1</span>],</span>
<span id="cb28-8"><a href="multilayer-perceptrons-mlp.html#cb28-8" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>,<span class="dv">0</span>],</span>
<span id="cb28-9"><a href="multilayer-perceptrons-mlp.html#cb28-9" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>,<span class="dv">0</span>],</span>
<span id="cb28-10"><a href="multilayer-perceptrons-mlp.html#cb28-10" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb28-11"><a href="multilayer-perceptrons-mlp.html#cb28-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-12"><a href="multilayer-perceptrons-mlp.html#cb28-12" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.array([</span>
<span id="cb28-13"><a href="multilayer-perceptrons-mlp.html#cb28-13" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>],</span>
<span id="cb28-14"><a href="multilayer-perceptrons-mlp.html#cb28-14" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb28-15"><a href="multilayer-perceptrons-mlp.html#cb28-15" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">1</span>],</span>
<span id="cb28-16"><a href="multilayer-perceptrons-mlp.html#cb28-16" aria-hidden="true" tabindex="-1"></a>  [<span class="dv">0</span>]</span>
<span id="cb28-17"><a href="multilayer-perceptrons-mlp.html#cb28-17" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb28-18"><a href="multilayer-perceptrons-mlp.html#cb28-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-19"><a href="multilayer-perceptrons-mlp.html#cb28-19" aria-hidden="true" tabindex="-1"></a>n_input <span class="op">=</span> <span class="bu">len</span>(X[<span class="dv">0</span>])</span>
<span id="cb28-20"><a href="multilayer-perceptrons-mlp.html#cb28-20" aria-hidden="true" tabindex="-1"></a>n_output <span class="op">=</span> <span class="bu">len</span>(Y[<span class="dv">0</span>])</span>
<span id="cb28-21"><a href="multilayer-perceptrons-mlp.html#cb28-21" aria-hidden="true" tabindex="-1"></a>hidden_layer_neurons <span class="op">=</span> [<span class="dv">2</span>]</span>
<span id="cb28-22"><a href="multilayer-perceptrons-mlp.html#cb28-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-23"><a href="multilayer-perceptrons-mlp.html#cb28-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-24"><a href="multilayer-perceptrons-mlp.html#cb28-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_weights(n_input, n_output, hidden_layer_neurons):</span>
<span id="cb28-25"><a href="multilayer-perceptrons-mlp.html#cb28-25" aria-hidden="true" tabindex="-1"></a>  W <span class="op">=</span> []</span>
<span id="cb28-26"><a href="multilayer-perceptrons-mlp.html#cb28-26" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(hidden_layer_neurons)<span class="op">+</span><span class="dv">1</span>):</span>
<span id="cb28-27"><a href="multilayer-perceptrons-mlp.html#cb28-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>: <span class="co"># first layer</span></span>
<span id="cb28-28"><a href="multilayer-perceptrons-mlp.html#cb28-28" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((n_input <span class="op">+</span> <span class="dv">1</span>, hidden_layer_neurons[i])))</span>
<span id="cb28-29"><a href="multilayer-perceptrons-mlp.html#cb28-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">==</span> <span class="bu">len</span>(hidden_layer_neurons): <span class="co"># last layer</span></span>
<span id="cb28-30"><a href="multilayer-perceptrons-mlp.html#cb28-30" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((hidden_layer_neurons[i<span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>, n_output)))</span>
<span id="cb28-31"><a href="multilayer-perceptrons-mlp.html#cb28-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: <span class="co"># middle layers</span></span>
<span id="cb28-32"><a href="multilayer-perceptrons-mlp.html#cb28-32" aria-hidden="true" tabindex="-1"></a>      W.append(np.random.random((hidden_layer_neurons[i<span class="op">-</span><span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>, hidden_layer_neurons[i])))</span>
<span id="cb28-33"><a href="multilayer-perceptrons-mlp.html#cb28-33" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(W)</span>
<span id="cb28-34"><a href="multilayer-perceptrons-mlp.html#cb28-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-35"><a href="multilayer-perceptrons-mlp.html#cb28-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_ones_to_input(x):</span>
<span id="cb28-36"><a href="multilayer-perceptrons-mlp.html#cb28-36" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(np.append(x, np.array([np.ones(<span class="bu">len</span>(x))]).T, axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb28-37"><a href="multilayer-perceptrons-mlp.html#cb28-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-38"><a href="multilayer-perceptrons-mlp.html#cb28-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-39"><a href="multilayer-perceptrons-mlp.html#cb28-39" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> generate_weights(n_input, n_output, hidden_layer_neurons)</span>
<span id="cb28-40"><a href="multilayer-perceptrons-mlp.html#cb28-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-41"><a href="multilayer-perceptrons-mlp.html#cb28-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-42"><a href="multilayer-perceptrons-mlp.html#cb28-42" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb28-43"><a href="multilayer-perceptrons-mlp.html#cb28-43" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb28-44"><a href="multilayer-perceptrons-mlp.html#cb28-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-45"><a href="multilayer-perceptrons-mlp.html#cb28-45" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> deriv_sigmoid(x):</span>
<span id="cb28-46"><a href="multilayer-perceptrons-mlp.html#cb28-46" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> x)</span>
<span id="cb28-47"><a href="multilayer-perceptrons-mlp.html#cb28-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-48"><a href="multilayer-perceptrons-mlp.html#cb28-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-49"><a href="multilayer-perceptrons-mlp.html#cb28-49" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(x, w):</span>
<span id="cb28-50"><a href="multilayer-perceptrons-mlp.html#cb28-50" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( sigmoid(x <span class="op">@</span> w) )</span>
<span id="cb28-51"><a href="multilayer-perceptrons-mlp.html#cb28-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-52"><a href="multilayer-perceptrons-mlp.html#cb28-52" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward(IN, OUT, W, Y, grad, k):</span>
<span id="cb28-53"><a href="multilayer-perceptrons-mlp.html#cb28-53" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> k <span class="op">==</span> <span class="bu">len</span>(grad)<span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb28-54"><a href="multilayer-perceptrons-mlp.html#cb28-54" aria-hidden="true" tabindex="-1"></a>    grad[k] <span class="op">=</span> deriv_sigmoid(OUT[k]) <span class="op">*</span> (Y<span class="op">-</span>OUT[k])</span>
<span id="cb28-55"><a href="multilayer-perceptrons-mlp.html#cb28-55" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>:</span>
<span id="cb28-56"><a href="multilayer-perceptrons-mlp.html#cb28-56" aria-hidden="true" tabindex="-1"></a>    grad[k] <span class="op">=</span> deriv_sigmoid(OUT[k]) <span class="op">*</span>(grad[k<span class="op">+</span><span class="dv">1</span>] <span class="op">@</span> W[k<span class="op">+</span><span class="dv">1</span>][<span class="dv">0</span>:<span class="bu">len</span>(W[k<span class="op">+</span><span class="dv">1</span>])<span class="op">-</span><span class="dv">1</span>].T)</span>
<span id="cb28-57"><a href="multilayer-perceptrons-mlp.html#cb28-57" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>(grad)</span>
<span id="cb28-58"><a href="multilayer-perceptrons-mlp.html#cb28-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-59"><a href="multilayer-perceptrons-mlp.html#cb28-59" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.03</span></span>
<span id="cb28-60"><a href="multilayer-perceptrons-mlp.html#cb28-60" aria-hidden="true" tabindex="-1"></a>errors <span class="op">=</span> []</span>
<span id="cb28-61"><a href="multilayer-perceptrons-mlp.html#cb28-61" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">40000</span>):</span>
<span id="cb28-62"><a href="multilayer-perceptrons-mlp.html#cb28-62" aria-hidden="true" tabindex="-1"></a>  IN <span class="op">=</span> []</span>
<span id="cb28-63"><a href="multilayer-perceptrons-mlp.html#cb28-63" aria-hidden="true" tabindex="-1"></a>  OUT <span class="op">=</span> []</span>
<span id="cb28-64"><a href="multilayer-perceptrons-mlp.html#cb28-64" aria-hidden="true" tabindex="-1"></a>  grad <span class="op">=</span> [<span class="va">None</span>]<span class="op">*</span><span class="bu">len</span>(W)</span>
<span id="cb28-65"><a href="multilayer-perceptrons-mlp.html#cb28-65" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)):</span>
<span id="cb28-66"><a href="multilayer-perceptrons-mlp.html#cb28-66" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> k<span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb28-67"><a href="multilayer-perceptrons-mlp.html#cb28-67" aria-hidden="true" tabindex="-1"></a>      IN.append(add_ones_to_input(X))</span>
<span id="cb28-68"><a href="multilayer-perceptrons-mlp.html#cb28-68" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb28-69"><a href="multilayer-perceptrons-mlp.html#cb28-69" aria-hidden="true" tabindex="-1"></a>      IN.append(add_ones_to_input(OUT[k<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb28-70"><a href="multilayer-perceptrons-mlp.html#cb28-70" aria-hidden="true" tabindex="-1"></a>    OUT.append(forward(x<span class="op">=</span>IN[k], w<span class="op">=</span>W[k]))</span>
<span id="cb28-71"><a href="multilayer-perceptrons-mlp.html#cb28-71" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-72"><a href="multilayer-perceptrons-mlp.html#cb28-72" aria-hidden="true" tabindex="-1"></a>  errors.append(Y <span class="op">-</span> OUT[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb28-73"><a href="multilayer-perceptrons-mlp.html#cb28-73" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-74"><a href="multilayer-perceptrons-mlp.html#cb28-74" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb28-75"><a href="multilayer-perceptrons-mlp.html#cb28-75" aria-hidden="true" tabindex="-1"></a>    grad <span class="op">=</span> backward(IN, OUT, W, Y, grad, k) </span>
<span id="cb28-76"><a href="multilayer-perceptrons-mlp.html#cb28-76" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-77"><a href="multilayer-perceptrons-mlp.html#cb28-77" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(W)):</span>
<span id="cb28-78"><a href="multilayer-perceptrons-mlp.html#cb28-78" aria-hidden="true" tabindex="-1"></a>    W[k] <span class="op">=</span> W[k] <span class="op">+</span> alpha <span class="op">*</span> (IN[k].T <span class="op">@</span> grad[k])</span>
<span id="cb28-79"><a href="multilayer-perceptrons-mlp.html#cb28-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-80"><a href="multilayer-perceptrons-mlp.html#cb28-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-81"><a href="multilayer-perceptrons-mlp.html#cb28-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-82"><a href="multilayer-perceptrons-mlp.html#cb28-82" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mean_square_error(error):</span>
<span id="cb28-83"><a href="multilayer-perceptrons-mlp.html#cb28-83" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span>( <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(error <span class="op">**</span> <span class="dv">2</span>) )</span>
<span id="cb28-84"><a href="multilayer-perceptrons-mlp.html#cb28-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-85"><a href="multilayer-perceptrons-mlp.html#cb28-85" aria-hidden="true" tabindex="-1"></a>mean_square_errors <span class="op">=</span> np.array(<span class="bu">list</span>(<span class="bu">map</span>(mean_square_error, errors)))</span>
<span id="cb28-86"><a href="multilayer-perceptrons-mlp.html#cb28-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-87"><a href="multilayer-perceptrons-mlp.html#cb28-87" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_error(errors, title):</span>
<span id="cb28-88"><a href="multilayer-perceptrons-mlp.html#cb28-88" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(errors)))</span>
<span id="cb28-89"><a href="multilayer-perceptrons-mlp.html#cb28-89" aria-hidden="true" tabindex="-1"></a>  y <span class="op">=</span> np.array(errors)</span>
<span id="cb28-90"><a href="multilayer-perceptrons-mlp.html#cb28-90" aria-hidden="true" tabindex="-1"></a>  pyplot.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb28-91"><a href="multilayer-perceptrons-mlp.html#cb28-91" aria-hidden="true" tabindex="-1"></a>  pyplot.plot(x, y, <span class="st">&quot;g&quot;</span>, linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb28-92"><a href="multilayer-perceptrons-mlp.html#cb28-92" aria-hidden="true" tabindex="-1"></a>  pyplot.xlabel(<span class="st">&quot;Iterations&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb28-93"><a href="multilayer-perceptrons-mlp.html#cb28-93" aria-hidden="true" tabindex="-1"></a>  pyplot.ylabel(<span class="st">&quot;Mean Square Error&quot;</span>, fontsize <span class="op">=</span> <span class="dv">16</span>)</span>
<span id="cb28-94"><a href="multilayer-perceptrons-mlp.html#cb28-94" aria-hidden="true" tabindex="-1"></a>  pyplot.title(title)</span>
<span id="cb28-95"><a href="multilayer-perceptrons-mlp.html#cb28-95" aria-hidden="true" tabindex="-1"></a>  pyplot.ylim(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb28-96"><a href="multilayer-perceptrons-mlp.html#cb28-96" aria-hidden="true" tabindex="-1"></a>  pyplot.show()</span>
<span id="cb28-97"><a href="multilayer-perceptrons-mlp.html#cb28-97" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb28-98"><a href="multilayer-perceptrons-mlp.html#cb28-98" aria-hidden="true" tabindex="-1"></a>plot_error(mean_square_errors, <span class="st">&quot;Mean-Square-Errors of MLP 2x2x1&quot;</span>)</span></code></pre></div>
<p><img src="gitbook-demo_files/figure-html/2_9-7.png" width="576" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="adding-trainable-bias.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mlp-example-credit-default.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/AxelCode-R/GitBook/edit/master/03_Multi_Layer_Perceptron.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["gitbook-demo.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
